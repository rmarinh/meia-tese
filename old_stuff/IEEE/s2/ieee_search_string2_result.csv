"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Unit Test Generation Multi-Agent AI System for Enhancing Software Documentation and Code Coverage","D. Stojanović; B. Pavković; N. Četić; M. Krunić; L. Vidaković","Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Departmant of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; LabSoft, Novi Sad, Serbia; LabSoft, Novi Sad, Serbia",2024 32nd Telecommunications Forum (TELFOR),"2 Jan 2025","2024","","","1","4","Software development necessitates a robust testing plan though test development can be laborious and nonappealing task. We explore the utilization of the application artificial intelligence agents for generating and executing unit tests, enhancing the “Mostly Basic Python Problems” dataset. We employ behavior-driven development within a three-agent system to generate user stories and unit tests. Empirical results indicate improvements in branch coverage, illustrating the effective utilization of large language models in software testing and development processes.","2994-5828","979-8-3503-9106-0","10.1109/TELFOR63250.2024.10819096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10819096","AI agents;unit test generation;code generation;SW test automation;transformers;BDD","Software testing;Codes;Large language models;Documentation;Telecommunications;Test pattern generators;Software development management;Python","","1","","7","IEEE","2 Jan 2025","26-27 Nov. 2024","26-27 Nov. 2024","IEEE","IEEE Conferences"
"Addressing Data Leakage in HumanEval Using Combinatorial Test Design","J. S. Bradbury; R. More","Ontario Tech University, Oshawa, ON, Canada; Ontario Tech University, Oshawa, ON, Canada","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","587","591","The use of large language models (LLMs) is widespread across many domains, including Software Engineering, where they have been used to automate tasks such as program generation and test classification. As LLM-based methods continue to evolve, it is important that we define clear and robust methods that fairly evaluate performance. Benchmarks are a common approach to assess LLMs with respect to their ability to solve problem-specific tasks as well as assess different versions of an LLM to solve tasks over time. For example, the HumanEval benchmark is composed of 164 hand-crafted tasks and has become an important tool in assessing LLM-based program generation. However, a major barrier to a fair evaluation of LLMs using benchmarks like HumanEval is data contamination resulting from data leakage of benchmark tasks and solutions into the training data set. This barrier is compounded by the black-box nature of LLM training data which makes it difficult to even know if data leakage has occurred. To address the data leakage problem, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to performance evaluation. To assess our benchmark construction method, we propose HumanEval_T, an alternative benchmark to HumanEval that was constructed using template tasks and combinatorial test design.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989022","Large Language Models (LLMs);software engineering;benchmark;program generation;combinatorial testing;evaluation;fairness;template","Performance evaluation;Large language models;Combinatorial testing;Training data;Closed box;Benchmark testing;Contamination;Software engineering","","3","","22","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"LLM-Driven Smart Test Case Generation for Scalable Software Testing","K. Kanagaraj; D. Handa; K. K. Nikhil; S. Duvarakanath; S. M.","Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, India","2025 2nd International Conference on Software, Systems and Information Technology (SSITCON)","22 Jan 2026","2025","","","1","6","Software testing is vital to modern development; however, traditional test case generation remains limited by imprecision, heavy manual intervention, and a lack of scalability. This study proposes a robust framework that integrates Large Language Models (LLMs) with multi-agent systems to generate fully automated, end-to-end test cases. By combining Deep Learning with Chain-of-Thought reasoning, the framework intelligently analyzes back-end APIs and front-end UIs, producing context-aware and comprehensive test suites. The evaluation of the system demonstrated a high-test success rate of 91.6 % and an average code coverage of 85.3 % in diverse applications. This study offers a much greater degree of adaptability to new requirements with minimal intervention while maintaining high reliability and performance qualities. These facts make it attractive for enterprise environments, where it can speed up testing with much greater precision, scaling, and enterprise readiness. Hence, the framework stands for an end-to-end solution for next-generation regression automation.","","979-8-3315-2623-8","10.1109/SSITCON66133.2025.11342080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11342080","LLM;Multi-Agent Systems;Testcase Generation;System Testing;CI/CD","Software testing;System testing;Codes;Automation;Manuals;Software systems;Software reliability;Stakeholders;Multi-agent systems;Software development management","","","","15","IEEE","22 Jan 2026","17-18 Oct. 2025","17-18 Oct. 2025","IEEE","IEEE Conferences"
"The GitHub Recent Bugs Dataset for Evaluating LLM-Based Debugging Applications","J. Y. Lee; S. Kang; J. Yoon; S. Yoo","KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","27 Aug 2024","2024","","","442","444","While Large Language Models (LLMs) have demon-strated strong natural language and code processing capabilities, concern has been raised as to whether existing bug benchmarks are included in their training data. We examine the training data of the open-source LLM StarCoder, and find it likely that data from the widely used Defects4J benchmark was included, raising the possibility of its inclusion in the training data of the GPT model as well. This makes it difficult to tell how well LLM-based results on Defects4J would generalize, as for any results it would be unclear whether a technique's performance is due to LLM generalization or memorization. To remedy this issue and facilitate continued research on LLM-based SE, we present the GitHub Recent Bugs (GHRB) framework, which continuously gathers real-world Java bugs for use in evaluation of LLM-based techniques. To date, we have gathered 89 bugs reported after the GPT-3.5 training data cutoff point of September 2021.","2159-4848","979-8-3503-0818-1","10.1109/ICST60714.2024.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638568","Benchmark;Debugging;Machine Learning","Software testing;Java;Large language models;Computer bugs;Natural languages;Training data;Debugging","","12","","15","IEEE","27 Aug 2024","27-31 May 2024","27-31 May 2024","IEEE","IEEE Conferences"
"Software Unit Test Automation with LLM-Based Generative AI: Evaluating Test Quality through Code Coverage and Edge-Case Analysis","S. Genç; M. F. Ceylan; A. İstanbullu","Computer Engineering, Balıkesir University, Balıkesir, Türkiye; Computer Engineering, Balıkesir University, Balıkesir, Türkiye; Computer Engineering, Balıkesir University, Balıkesir, Türkiye",2025 10th International Conference on Computer Science and Engineering (UBMK),"24 Oct 2025","2025","","","242","247","Software unit testing is a critical verification step to ensure the correctness and reliability of software. However, manual writing of test cases is a time-consuming and error-prone process. This paper examines the integration of generative artificial intelligence models (LLM) into software test engineering and addresses automatic unit test generation. In the proposed method, test functions in pytest format are generated from the GPT-4 model using sample functions written in Python and these outputs are systematically evaluated. The validity of the tests is tested in terms of functional correctness by running them with pytest, and code coverage rates are measured with the coverage.py tool. Furthermore, the automatic generation of edge-case scenarios was analysed. The results show that the proposed method can produce tests with high accuracy and an average code coverage rate as high as 100%. At least one edge-case for each function was successfully tested. The results obtained show that generative artificial intelligence-based test generation can be integrated into software development processes and can significantly speed up and improve the accuracy of manual test writing. In this context, the study demonstrates that LLM-based test automation can be used as a productivity enhancing tool in software engineering and makes an original contribution to the literature.","2521-1641","979-8-3315-9975-1","10.1109/UBMK67458.2025.11206953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11206953","Generative Artificial Intelligence;LLM (Large Language Models);Automated Unit Test Automation;Code Coverage;Edge-Case Analysis","Codes;Automation;Accuracy;Generative AI;Manuals;Writing;Software;Software reliability;Test pattern generators;Testing","","","","15","IEEE","24 Oct 2025","17-21 Sept. 2025","17-21 Sept. 2025","IEEE","IEEE Conferences"
"Static Analysis and LLM for Comprehensive Java Unit Test Generation","W. Wei","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","24 Jun 2025","2025","","","87","92","Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a $10 \%$ improvement in code coverage and a $13 \%$ enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of $89.7 \%$, proving its effectiveness in producing both high-quality and reliable test cases.","","979-8-3315-1091-6","10.1109/AEMCSE65292.2025.11042526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11042526","software testing;Java unit testing;static analysis;LLM;test case generation","Software testing;Java;Large language models;Static analysis;Manuals;Syntactics;Software;Software reliability;Test pattern generators;Software engineering","","","","19","IEEE","24 Jun 2025","9-11 May 2025","9-11 May 2025","IEEE","IEEE Conferences"
"AI-Powered Unit Test Generation via Multi-LLM Chaining: A Case Study With GPT-4o, Gemini, and Claude-3.5","C. Kumar; U. Sri Ponaka; P. V. L. N. Naidu; P. Bhuvaneswari; M. Prasad; S. K. Singh","Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amaravati Campus, Amaravati, India; Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amaravati Campus, Amaravati, India; Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amaravati Campus, Amaravati, India; Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amaravati Campus, Amaravati, India; School of Computer Science, University of Technology Sydney, Sydney, Ultimo, NSW, Australia; Department of Computer Science and Engineering, Manipal Academy of Higher Education, Manipal Institute of Technology Bengaluru, Manipal, India",IEEE Access,"5 Dec 2025","2025","13","","204058","204071","Software testing is a crucial activity in the software development cycle, as it verifies code correctness, reliability, and maintainabilily. Unit testing involves verifying the correctness of the individual components of a program. Manually writing these tests is resource-intensive and time-consuming. Researchers have proposed various automated test generation methods to overcome this problem. Large Language Models have shown great success in automatically generating unit tests recently. Although a single LLM configuration can produce satisfactory responses, there are possible risks to the effectiveness of the generated tests. We present a novel method, LLM Chaining, that uses the collaboration of several LLMs, namely Gemini, GPT-4o, and Claude-3.5 Sonnet, to collaborate and iteratively enhance tests. We began by testing a single LLM writing unit tests, but after noting their suboptimal performance, we explored LLM chaining as a method to enhance the accuracy and coverage of the produced tests. Gemini and GPT-4o yielded the most reliable results of all the configurations we tested. Under this configuration, the LLM system uses Gemini to generate the initial JUnit tests, then sends the generated results to GPT-4o to improve accuracy. The HumanEval dataset was used to create JUnit test cases for this study. JaCoCo is used to collect coverage statistics, and PIT is used for mutation-based testing to measure an LLM’s ability to detect faults in tests. Our approach achieved 99.05% branch coverage, 90.48% line coverage, and 94.32% mutation coverage. In contrast, Randoop, a well-known automated test generation tool, obtained 68.64% branch coverage and 78.84% line coverage. This demonstrates how multi-step LLM refinement works well to advance automated test generation.","2169-3536","","10.1109/ACCESS.2025.3637221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11269709","Software testing;unit testing;LLM;Gemini;GPT-4o;Claude-3.5 Sonnet;LLM chaining;JUnit;JaCoCo","Test pattern generators;Software;Codes;Software testing;Reliability;Writing;Manuals;Large language models;Software development management;Software reliability","","","","38","CCBY","26 Nov 2025","2025","","IEEE","IEEE Journals"
"Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation","A. Abdullin; P. Derakhshanfar; A. Panichella","JetBrains Research, TV Delft, Amsterdam, The Netherlands; JetBrains Research, Amsterdam, The Netherlands; TU Delft, Delft, The Netherlands","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","221","232","Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new op-portunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM -based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools' performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods w.r.t. coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989033","automatic test generation;symbolic execution;concolic testing;large language models;search-based software testing","Software testing;Measurement;Java;Codes;Large language models;Fault detection;Semantics;Computer bugs;Test pattern generators;Software engineering","","2","","62","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"MAGISTER: LLM-Based Test Generation with Role-Specialized Agents","A. Ahammad; M. El Bajta; M. Radgui","SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco; SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco; SI2M Laboratory, National Institute of Statistics and Applied Economics, Rabat, Morocco",2025 International Conference on Intelligent Systems: Theories and Applications (SITA),"9 Dec 2025","2025","","","1","7","Automated test generation is one of the most critical topics in software testing, with numerous challenges due to the need for deep code understanding and the creation of meaningful assertions. Traditional approaches often generate low-quality and difficult-to-read tests that lack real code understanding and rely on statistical and dynamic analysis. With the recent advances in Large Language Models (LLMs), new opportunities emerge for generating unit tests that prioritize readability and context understanding. In this paper, we introduce MAGISTER, a multiagent framework for LLM-based unit test generation, where each agent (Analyzer Agent, Test Generation Agent, Executor Agent, and Refiner Agent) specializes in a specific role within the framework workflow, which involves analyzing the codebase to identify testable units and generating test code with feedbackdriven refinement. We evaluated MAGISTER on five open-source Python projects, demonstrating a significant improvement in code coverage for modular codebases compared to the original userwritten tests. However, it still has limitations when handling large and complex projects requiring domain-specific knowledge. Our results demonstrate the promising potential of LLM-driven and agent-based architectures in advancing test automation while highlighting directions for future improvement.","","979-8-3315-5989-2","10.1109/SITA67914.2025.11273637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11273637","Automated Test Generation;Large Language Models;Multi-Agent Systems;Unit Testing;LLM-based Testing","Software testing;Codes;Automation;Large language models;Manuals;Software reliability;Test pattern generators;Logic;Python;Multi-agent systems","","","","39","IEEE","9 Dec 2025","20-21 Oct. 2025","20-21 Oct. 2025","IEEE","IEEE Conferences"
"RLHavoc:Enhanced Historical Data Orientation Enhances Code Coverage","S. Zhang; C. Liao; R. Liu","School of Computer Science and Engineering, University of Electronic Science and Technology of China, ChengDu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, ChengDu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, ChengDu, China",2023 8th International Conference on Signal and Image Processing (ICSIP),"9 Oct 2023","2023","","","1079","1084","Fuzzing is a common method in software test engineering, which tries to trigger potential vulnerabilities in the target program by continuously providing modified input to the test target. The traditional fuzzy test tools are white box test, gray box test, black box test. The mutation strategies used in these traditional testing tools have high randomness. As a result, the traditional test methods have high randomness in the selection of strategic actions, so effective strategic actions maybe be ignored. Aiming at improving the coverage rate of fuzzy test code based on variation, we implement RLHavoc to improve the efficiency of American fuzzy lop(AFL) based on Deep Q Network(DQN) algorithm by using DQN to prioritize the generation of test cases in AFL. RLHavoc combines AFL’s Havoc phase mutation strategy with DQN algorithm. DQN algorithm takes advantage of AFL’s Havoc strategy to conduct grey box testing for different binaries and use comprehensive historical data to guide the action selection of variation strategies in the fuzzy test vulnerability detection process, taking seeds as state and new the number of unique execution paths as reward, which enhances the action space orientation of the secondary fuzz target binaries, thus improving the code coverage. Compared to the traditional AFL fuzz effect, our experimental results show that the code coverage rate is improved by about 3%.","","979-8-3503-9793-2","10.1109/ICSIP57908.2023.10271090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271090","Havoc;DQN;Comprehensive Historical Data;Code Coverage","Training;Codes;Image processing;Closed box;Fuzzing;Testing;Glass box","","1","","22","IEEE","9 Oct 2023","8-10 July 2023","8-10 July 2023","IEEE","IEEE Conferences"
"WIP: CodeInspector: Automated LLM-Supported CS1-Level Code Assessment","E. Imhmed; E. Ceh-Varela; L. Scherer; G. Candal; I. Sanjaya","Department of Mathematical Sciences, Eastern New Mexico University, Portales, NM, USA; Department of Mathematical Sciences, Eastern New Mexico University, Portales, NM, USA; Department of Mathematical Sciences, Eastern New Mexico University, Portales, NM, USA; Department of Mathematical Sciences, Eastern New Mexico University, Portales, NM, USA; Department of Mathematical Sciences, Eastern New Mexico University, Portales, NM, USA",2025 IEEE Frontiers in Education Conference (FIE),"13 Jan 2026","2025","","","1","5","This research-to-practice WIP paper presents CodeInspector—an automated assessment system for CS1-level Java assignments. Designed for scalability and modularity, CodeInspector streamlines and standardizes code assessment, promoting a more equitable learning environment. Our preliminary findings suggest that CodeInspector is promising. It reduces instructors' assessment workload while providing students with immediate, formative feedback that supports an iterative learning process. Additionally, CodeInspector offers students insights into their performance through error-weighted densities benchmarked against class-wide averages.","2377-634X","979-8-3315-0105-1","10.1109/FIE63693.2025.11328429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11328429","automated assessment;feedback;code quality;software testing and automation;large language models","Software testing;Java;Codes;Automation;Scalability;Large language models;Benchmark testing;Iterative methods","","","","28","IEEE","13 Jan 2026","2-5 Nov. 2025","2-5 Nov. 2025","IEEE","IEEE Conferences"
"LLM Prompt Engineering for Automated White-Box Integration Test Generation in REST APIs","A. M. Rincon; A. M. R. Vincenzi; J. P. Faria","Campus Paraíso do Tocantins, Federal Institute of Tocantins (IFTO), Paraíso do Tocantins, Brazil; Department of Computing (DC), Federal University of São Carlos (UFSCar), São Carlos, Brazil; INESC TEC, Faculty of Engineering, University of Porto, Porto, Portugal","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","21","28","This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90% coverage, although at higher costs. Additionally, combining test sets from different models increased coverage, reaching 96% in some cases. We also compared the results with EvoMaster, a specialized tool for generating tests for REST APIs, where LLM-generated tests achieved comparable or higher coverage in the benchmark projects. Despite higher execution costs, LLMs demonstrated superior adaptability and flexibility in test generation.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962507","Prompt Engineering;Large Language Models;Automated Integration Testing;White-Box Testing;REST APIs","Software testing;Costs;Codes;Large language models;Conferences;Restful API;Benchmark testing;Prompt engineering;Test pattern generators;Glass box","","1","","22","IEEE","16 Apr 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Evaluation of an agent-based reinforcement learning framework for model-based testing","J. Ramírez-Méndez; C. Quesada-López","Graduate Program in Computing and Informatics, University of Costa Rica, San Pedro, Costa Rica; Department of Computer Science and Informatics, University of Costa Rica, San Pedro, Costa Rica","2024 IEEE VII Congreso Internacional en Inteligencia Ambiental, Ingeniería de Software y Salud Electrónica y Móvil (AmITIC)","12 Nov 2024","2024","","","1","7","This study evaluates an agent-based reinforcement learning framework for model-based testing (MBT). The framework’s performance was assessed on three key metrics: effectiveness and efficiency in achieving model coverage objectives, the quantity and uniqueness of generated test cases, and code coverage. The results show improved metrics for the framework compared to traditional testers in model coverage evaluation and test case metrics. Specifically, the framework achieved higher effectiveness and efficiency, generating a higher average number of test cases with a substantial proportion of unique cases, indicating more diverse and thorough testing. Additionally, the framework, along with random and greedy testers, achieved over 70% coverage across branch, method, and line code metrics. The framework showed slightly higher values in method and line coverage compared to other testers. The evaluation highlights the use of agent-based reinforcement learning to support model-based testing in planning test case generation, exploring models using multiple strategies, and learning guided by testing metrics. Future work will focus on further refining the framework learning model, including error coverage evaluation, and testing its applicability across different software systems.","","979-8-3503-6453-8","10.1109/AmITIC62658.2024.10747593","Universidad de Costa Rica; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10747593","evaluation;model-based testing (MBT);agent-based software testing (ABST);reinforcement learning (RL)","Measurement;Software testing;Codes;Calculators;Refining;Reinforcement learning;Software systems;Planning;Testing","","1","","29","IEEE","12 Nov 2024","25-27 Sept. 2024","25-27 Sept. 2024","IEEE","IEEE Conferences"
"LLMLOOP: Improving LLM-Generated Code and Tests Through Automated Iterative Feedback Loops","R. Ravi; D. Bradshaw; S. Ruberto; G. Jahangirova; V. Terragni","University of Auckland, Auckland, New Zealand; University of Auckland, Auckland, New Zealand; JRC European Commission, Ispra, Italy; King's College London, London, United Kingdom; University of Auckland, Auckland, New Zealand",2025 IEEE International Conference on Software Maintenance and Evolution (ICSME),"31 Oct 2025","2025","","","930","934","Large Language Models (LLMs) are showing remarkable performance in generating source code, yet the generated code often has issues like compilation errors or incorrect code. Researchers and developers often face wasted effort in implementing checks and refining LLM-generated code, frequently duplicating their efforts. This paper presents LLMLOOP, a framework that automates the refinement of both source code and test cases produced by LLMs. LLMLOOP employs five iterative loops: resolving compilation errors, addressing static analysis issues, fixing test case failures, and improving test quality through mutation analysis. These loops ensure the generation of high-quality test cases that serve as both a validation mechanism and a regression test suite for the generated code. We evaluated llmloop on HumanEval-X, a recent benchmark of programming tasks. Results demonstrate the tool effectiveness in refining LLM-generated outputs. A demonstration video of the tool is available at https://youtu.be/2CLG9x1fsNI.","2576-3148","979-8-3315-9587-6","10.1109/ICSME64153.2025.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11185878","AI4SE;software testing;program synthesis;automated test generation;Large Language Models","Software testing;Feedback loop;Codes;Source coding;Large language models;Refining;Static analysis;Iterative methods;Test pattern generators;Videos","","2","","21","IEEE","31 Oct 2025","7-12 Sept. 2025","7-12 Sept. 2025","IEEE","IEEE Conferences"
"Fastbot: A Multi-Agent Model-Based Test Generation System : Beijing Bytedance Network Technology Co., Ltd.","T. Cai; Z. Zhang; P. Yang","Product RD & Infrastructure, Bytedance Network Technology, Beijing, China; Product RD & Infrastructure, Bytedance Network Technology, Beijing, China; Product RD & Infrastructure, Bytedance Network Technology, Beijing, China",2020 IEEE/ACM 15th International Conference on Automation of Software Test (AST),"24 Jul 2023","2020","","","93","96","Model-based test (MBT) generation techniques for automated GUI testing are of great value for app testing. Existing GUI model-based testing tools may fall into cyclic operations and run out of resources, when applied to apps with industrial complexity and scalability. In this work, we present a multi-agent GUI MBT system named Fastbot. Fastbot performs model construction on the server end. It applies multi-agent collaboration mechanism to speed up the model construction procedure. The proposed approach was applied on more than 20 applications from Bytedance with more than 1500 million monthly active users. Higher code coverage in less testing time is achieved with comparison of three other automated testing tools including Droidbot, Humanoid and Android Monkey.","2833-9061","978-1-4503-7957-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10186507","Model-based GUI testing;dynamic DAG exploration;multi-agent collaboration;automatic testing;traversal algorithm","Software testing;Heuristic algorithms;Software algorithms;Collaboration;Software;Test pattern generators;Servers","","6","","7","","24 Jul 2023","5-11 Oct. 2020","5-11 Oct. 2020","IEEE","IEEE Conferences"
"Large Language Model-Enhanced Test Case Generation*","Q. Zhang; R. Kang; Y. Liu; X. Cao","Software Product Research & Development Center, NARI Information & Communication Technology Co., Ltd., NanJing, China; Software Product Research & Development Center, NARI Information & Communication Technology Co., Ltd., NanJing, China; Software Product Research & Development Center, NARI Information & Communication Technology Co., Ltd., NanJing, China; Software Product Research & Development Center, NARI Information & Communication Technology Co., Ltd., NanJing, China","2025 4th International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)","20 Jan 2026","2025","","","793","797","Software testing is essential for ensuring software quality, with test case generation being a core yet labor-intensive task. Traditional methods often suffer from low efficiency, inadequate coverage, and poor adaptability to evolving requirements. Recent advances in Large Language Models (LLMs) offer promising capabilities for automated test generation through their proficiency in code understanding and synthesis. However, challenges such as hallucination, lack of domain-specific context, and output inconsistency limit their direct applicability. This paper analyzes the limitations of conventional testing approaches and proposes an improved LLM-based method that integrates structured prompt engineering with Retrieval-Augmented Generation (RAG). By retrieving relevant historical test cases and requirements, our approach enhances contextual awareness and guides the model to produce more accurate and semantically meaningful test cases. Experiments on real-world software modules evaluate the method in terms of code coverage, pass rate, and generation efficiency. Results demonstrate that the proposed approach significantly outperforms baseline methods, achieving higher test quality and reliability while reducing manual effort. This work highlights the effectiveness of context-aware prompting in leveraging LLMs for practical, intelligent test automation.","","979-8-3315-6491-9","10.1109/CBASE67452.2025.11335495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11335495","Large Language Models;Test Case Generation;Software Testing;Prompt Engineering;Retrieval-Augmented Generation","Software testing;Codes;Large language models;Retrieval augmented generation;Software quality;Manuals;Software reliability;Prompt engineering;Test pattern generators;Software engineering","","","","10","IEEE","20 Jan 2026","24-26 Oct. 2025","24-26 Oct. 2025","IEEE","IEEE Conferences"
"Evaluating Correct-Consistency and Robustness in Code-Generating LLMs","S. Honarvar","Department of Computing, Imperial College London, London, UK","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","797","800","Ensuring the reliability of large language models (LLMs) for code generation is crucial for their safe integration into software engineering practices, especially in safety-critical domains. Despite advances in LLM tuning, they frequently generate incorrect code, raising concerns about robustness and trustworthiness. This PhD research introduces Turbulence, a novel benchmark and evaluation framework designed to assess both the correct-consistency and robustness of LLMs through a structured coding question neighbourhood approach. By evaluating model performance across sets of semantically related but non-equivalent coding tasks, Turbulence identifies discontinuities in LLM generalisation, revealing patterns of success and failure that standard correctness evaluations often overlook. Applied to 22 instruction-tuned LLMs across Python coding question neighbourhoods, the benchmark highlights significant variability in correctness, including error patterns persisting even under deterministic settings. Future work will extend the question neighbourhood concept to Capture The Flag (CTF) challenges, enabling a deeper analysis of model reasoning capabilities in progressively complex tasks. This extension has attracted interest from the UK AI Safety Institute, which recognises the frame-work's potential for advancing rigorous evaluation methodologies in the context of safe and trusted AI for software engineering.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10988971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10988971","Large Language Models;Correct-consistency;Robustness;Evaluation;Code Generation","Software testing;Codes;Large language models;Benchmark testing;Robustness;Encoding;Software reliability;Tuning;Standards;Software engineering","","","","39","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Automated Functionality and Security Evaluation of Large Language Models","M. Ding; Y. Shen; M. Chen","Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China; Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology, Shanghai, China",2024 9th IEEE International Conference on Smart Cloud (SmartCloud),"24 Jun 2024","2024","","","37","41","Natural language processing (NLP) is rapidly developing. A series of Large Language Models (LLMs) have emerged, represented by ChatGPT, which have made significant breakthroughs in natural language understanding and generation, enabling fluent dialogue with humans, understanding human intentions, and completing complex tasks. However, in addition to the fairness and toxicity of traditional language models, some new problems, including hallucination, have also emerged in LLMs, making them hard to use. Evaluating LLMs manually is challenging due to subjectivity and inefficiency. In this paper, we focused on the fuzzy matching, toxicity detection, and hallucination detection in the evaluation of LLMs automatically, and fine-tune the Mixtral-8x7B Model, which can be deployed in private cloud environment, and prove the effectiveness of our method through experiments.","","979-8-3503-8950-0","10.1109/SmartCloud62736.2024.00014","Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10566308","LLM;evaluation;fuzzy matching;toxicity;hallucination","Cloud computing;Toxicology;Accuracy;Graphics processing units;Chatbots;Security;Task analysis","","2","","23","IEEE","24 Jun 2024","10-12 May 2024","10-12 May 2024","IEEE","IEEE Conferences"
"Evaluating the Effectiveness of Neuroevolution for Automated GUI-Based Software Testing","D. Zimmermann; P. Deubel; A. Koziolek","Software Engineering (SE), FZI Research Center for Information Technology, Karlsruhe, Germany; Software Engineering (SE), FZI Research Center for Information Technology, Karlsruhe, Germany; KASTEL - Institute of Information Security and Dependability, Karlsruhe Institute of Technology, Karlsruhe, Germany",2023 38th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),"2 Nov 2023","2023","","","119","126","As software systems become increasingly complex, testing has become an essential component of the development process to ensure the quality of the final product. However, manual testing can be costly and time-consuming due to the need for human intervention. This constrains the number of test cases that can be run within a given timeframe and, as a result, limits the ability to detect defects in software in a timely manner. Automated testing, on the other hand, can reduce the cost and time associated with testing, but traditional approaches have limitations. These include the inability to thoroughly explore the entire state space of software or process the high-dimensional input space of graphical user interfaces (GUIs). In this study, we propose a new approach for automated GUI-based software testing utilizing neuroevolution (NE), a branch of machine learning that employs evolutionary algorithms to train artificial neural networks with multiple hidden layers of neurons. NE offers a scalable alternative to established deep reinforcement learning methods and provides higher robustness to parameter influences and improved handling of sparse rewards. The agents are trained to explore software and identify errors while being rewarded for high test coverage. We evaluate our approach using a realistic benchmark software application and compare it to monkey testing, a widely adopted automated software testing method.","2151-0849","979-8-3503-3032-8","10.1109/ASEW60602.2023.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298672","UI Testing;Test Automation;Deep Learning;Neuroevolution","Software testing;Training;Neurons;Reinforcement learning;Parallel processing;Software systems;Robustness","","2","","17","IEEE","2 Nov 2023","11-15 Sept. 2023","11-15 Sept. 2023","IEEE","IEEE Conferences"
"Improving the Readability of Automatically Generated Tests Using Large Language Models","M. Biagiola; G. Ghislotti; P. Tonella","Software Institute - Università della Svizzera italiana, Lugano, Switzerland; Software Institute - Università della Svizzera italiana, Lugano, Switzerland; Software Institute - Università della Svizzera italiana, Lugano, Switzerland","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","162","173","Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989020","Large Language Models;Software Testing;Readability","Software testing;Hands;Codes;Large language models;Semantics;Generators","","1","","59","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Evaluating Large Language Model Robustness using Combinatorial Testing","J. Chandrasekaran; A. R. Patel; E. Lanus; L. J. Freeman","Sanghani Center for Artificial Intelligence & Data Analytics, Virginia Tech, Arlington, VA, USA; NA; National Security Institute, Virginia Tech, Arlington, VA, USA; National Security Institute, Virginia Tech, Arlington, VA, USA","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","300","309","Recent advancements in large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like text, leading to widespread adoption across domains. Given LLM’s versatile capabilities, current evaluation practices assess LLMs across a wide variety of tasks, including answer generation, sentiment analysis, text completion, and question and answers, to name a few. Multiple choice questions (MCQ) have emerged as a widely used evaluation task to assess LLM’s understanding and reasoning across various subject areas. However, studies from the literature have revealed that LLMs exhibit sensitivity to the ordering of options in MCQ tasks, with performance variations based on option sequence, thus underscoring the robustness concerns in LLM performance.This work presents a combinatorial testing-based framework for systematic and comprehensive robustness assessment of pre-trained LLMs. By leveraging the sequence covering array, the framework constructs test sets by systematically swapping the order of options, which are then used in ascertaining the robustness of LLMs. We performed an experimental evaluation using the Measuring Massive Multitask Language Understanding (MMLU) dataset, a widely used MCQ dataset and evaluated the robustness of GPT 3.5 Turbo, a pre-trained LLM. Results suggest the framework can effectively identify numerous robustness issues with a relatively minimal number of tests.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962520","U.S. Department of Defense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962520","Testing AI;Combinatorial Testing;Testing LLM;LLM Robustness;LLM Evaluation;Option Order Swapping","Sentiment analysis;Systematics;Sensitivity;Large language models;Combinatorial testing;Conferences;Robustness;Cognition","","2","","30","IEEE","16 Apr 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs","Z. Li; D. Shin","University of Sheffield, Sheffield, UK; University of Sheffield, Sheffield, UK",2024 IEEE/ACM 3rd International Conference on AI Engineering – Software Engineering for AI (CAIN),"18 Jun 2024","2024","","","150","159","Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.CCS CONCEPTS• Software and its engineering → Software testing and debug-ging; Empirical software validation.","","979-8-4007-0591-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556138","Large Language Models;Software Engineering;Mutation Analysis","Software testing;Training;Analytical models;Codes;Sensitivity;Semantics;Benchmark testing","","2","","47","","18 Jun 2024","14-15 April 2024","14-15 April 2024","IEEE","IEEE Conferences"
"Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents","J. Yoon; R. Feldt; S. Yoo","School of Computing, KAIST, Daejeon, Republic of Korea; Dept. of Computer Science & Engineering, Chalmers University, Gothenburg, Sweden; School of Computing, KAIST, Daejeon, Republic of Korea","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","27 Aug 2024","2024","","","129","139","GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51 % for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.","2159-4848","979-8-3503-0818-1","10.1109/ICST60714.2024.00020","National Research Foundation of Korea (NRF); Korean Government MSIT(grant numbers:RS-2023-00208998,2022-0-00995); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638557","software testing;GUI testing;test automation;artificial intelligence;large language model","Software testing;Measurement;Automation;Large language models;Semantics;Manuals;Software systems","","14","","36","IEEE","27 Aug 2024","27-31 May 2024","27-31 May 2024","IEEE","IEEE Conferences"
"Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code","S. Honarvar; M. van der Wilk; A. F. Donaldson","Department of Computing, Imperial College London, London, UK; Department of Computer Science, University of Oxford, Oxford, UK; Department of Computing, Imperial College London, London, UK","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","80","91","We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language question templates, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated test oracle that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a neighbourhood of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including anomalies where the LLM correctly solves almost all questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Cohere and Meta, each at two temperature configurations. Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond merely highlighting that LLMs sometimes produce wrong code (which is no surprise): by systematically identifying cases where LLMs are able to solve some problems in a neighbourhood but do not manage to generalise to solve the whole neighbourhood, our method is effective at highlighting robustness issues. We present data and examples that shed light on the kinds of mistakes that LLMs make when they return incorrect code results.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989005","Large language models;correctness;robustness;AI evaluation;code generation","Software testing;Codes;Large language models;Natural languages;Programming;Benchmark testing;Robustness;Cognition","","1","","71","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Using Reinforcement Learning for Security Testing: A Systematic Mapping Study","T. Ahmad; M. Butkovic; D. Truscan","Dept. of Information Technology, Åbo Akademi University, Turku, Finland; Dept. of Information Technology, Åbo Akademi University, Turku, Finland; Dept. of Information Technology, Åbo Akademi University, Turku, Finland","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","208","216","Security of software systems has become increasingly important due to the advancement in technology that occurs on a daily basis and due to the interconnectivity that the Internet network system provides. The manual testing process is time-consuming process and inefficient, especially for very large and complex systems. Reinforcement learning has shown promising results in different test generation approaches due to its ability to optimize the test generation process towards relevant parts of the system. A considerable body of work has been developed in recent years to exploit reinforcement learning for security test generation. This study provides a list of approaches and tools for security test generation using Reinforcement Learning (RL). By searching popular research publication databases, a list of 47 relevant studies has been identified and classified according to the type of approach, RL algorithm, application domain and publication metadata.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962455","Business Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962455","security testing;test generation;AI-assisted software testing;reinforcement learning;systematic mapping study","Software testing;Systematics;Software algorithms;Reinforcement learning;Software systems;Robustness;Classification algorithms;Security;Test pattern generators;Testing","","1","","70","IEEE","16 Apr 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Assured LLM-Based Software Engineering","N. Alshahwan; M. Harman; I. Harper; A. Marginean; S. Sengupta; E. Wang","Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA; Meta Platforms Inc., Menlo Park, California, USA","2024 IEEE/ACM 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE)","10 Sep 2024","2024","","","7","12","In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.","","979-8-4007-0564-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10669832","Large Language Models (LLMs);Genetic Improvement (GI);Search Based Software Engineering (SBSE);Llama;CodeLlama;Automated Code Generation","Codes;Filters;Large language models;Conferences;Semantics;Benchmark testing;Genetics","","","","41","","10 Sep 2024","15-15 April 2024","15-15 April 2024","IEEE","IEEE Conferences"
"Business Scenario Driven Reinforcement Learning Testing Method","L. Cai; J. Wang","Shanghai Key Laboratory of Computer, Software Testing and Evaluating, School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; Shanghai Key Laboratory of Computer, Software Testing and Evaluating, School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)","28 Aug 2023","2023","","","158","165","Reinforcement learning has been successfully applied in software testing, but the existing testing methods cannot perform effective testing according to the characteristics of applications, and using outdated interactive experience during training, resulting in inefficient testing. In this paper, we propose BSDRTesting. Firstly, the demonstration experience of human users is collected according to the functional scenarios and business logic of each application, and combining reinforcement learning and imitation learning to maximize rewards while imitating user behavior, experience replay aims to sample experiences from the agent's self-exploration and expert demonstrations to improve sampling efficiency. At the same time, according to the input rules, the black-box testing method is used to fully test the input events, and finally an experience filtering mechanism is proposed, and the reward value and TD-Error are used as the basis for priority sampling. The experimental results on 10 open source applications show BSDRTesting has achieved significant improvements in code coverage and branch coverage compared with existing methods.","","979-8-3503-4586-5","10.1109/SNPD-Winter57765.2023.10223736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223736","Business Scenario;Reinforcement Learning;Testing Method","Training;Software testing;Codes;Filtering;Closed box;Reinforcement learning;Behavioral sciences","","","","20","IEEE","28 Aug 2023","5-7 July 2023","5-7 July 2023","IEEE","IEEE Conferences"
"ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation","Y. Tang; Z. Liu; Z. Zhou; X. Luo","University of Glasgow, Glasgow, U.K.; ShanghaiTech University, Shanghai, China; ShanghaiTech University, Shanghai, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong SAR, China",IEEE Transactions on Software Engineering,"14 Jun 2024","2024","50","6","1340","1359","Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.","1939-3520","","10.1109/TSE.2024.3382365","Hong Kong RGC Project(grant numbers:PolyU15224121); HKPolyU(grant numbers:ZGGG); National Natural Science Foundation of China(grant numbers:62202306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485640","ChatGPT;search-based software testing;large language models","Chatbots;Codes;Task analysis;Software;Question answering (information retrieval);Computer bugs;Benchmark testing","","48","","86","IEEE","29 Mar 2024","June 2024","","IEEE","IEEE Journals"
"Adaptive Random Testing for Multiagent Path Finding Systems","Y. Liu; X. -Y. Zhang","Beihang University, Beijing, China; National Institute of Informatics, Tokyo, Japan",IEEE Transactions on Reliability,"2 Mar 2022","2022","71","1","295","308","The multiagent path finding (MAPF) problem identifies the scheduling of multiple agents simultaneously, such that all of them can reach their targets efficiently. To date, MAPF systems have been assigned important tasks such as traffics and warehouses. It is essential to conduct testing for MAPF systems to detect potential failures. Namely, in an MAPF system, a test case is a specific MAPF scenario, including the initial locations of the agents and the environment for these agents to play in. By testing, we intend to find the scenarios (i.e., test cases) whose executions reveal failures. Testing MAPF systems is challenging due to the complexity of its input and the interactions among multiple agents. This article proposes the testing approach based on the adaptive random testing (ART) for MAPF systems. ART aims to generate new test cases far from the already executed ones. Particularly, to calculate the distance between each pair of test cases, we introduce two metrics, the initial density distribution and the destination density distribution, to characterize the distribution of the agents’ initial and destination nodes, respectively. Benefit from ART, the diversity of the information generated during testing can be improved. Experimental results show that compared with the random testing, our approach can detect more diverse failure-revealing scenarios.","1558-1721","","10.1109/TR.2022.3146323","ERATO HASUO Metamathematics for Systems Design(grant numbers:JPMJER1603); JST; Engineerable AI Techniques for the Practical Applications of High-Quality Machine Learning-based Systems(grant numbers:JPMJMI20B8); JST-Mirai Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725384","Adaptive random testing (ART);density measurement;multiagent path finding (MAPF);multiagent systems;software testing","Measurement;Adaptive systems;Job shop scheduling;Scalability;Subspace constraints;Benchmark testing;Planning","","","","49","IEEE","2 Mar 2022","March 2022","","IEEE","IEEE Journals"
"Human vs. computer behavior in multi-issue negotiation","T. Bosse; C. M. Jonker","Department of Artificial Intelligence, Vrije Universiteit Amsterdam; Nijmegen Institute for Cognition and Information, Radboud Universiteit Nijmegen, Netherlands","Rational, Robust, and Secure Negotiation Mechanisms in Multi-Agent Systems (RRS'05)","6 Feb 2006","2005","","","11","24","This paper presents two experiments that contribute to the comparison of human- versus computer behavior in the domain of multi-issue negotiation. The experiments are part of an ongoing endeavor of improving the quality of computer negotiators when negotiating against human negotiators. The validity of the experiments was tested in a case study of closed multi-issue negotiation involving the ABMP negotiation software agents. The results indeed reveal a number of strengths and weaknesses of the ABMP agents. For example, the fairness of deals in negotiations performed purely by ABMP agents is better than the fairness of deals in the comparable negotiations in which humans were involved. Furthermore, in mixed negotiations (i.e., involving human- and software agents) the humans outperform the software agent with respect to the individual performance. Based on the results of the experiments, several suggestions are made to improve the ABMP agent's performance.","","0-7695-2480-X","10.1109/RRS.2005.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587845","","Humans;Software agents;Benchmark testing;Artificial intelligence;Cognition;Software testing;Software design;Software tools;Software libraries;System testing","","28","","15","IEEE","6 Feb 2006","25-25 July 2005","25-25 July 2005","IEEE","IEEE Conferences"
"Impact of Large Language Models of Code on Fault Localization","S. Ji; S. Lee; C. Lee; Y. -S. Han; H. Im","Yonsei University, Seoul, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea; Yonsei University, Seoul, Republic of Korea; Kangwon National University, Chuncheon, Republic of Korea","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","302","313","Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each method or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest. In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune 13 representative encoder, encoder-decoder, and decoder-based LLMCs (across 7 different architectures) for FL tasks. Unlike previous approaches, LLM Cs can analyze code sequences that do not compile. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6%, 64.2%, and 72.3% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. We also conduct an in-depth investigation of key factors that may affect the FL performance of LLMCs. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989036","Fault Localization;Vulnerability Detection;Large Language Model of Code;Fine-Tuning;Deep Learning","Location awareness;Software testing;Codes;Large language models;Source coding;Computer architecture;Benchmark testing;Feature extraction;Software debugging;Software engineering","","4","","69","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models","C. Lemieux; J. P. Inala; S. K. Lahiri; S. Sen","University of British Columbia, Canada; Microsoft Research, USA; Microsoft Research, USA; Microsoft Research, USA",2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE),"14 Jul 2023","2023","","","919","931","Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.","1558-1225","978-1-6654-5701-9","10.1109/ICSE48619.2023.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172800","search based software testing;codex;test suite generation;python;large language model;automated testing","Software testing;Codes;Benchmark testing;Software;Space exploration;Test pattern generators;Software engineering","","192","","62","IEEE","14 Jul 2023","14-20 May 2023","14-20 May 2023","IEEE","IEEE Conferences"
"Testing Open-World Games: A Minecraft Case Study","Q. Chen; M. Zhang; X. Zhang","University of Science and Technology Beijing, Beijing, China; Southwest University, Chongqing, China; University of Science and Technology Beijing, Beijing, China",2025 IEEE 36th International Symposium on Software Reliability Engineering Workshops (ISSREW),"1 Dec 2025","2025","","","01","08","With the ever-increasing demand for games, game engines continue to upgrade and become more complex. On the other hand, bugs are detrimental to games, affecting players' experiences, causing imbalances, and even forcing players to withdraw. Thus, game testing is an essential but challenging task. Particularly, as a popular genre, open-world games are defined by their emphasis on player autonomy, allowing individuals to choose what to engage with and shape their own experiences. This high degree of freedom results in significant differences in player behaviours, which brings much complexity for game testers to explore valuable game scenarios and hence detect potential failures or requirement violations. In this paper, we focus on the oracle problem in testing open-world games. Indeed, since open-world games strive to independently construct a self-contained logical system with coherent operational rules, it is challenging to formulate a comprehensive specification that completely describes the requirements and then identify failures during testing. To address these issues, we examine the underlying logic of open-world games by comparing the physical rules and theories of the real world with those of the game world. Intuitively, although open worlds possess their own physical and natural laws, if these laws deviate significantly from those of the real world, they may also be inconsistent with players' expectations and thus violate the requirements. Based on this, we take the gravity and aggro mechanics as examples, using Minecraft as a benchmark, and generate test cases through the boundary testing and metamorphic testing methods, respectively. The testing results show that we can generate interesting test cases that violate real-world rules and provide guidance for further analysis.","2994-810X","979-8-3315-5325-8","10.1109/ISSREW67781.2025.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11262358","Game testing;open-world game;Minecraft;software testing","Software testing;Hands;Shape;Computer bugs;Games;Software reliability;Logic;Engines;Testing;Gravity","","","","24","IEEE","1 Dec 2025","21-21 Oct. 2025","21-21 Oct. 2025","IEEE","IEEE Conferences"
"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction","S. Kang; J. Yoon; S. Yoo","School of Computing, KAIST, Daejeon, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea",2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE),"14 Jul 2023","2023","","","2312","2323","Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.","1558-1225","978-1-6654-5701-9","10.1109/ICSE48619.2023.00194","National Research Foundation of Korea (NRF)(grant numbers:NRF-2020R1A2C1013629,NRF-2018R1A5A1059921); Institute for Information & Communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172763","test generation;natural language processing;software engineering","Codes;Computer bugs;Semantics;Training data;Benchmark testing;Writing;Test pattern generators","","135","","42","IEEE","14 Jul 2023","14-20 May 2023","14-20 May 2023","IEEE","IEEE Conferences"
"Fairness Testing of Machine Learning Models Using Deep Reinforcement Learning","W. Xie; P. Wu","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China","2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","9 Feb 2021","2020","","","121","128","Machine learning models play an important role for decision-making systems in areas such as hiring, insurance, and predictive policing. However, it still remains a challenge to guarantee their trustworthiness. Fairness is one of the most critical properties of these machine learning models, while individual discriminatory cases may break the trustworthiness of these systems severely. In this paper, we present a systematic approach of testing the fairness of a machine learning model, with individual discriminatory inputs generated automatically in an adaptive manner based on the state-of-the-art deep reinforcement learning techniques. Our approach can explore and exploit the input space efficiently, and find more individual discriminatory inputs within less time consumption. Case studies with typical benchmark models demonstrate the effectiveness and efficiency of our approach, compared to the state-of-the-art black-box fairness testing approaches.","2324-9013","978-1-6654-0392-4","10.1109/TrustCom50675.2020.00029","National Key R&D Program of China(grant numbers:2017YFB0801900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343006","fairness testing;black-box testing;deep reinforcement learning;test data generation;machine learning models","Adaptation models;Systematics;Computational modeling;Reinforcement learning;Benchmark testing;Data models;Test pattern generators","","7","","30","IEEE","9 Feb 2021","29 Dec.-1 Jan. 2021","29 Dec.-1 Jan. 2021","IEEE","IEEE Conferences"
"Learning Environment Models with Continuous Stochastic Dynamics - with an Application to Deep RL Testing","M. Tappler; E. Muškardin; B. K. Aichernig; B. Könighofer","DES Lab, Silicon Austria Labs, Institute of Software Technology, Graz University of Technology, Graz, Austria; DES Lab, Silicon Austria Labs, Institute of Software Technology, Graz University of Technology, Graz, Austria; Institute of Software Technology, Graz University of Technology, Graz, Austria; Institute of Applied Information Processing and Communications, Graz University of Technology, Graz, Austria","2024 IEEE Conference on Software Testing, Verification and Validation (ICST)","27 Aug 2024","2024","","","197","208","Techniques like deep reinforcement learning (DRL) enable autonomous agents to solve tasks in complex environments automatically through learning. Despite their potential, neural-network-based decision-making policies are hard to understand and test. To ease the adoption of such techniques, we learn automata models of environmental behavior under the control of an agent. These models provide insights into the decisions faced by agents and a basis for testing. To scale automata learning to environments with complex and continuous dynamics, we compute an abstract state-space representation through dimensionality reduction and clustering of observed environmental states. The stochastic transitions are learned via passive automata learning from agent-environment interactions. Furthermore, we iteratively sample additional tra-jectories to enhance the learned model's accuracy. We demonstrate the potential of our automata learning frame-work by (1) solving popular RL benchmark problems and (2) applying it for differential testing of DRL agents. Our results show that the learned models are sufficiently precise to compute policies that solve the respective control tasks. Yet the models are sufficiently general for coverage-guided testing, where we reveal significant differences in the functional failure frequency of pairs of DRL agents.","2159-4848","979-8-3503-0818-1","10.1109/ICST60714.2024.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638603","Automata Learning;Markov Decision Processes;Reinforcement Learning;Differential Testing","Dimensionality reduction;Software testing;Runtime;Computational modeling;Learning automata;Stochastic processes;Probabilistic logic;Trajectory;Safety;Testing","","1","","55","IEEE","27 Aug 2024","27-31 May 2024","27-31 May 2024","IEEE","IEEE Conferences"
"Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation","G. Black; V. Mathew Vaidyan; G. Comert","Department of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Department of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Benedict College, Columbia, SC, USA",IEEE Access,"29 Oct 2024","2024","12","","156065","156081","Fuzzing is a crucial technique for detecting software defects by dynamically generating and testing program inputs. This study introduces a framework designed to assess the application of Large Language Models (LLMs) to automate the generation of effective seed inputs for fuzzing, particularly in the Python programming environment where traditional approaches are less effective. Utilizing the Atheris fuzzing framework, we created over 38,000 seed inputs from LLMs targeted at 50 Python functions from widely-used libraries. Our findings underscore the critical role of LLM selection in seed effectiveness. In certain cases, seeds generated by LLMs rivaled or surpassed traditional fuzzing campaigns, with a corpus of fewer than 100 LLM-generated entries outperforming over 100,000 conventionally produced inputs. These seeds significantly improved code coverage and instruction count during fuzzing sessions, illustrating the efficacy of our framework in facilitating an automated, scalable approach to evaluating LLM effectiveness. The results, validated through linear regression analysis, demonstrate that selecting the appropriate LLM based on its training and capabilities is essential for optimizing fuzzing efficiency and facilitates the testing of future LLM versions.","2169-3536","","10.1109/ACCESS.2024.3484947","National Center for Transportation Cybersecurity and Resiliency (TraCR), USA, headquartered at Clemson University, Clemson, SC, USA; Cyber-Ag-Law Research Collaborations with the Department of Energy Minority Serving Institutions Partnership Program (MSIPP) managed by the Savannah River National Laboratory under Battelle Savannah River Alliance (BSRA)(grant numbers:TOA 0000525174 CN1); Minority Science and Engineering Improvement Program (MSEIP) II Cyber, USA(grant numbers:P120A190061,P120A210048,FM-MHP-0678-22-01-00); National Science Foundation (NSF), USA(grant numbers:1954532,2131080,2200457,OIA-2242812,2234920,2305470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10731701","Fuzzing;machine learning;large language models;python","Fuzzing;Testing;Large language models;Python;Codes;Security;Training;Protocols;Measurement;Machine learning","","5","","52","CCBYNCND","23 Oct 2024","2024","","IEEE","IEEE Journals"
"Rug: Turbo Llm for Rust Unit Test Generation","X. Cheng; F. Sang; Y. Zhai; X. Zhang; T. Kim",Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; George Mason University; Georgia Institute of Technology,2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","2983","2995","Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, large language models (LLMs) have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLMs with a basic prompt like “generate unit test for the following source code” often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average $24,937 \text{LoC}$), we show that RUG can achieve a high code coverage, up to $\mathbf{7 1. 3 7 \%}$, closely comparable to human effort $(\mathbf{7 3. 1 8 \%})$. We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00097","NSF(grant numbers:CNS-1749711); ONR(grant numbers:N00014-23-1-2095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029738","Unit testing;Large language model;Rust","Codes;Reviews;Large language models;Source coding;Scalability;Software quality;Fuzzing;Test pattern generators;Testing;Software engineering","","2","","71","IEEE","23 Jun 2025","26 April-6 May 2025","26 April-6 May 2025","IEEE","IEEE Conferences"
"Evaluating Large Language Models Via Multi-Modal User Knowledge Graphs: A Comprehensive Assessment Framework","P. Liu; Z. Chen; Y. Li; W. E. Wong","Faculty of Business Information, Shanghai Business School, Shanghai; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; China School of Information and Electrical Engineering, Ludong University, Yantai, China; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA","2025 11th International Symposium on System Security, Safety, and Reliability (ISSSR)","9 Jun 2025","2025","","","278","285","Large language models (LLMs) have been widely adopted across various industries, but issues such as hallucinations, biases, and erroneous outputs frequently arise, compromising their reliability and safety. Objectively assessing how well an LLM interprets user queries is crucial for selecting the right model for practical problem-solving. This paper proposes an evaluation method that leverages user knowledge graphs to measure an LLM's comprehension of user input. Specifically, we construct both text-based and graphical test cases derived from a user knowledge graph, thereby enabling multi-modal assessment of the LLM's understanding. To implement our approach, a knowledge graph is first built from the user's input and then mutated to produce diverse test cases. A search-based testing method is then applied to evaluate the model's comprehension. We provide a case study demonstrating the framework. Our findings indicate that multi-modal test cases outperform purely text-based test cases in revealing the true understanding capability of LLMs. Among the eight models tested, DeepSeek and Doubao exhibit stronger comprehension than the remaining six.","2835-2823","979-8-3315-0124-2","10.1109/ISSSR65654.2025.00047","Natural Science Foundation of Shanghai(grant numbers:23ZR1445900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11021674","User knowledge graph;LLM;Understanding ability evaluation;Multi-modal test case;Knowledge graph mutation;","Industries;Visualization;Large language models;Knowledge graphs;Cognition;Robustness;Safety;Complexity theory;Security;Testing","","3","","22","IEEE","9 Jun 2025","12-13 April 2025","12-13 April 2025","IEEE","IEEE Conferences"
"Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases","H. Tanaka; H. Tanaka; K. Shimari; K. Matsumoto","Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan",2025 2nd IEEE/ACM International Conference on AI-powered Software (AIware),"19 Jan 2026","2025","","","11","18","As Large Language Models (LLMs) increasingly generate code in software development, ensuring the quality of LLM-generated code has become important. Traditional testing approaches using Example-based Testing (EBT) often miss edge cases—defects that occur at boundary values, special input patterns, or extreme conditions. This research investigates the characteristics of LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge cases. We analyze 16 HumanEval problems where standard solutions failed on extended test cases, generating both PBT and EBT test codes using Claude-4-sonnet. Our experimental results reveal that while each method individually achieved a 68.75% bug detection rate, combining both approaches improved detection to $81.25\%$. The analysis demonstrates complementary characteristics: PBT effectively detects performance issues and edge cases through extensive input space exploration, while EBT effectively detects specific boundary conditions and special patterns. These findings suggest that a hybrid approach leveraging both testing methods can improve the reliability of LLM-generated code, providing guidance for test generation strategies in LLM-based code generation.","","979-8-3315-8269-2","10.1109/AIware69974.2025.00009","JSPS(grant numbers:JP23K16862); KAKENHI(grant numbers:JP23K16862); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11334197","Software Testing;Code Generation;Large Language Models","Codes;Systematics;Image edge detection;Large language models;Computer bugs;Boundary conditions;Space exploration;Test pattern generators;Standards;Testing","","","","17","IEEE","19 Jan 2026","19-20 Nov. 2025","19-20 Nov. 2025","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Generating Test Cases for Web Applications","X. Chang; Z. Liang; Y. Zhang; L. Cui; Z. Long; G. Wu; Y. Gao; W. Chen; J. Wei; T. Huang","State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; Joint Laboratory on Cyberspace Security, China Southern Power Grid, Guangzhou, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, Beijing, China",2023 IEEE/ACM International Conference on Automation of Software Test (AST),"12 Jul 2023","2023","","","13","23","Web applications play an important role in modern society. Quality assurance of web applications requires lots of manual efforts. In this paper, we propose WebQT, an automatic test case generator for web applications based on reinforcement learning. Specifically, to increase testing efficiency, we design a new reward model, which encourages the agent to mimic human testers to interact with the web applications. To alleviate the problem of state redundancy, we further propose a novel state abstraction technique, which can identify different web pages with the same functionality as the same state, and yields a simplified state space. We evaluate WebQT on seven open-source web applications. The experimental results show that WebQT achieves 45.4% more code coverage along with higher efficiency than the state-of-the-art technique. In addition, WebQT also reveals 69 exceptions in 11 real-world web applications.","2833-9061","979-8-3503-2402-0","10.1109/AST58925.2023.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173983","State exploration;Reinforcement learning;Software testing","Software testing;Quality assurance;Codes;Automation;Redundancy;Web pages;Reinforcement learning","","4","","40","IEEE","12 Jul 2023","15-16 May 2023","15-16 May 2023","IEEE","IEEE Conferences"
"Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback","S. Lin",Independent Researcher,2025 Cyber Awareness and Research Symposium (CARS),"21 Jan 2026","2025","","","1","9","Software fuzzing has become a cornerstone in automated vulnerability discovery, yet existing mutation strategies often lack semantic awareness, leading to redundant test cases and slow exploration of deep program states. In this work, I present a hybrid fuzzing framework that integrates static and dynamic analysis with Large Language Model (LLM)-guided input mutation and semantic feedback. Static analysis extracts control-flow and data-flow information, which is transformed into structured prompts for the LLM to generate syntactically valid and semantically diverse inputs. During execution, I augment traditional coverage-based feedback with semantic feedback signals–derived from program state changes, exception types, and output semantics–allowing the fuzzer to prioritize inputs that trigger novel program behaviors beyond mere code coverage. I implement our approach atop AFL++, combining program instrumentation with embedding-based semantic similarity metrics to guide seed selection. Evaluation on real-world open-source targets, including libpng, tcpdump, and sqlite, demonstrates that our method achieves faster time-to-first-bug, higher semantic diversity, and a competitive number of unique bugs compared to state-of-the-art fuzzers. This work highlights the potential of combining LLM reasoning with semantic-aware feedback to accelerate and deepen vulnerability discovery.","","979-8-3315-9628-6","10.1109/CARS67163.2025.11337923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11337923","Hybrid Fuzzing;Large Language Models;Input Mutation;Semantic Feedback;Software Vulnerability Discovery;Automated Software Testing","Measurement;Large language models;Instruments;Semantics;Diversity reception;Computer bugs;Static analysis;Fuzzing;Software;Data mining","","","","25","IEEE","21 Jan 2026","27-30 Oct. 2025","27-30 Oct. 2025","IEEE","IEEE Conferences"
"Large Language Models for Software Engineering: Survey and Open Problems","A. Fan; B. Gokkaya; M. Harman; M. Lyubarskiy; S. Sengupta; S. Yoo; J. M. Zhang","Generative AI Team Meta Platforms Inc., New York, NY, USA; PyTorch Team Meta Platforms Inc., Menlo Park, CA, USA; Instagram Product Foundation Meta Platforms Inc., London, UK; Developer Infrastructure Meta Platforms Inc., London, UK; FAIR Meta Platforms Inc., Menlo Park, CA, USA; School of Computing KAIST, Daejeon, Korea; Department of Informatics, King's College London, London, UK",2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE),"4 Mar 2024","2023","","","31","53","This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.","","979-8-3503-2496-9","10.1109/ICSE-FoSE59343.2023.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10449667","Automated Program Repair;Documentation generation;Generative AI;Genetic Improvement;Human-Computer Interaction;Large Language Models;Refactoring;Requirements engineering;Search Based Software Engineering (SBSE);Software Analytics;Software Engineering Education;Software Processes;Software Maintenance and Evolution;Software Testing","Surveys;Maintenance engineering;Reliability engineering;Software;Software reliability;Software engineering;Testing","","300","","236","IEEE","4 Mar 2024","14-20 May 2023","14-20 May 2023","IEEE","IEEE Conferences"
"RoboCup: robot world cup","H. Kitano; M. Asada; I. Noda; H. Matsubara","Sony Computer Science Laboratories, Inc., Shinagawa, Tokyo, Japan; Department of Mechancical Engineering, Osaka University, Suita, Osaka, Japan; Electro Technical Laboratory, Tsukuba, Japan; Electro Technical Laboratory, Tsukuba, Japan",IEEE Robotics & Automation Magazine,"6 Aug 2002","1998","5","3","30","36","RoboCup is an attempt to foster intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined. The First Robot World Cup Soccer Games and Conferences (RoboCup-97) was held during IJCAI-97, Nagoya, with over 40 teams participating from throughout the world. RoboCup soccer is a task for a team of fast-moving robots in a dynamic, noisy environment. In order for a robot team to actually perform a soccer game, various technologies must be incorporated including: design principles of autonomous agents, multi-agent collaboration, strategy acquisition, real-time reasoning, robotics, and sensor-fusion. This article describes technical challenges involved in RoboCup, its official rules, a report of RoboCup-97, and future perspectives.","1558-223X","","10.1109/100.728221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=728221","","Intelligent robots;Robot sensing systems;Intelligent agent;Intelligent sensors;Educational robots;Autonomous agents;Software standards;Educational programs;Benchmark testing;Software testing","","80","","7","IEEE","6 Aug 2002","Sept. 1998","","IEEE","IEEE Magazines"
"Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging","P. Straubinger; M. Kreis; S. Lukasczyk; G. Fraser","University of Passau, Passau, Germany; University of Passau, Passau, Germany; JetBrains Research, Munich, Germany; University of Passau, Passau, Germany","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","358","367","Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962485","Large Language Models;Test Generation;Mutation Testing;Scientific Debugging","Codes;Costs;Large language models;Fault detection;Computational modeling;Debugging;Software;Cognition;Test pattern generators;Iterative methods","","2","","47","IEEE","16 Apr 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"The Impact of Prompt Programming on Function-Level Code Generation","R. Khojah; F. G. de Oliveira Neto; M. Mohamad; P. Leitner","Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden; Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden; Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden; Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden",IEEE Transactions on Software Engineering,"15 Aug 2025","2025","51","8","2381","2395","Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. While some prompt techniques have been studied, the impact of different techniques — and their interactions — on code generation is still not fully understood. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.","1939-3520","","10.1109/TSE.2025.3587794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11077752","Large language models;prompt programming;code generation","Codes;Software;Programming;Few shot learning;Benchmark testing;Accuracy;Training;Software engineering;Prompt engineering;Encoding","","3","","53","CCBY","10 Jul 2025","Aug. 2025","","IEEE","IEEE Journals"
"SpecGen: Automated Generation of Formal Program Specifications via Large Language Models","L. Ma; S. Liu; Y. Li; X. Xie; L. Bu","State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China; State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China; Nanyang Technological University, Singapore; Singapore Management University, Singapore; State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","16","28","In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00129","Nanjing University; National Natural Science Foundation of China(grant numbers:62232008,62172200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029962","program verification;specification inference;large language model","Software testing;Java;Codes;Large language models;Semantics;Benchmark testing;Software;Grammar;Software engineering;Software development management","","14","","79","IEEE","23 Jun 2025","26 April-6 May 2025","26 April-6 May 2025","IEEE","IEEE Conferences"
"Generative AI Efficiency and Effectiveness in Software Project Documentation Review Process","K. A. Demir; T. N. V. Muppalla; B. Liu","Department of Computer Science, Texas A&M University - Corpus Christi, Corpus Christi, USA; Department of Computer Science, Texas A&M University - Corpus Christi, Corpus Christi, USA; Department of Computer Science, Texas A&M University - Corpus Christi, Corpus Christi, USA","2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)","24 Sep 2025","2025","","","1","7","In this research, we report the capabilities of Large Language Model (LLM)-based Generative Artificial Intelligence (GenAI) tools, such as ChatGPT and Gemini, in the software project documentation review process. In software projects, the document artifact review process is an essential part of the quality assurance process. This review process is a human effort-intensive process. Introducing automation in this process will help reduce human effort, speed up the development, and reduce the potential of errors in the documents. With its enhanced natural language processing capabilities, LLM-based GenAI tools provide significant potential in the above process. In this research, we investigate the efficiency and effectiveness of LLM-based GenAI tools (such as ChatGPT and Gemini) for software documentation review process. Our experimental design includes comparing the manual document review process with the automated document review process conducted by LLM-based GenAI tools. The research results indicate that utilizing LLM-based GenAI tools provides significant efficiency in the document review process. However, LLM-based GenAI tools can only reach half the effectiveness and review quality that can be achieved by a human. To the best of our knowledge, this study is one of the first studies investigating the effectiveness and efficiency of utilizing GenAI for the software documentation review process. This research provides significant insights into improving the software document artifact review process utilizing LLM-based GenAI techniques.","","979-8-3315-3562-9","10.1109/ACDSA65407.2025.11165833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11165833","Generative AI;software documentation;document review;software engineering;software development;software process improvement","Quality assurance;Reviews;Generative AI;Large language models;Documentation;Manuals;Chatbots;Software;Software development management;Software engineering","","","","43","IEEE","24 Sep 2025","7-9 Aug. 2025","7-9 Aug. 2025","IEEE","IEEE Conferences"
"Model-Based Testing Computer Games: Does It Work?","I. S. W. B. Prasetya","Utrecht University, The Netherlands","2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","16 Apr 2025","2025","","","85","92","Model-based testing (MBT) allows a target software to be tested systematically and automatically by making use of a model of the software under test. It has been successfully applied in various domains. However its application for testing computer games has not been much studied. The highly dynamic nature of computer games makes it challenging for modeling. In this paper we propose a predicate-based modeling approach coupled with an on-line test generation approach. Both aspects ease the details that need to be incorporated in the model to facilitate effective test generation. Additionally we also leverage the use of intelligent agents, so that dealing with hazards and obstacles can optionally be delegated to such an agent, hence keeping the model clean from such aspects. A case study with a game called MiniDungeon is included to discuss the viability and benefit of the approach, e.g. in terms of code coverage and ability to cover deep states.","2159-4848","979-8-3315-3467-7","10.1109/ICSTW64639.2025.10962518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962518","model based testing computer games;MBT computer games;online model based testing;automated testing computer games","Software testing;Video games;Computational modeling;Conferences;Games;Software;Hazards;Test pattern generators;Intelligent agents;Testing","","1","","23","IEEE","16 Apr 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"Optimized Mutation of Grey-box Fuzzing: A Deep RL-based Approach","J. Shao; Y. Zhou; G. Liu; D. Zheng","School of Cyber Science and Engineering, Southeast University, Nanjing, P. R. China; School of Cyber Science and Engineering, Southeast University, Nanjing, P. R. China; School of Mathematics, Southeast University, Nanjing, P. R. China; Advanced Research Institute of Multidisciplinary Science, Beijing Institute of Technology, Beijing, P. R. China",2023 IEEE 12th Data Driven Control and Learning Systems Conference (DDCLS),"7 Jul 2023","2023","","","1296","1300","As a vulnerability discovery technique, fuzzing has been widely used in the field of software test in the past years. Traditional fuzzing has several drawbacks, including poor efficiency, low code coverage, and a high dependence on expert experience. By introducing the deep reinforcement learning technique, one can train the mutator of the fuzzer to move in a desired direction, such as maximizing code coverage or finding more code paths. This paper proposes a reinforcement learning-based fuzzing method to enhance the code coverage and explore potential code vulnerabilities. First, the concept of the input field is introduced to the seed file, reducing invalid operations by marking whether each byte of the seed file is a valid byte. Then, we optimize mutation by modeling the grey-box fuzzing as a reinforcement learning problem and training mutator's behavior on test cases. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. Finally, experimental results show that the proposed deep reinforcement learning-based fuzzing method outperforms the baseline random fuzzing algorithms.","2767-9861","979-8-3503-2105-0","10.1109/DDCLS58216.2023.10166955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10166955","Fuzzing;Reinforcement Learning;Seed Mutation;Software Testing","Deep learning;Training;Learning systems;Codes;Neural networks;Reinforcement learning;Fuzzing","","1","","15","IEEE","7 Jul 2023","12-14 May 2023","12-14 May 2023","IEEE","IEEE Conferences"
"Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs","J. Corazza; I. Gavran; G. Moreira; D. Neider","Research Center Trustworthy Data Science and Security, TU Dortmund University, Dortmund, Germany; Informal Systems, Vienna, Austria; Informal Systems, Joinville, Brazil; Research Center Trustworthy Data Science and Security, TU Dortmund University, Dortmund, Germany","2025 IEEE Conference on Software Testing, Verification and Validation (ICST)","20 May 2025","2025","","","542","552","When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct–vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model–a mathematical abstraction of the software system–which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we “fill in the blanks” using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.","2159-4848","979-8-3315-0814-2","10.1109/ICST62969.2025.10989026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10989026","Code Generation;Large Language Models;Formal Methods;Model Synthesis;Smart Contracts;Model-Based Techniques;Software Auditing","Codes;Large language models;Smart contracts;Syntactics;Maintenance engineering;Software;Mathematical models;User experience;Trustless services;Blockchains","","","","37","IEEE","20 May 2025","31 March-4 April 2025","31 March-4 April 2025","IEEE","IEEE Conferences"
"LLMMutation: Mutation Testing for Large Language Models","Z. Liu; X. Duan; Z. Cui; Z. Zeng","College of Computer Science, Beijing Information Science and Technology University, Beijing, China; College of Computer Science, Beijing Information Science and Technology University, Beijing, China; College of Computer Science, Beijing Information Science and Technology University, Beijing, China; College of Computer Science, Beijing Information Science and Technology University, Beijing, China",2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI),"15 Dec 2025","2025","","","355","362","The performance of Large Language Models (LLMs) heavily depends on the quality of their training data, while their capabilities are primarily evaluated through taskspecific test datasets. This evaluation approach makes the quality of test datasets a critical factor in ensuring LLM reliability. Even high-accuracy LLMs may exhibit weak generalizability and insufficient robustness when they are evaluated using flawed test sets with insufficient sample sizes or limited diversity. In traditional software testing, mutation testing is a well-established technique for assessing test suite effectiveness by measuring the ability to detect artificially injected faults. However, fundamental differences exist between traditional software and Transformerbased LLMs, preventing the direct application of classical mutation testing techniques. To address this, we propose LLMMutation, a dedicated test dataset quality evaluation method for LLMs, which applies multi-level mutation operators to introduce perturbations into the model. Experimental results demonstrate that models perturbed by different mutation operators exhibit significant accuracy degradation, which validate LLMMutation's effectiveness in fault injection. In addition, different test datasets exhibit distinct mutation score distributions under LLMMutation perturbations, which is positively correlated with the quality of the datasets. This confirms that the mutation score can effectively distinguish the quality of datasets used for the same task.","2375-0197","979-8-3315-4919-0","10.1109/ICTAI66417.2025.00053","Beijing Information Science and Technology University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11272669","Natural Language Processing;Dataset Quality;Mutation Testing","Software testing;Perturbation methods;Large language models;Training data;Transformers;Software;Robustness;Natural language processing;Software reliability;Software measurement","","","","26","IEEE","15 Dec 2025","3-5 Nov. 2025","3-5 Nov. 2025","IEEE","IEEE Conferences"
"From LLMs to Randomness: Analyzing Program Input Efficacy With Resource and Language Metrics","G. Black; E. Yocam; V. Mathew Vaidyan; G. Comert; Y. Wang","Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA; Department of Computer Science, College of Engineering, North Carolina Agricultural and Technical State University, Greensboro, NC, USA; Beacom College of Computer and Cyber Sciences, Dakota State University, Madison, SD, USA",IEEE Access,"23 May 2025","2025","13","","87928","87940","Security-focused program testing typically focuses on crash detection and code coverage while overlooking additional system behaviors that can impact program confidentiality and availability. To address this gap, we propose a statistical framework that combines embedding-based anomaly detection, resource usage metrics, and resource-state distance measures to systematically profile software behaviors beyond traditional coverage-based methods. Leveraging over 5 million labeled samples from 50 Python programs, we evaluate how these independent scoring terms distinguish among different sources of input, including Large Language Model (LLM)-generated inputs, and demonstrate how standard statistical tests (e.g., Kolmogorov—Smirnov and Kendall’s  $\tau $ ) confirm their effectiveness. Our findings show that LLM-generated samples can trigger diverse behaviors but are often less effective at exploring resource usage dynamics (CPU, memory) compared with conventional fuzzing. However, combining LLM outputs with existing techniques broadens behavior coverage and reveals commonalities between commercial LLM outputs. We provide open-source tools for this evaluation framework, demonstrating the potential to refine software testing by integrating behavior metrics into security-testing workflows.","2169-3536","","10.1109/ACCESS.2025.3571205","U.S. Department of Energy Minority Serving Institutions Partnership Program (MSIPP); Savannah River National Laboratory under BSRA Contract(grant numbers:TOA0000525174CN1); MSIPP-Claflin(grant numbers:FM-MHP-0678-22-01-00); National Science Foundation(grant numbers:2131080,2200457,OIA-2242812,2234920,2305470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11006641","Software profiling;program behavior analysis;fuzzing techniques;resource usage metrics;large language models;anomaly detection","Measurement;Software;Fuzzing;Testing;Memory management;Computer crashes;Codes;Anomaly detection;Python;Statistical analysis","","","","49","CCBY","19 May 2025","2025","","IEEE","IEEE Journals"
"Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework","N. -K. Le; H. Nguyen; M. N. Nguyen; S. T. Luu; T. Vo; Q. M. Bui; S. Nomura; L. -M. Nguyen","Japan Advanced Institute of Science and Technology, Ishikawa, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan; Amifiable Inc., Tokyo, Japan; Amifiable Inc., Tokyo, Japan; Japan Advanced Institute of Science and Technology, Ishikawa, Japan",2025 17th International Conference on Knowledge and System Engineering (KSE),"30 Dec 2025","2025","","","1","6","Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.","2694-4804","979-8-3315-8900-4","10.1109/KSE68178.2025.11309697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11309697","Web application testing;Large language models;Form interaction automation","Training;Measurement;Software testing;Automation;Systematics;Large language models;Syntactics;Systems engineering and theory;Selenium;Testing","","","","18","IEEE","30 Dec 2025","6-8 Nov. 2025","6-8 Nov. 2025","IEEE","IEEE Conferences"
"Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models","J. Wu; C. Lu; A. Arrieta; T. Yue; S. Ali","Simula Research Laboratory, University of Oslo, Oslo, Norway; Simula Research Laboratory, University of Oslo, Oslo, Norway; Mondragon University, Mondragon, Spain; Beihang University, Beijing, China; Simula Research Laboratory, Oslo, Norway",2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:,"30 Jul 2024","2024","","","40","51","Large Language Models (LLMs) are demonstrating outstanding potential for tasks such as text generation, summarization, and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that LLMs can as-sess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empir-ical evaluation to assess whether LLMs are effective and robust in performing the task. This reality check is an important step towards devising LLM-based autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from DeepScenario-an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic sce-narios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three LLMs (GPT-3.5, Llama2-13B, and Mistral-7B) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, GPT- 3.5 achieved the highest robustness compared to Llama2-13B and Mistral-7B, consistently throughout almost all scenarios, roads, and weather conditions; (2) Mistral-7B performed the worst consistently; (3) Llama2-13B achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the LLMs.","","979-8-4007-0536-6","10.1145/3650105.3652296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10599585","Large Language Models;Realistic Driving Scenarios;Robustness","Terminology;Roads;Large language models;Robustness;Task analysis;Autonomous vehicles;Testing","","5","","58","","30 Jul 2024","14-14 April 2024","14-14 April 2024","IEEE","IEEE Conferences"
"Clozemaster: Fuzzing Rust Compiler by Harnessing Llms for Infilling Masked Real Programs","H. Gao; Y. Yang; M. Sun; J. Wu; Y. Zhou; B. Xu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE),"23 Jun 2025","2025","","","1422","1435","Ensuring the reliability of the Rust compiler is of paramount importance, given increasing adoption of Rust for critical systems development, due to its emphasis on memory and thread safety. However, generating valid test programs for the Rust compiler poses significant challenges, given Rust's complex syntax and strict requirements. With the growing popularity of large language models (LLMs), much research in software testing has explored using LLMs to generate test cases. Still, directly using LLMs to generate Rust programs often results in a large number of invalid test cases. Existing studies have indicated that test cases triggering historical compiler bugs can assist in software testing. Our investigation into Rust compiler bug issues supports this observation. Inspired by existing work and our empirical research, we introduce a bracket-based masking and filling strategy called clozeMask. The clozeMask strategy involves extracting test code from historical issue reports, identifying and masking code snippets with specific structures, and using an LLM to fill in the masked portions for synthesizing new test programs. This approach harnesses the generative capabilities of LLMs while retaining the ability to trigger Rust compiler bugs. It enables comprehensive testing of the compiler's behavior, particularly exploring edge cases. We implemented our approach as a prototype ClozeMaster. ClozeMaster has identified 27 confirmed bugs for rustc and mrustc, of which 10 have been fixed by developers. Furthermore, our experimental results indicate that ClozeMaster outperforms existing fuzzers in terms of code coverage and effectiveness.","1558-1225","979-8-3315-0569-1","10.1109/ICSE55347.2025.00175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11029729","Rust Compiler;fuzzing;large language model;bug detection","Codes;Large language models;Instruction sets;Computer bugs;Prototypes;Fuzzing;Syntactics;Safety;Reliability;Software engineering","","6","","69","IEEE","23 Jun 2025","26 April-6 May 2025","26 April-6 May 2025","IEEE","IEEE Conferences"
"Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation","Z. Jiang; M. Wen; J. Cao; X. Shi; H. Jin","Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; The Hong Kong University of Science and Technology, Hong Kong, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China",2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE),"29 Nov 2024","2024","","","1408","1420","Automatic testing has garnered significant attention and success over the past few decades. Techniques such as unit testing and coverage-guided fuzzing have revealed numerous critical software bugs and vulnerabilities. However, a long-standing, formidable challenge for existing techniques is how to achieve higher testing coverage. Constraint-based techniques, such as symbolic execution and concolic testing, have been well-explored and integrated into the existing approaches. With the popularity of Large Language Models (LLMs), recent research efforts to design tailored prompts to generate inputs that can reach more uncovered target branches. However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based solutions has not been systematically explored.To bridge this gap, we conduct the first systematic study on the mainstream LLMs and constraint-based tools for directed input generation with a comparative perspective. We find that LLMs such as ChatGPT are comparable to or even better than the constraint-based tools, succeeding in 43.40%-58.57% samples in our dataset. Meanwhile, there are also limitations for LLMs in specific scenarios such as sequential calculation, where constraint-based tools are in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and implement a prototype based on ChatGPT and constraint-based tools. Our evaluation shows that our approach can outperform the baselines by 1.4x to 2.3x relatively. We believe our study can provide novel insights into directed input generation using LLMs, and our findings are essential for future testing research.CCS Concepts• Software and its engineering → Automatic programming; Software testing and debugging.","2643-1572","979-8-4007-1248-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10765050","LLM;Symbolic Execution;Directed Input Generation","Computer languages;Systematics;Codes;Sensitivity;Large language models;Prototypes;Transforms;Chatbots;Software;Software engineering","","1","","65","","29 Nov 2024","27 Oct.-1 Nov. 2024","27 Oct.-1 Nov. 2024","IEEE","IEEE Conferences"
"Evaluating the Effectiveness of Large Language Models in Automated Unit Test Generation","T. Godage; S. Nimishan; S. Vasanthapriyan; V. Palanisamy; C. Joseph; S. Thuseethan","Sabaragamuwa University of Sri Lanka, Sri Lanka; University of Peradeniya, Sri Lanka; Sabaragamuwa University of Sri Lanka, Sri Lanka; Charles Darwin University, Australia; University of Southern Queensland, Australia; Charles Darwin University, Australia",2025 5th International Conference on Advanced Research in Computing (ICARC),"16 Apr 2025","2025","","","1","6","The increasing use of Artificial Intelligence (AI) in software development underscores the need to select suitable Large Language Models (LLMs) for automating software unit test generation. No prior work has been conducted to evaluate the performance of LLM in this domain. To address this gap, this study evaluates the effectiveness of four prominent LLMs—GPT-4, Claude 3.5, Command-R-08-2024 and Llama 3.1—in generating unit test cases. This study particularly aims to evaluate the performance of these models in real-world testing scenarios. Hence, 106 test cases from 23 test suites based on interviews with software experts and QA engineers are used to ensure relevance and comprehensiveness. These test cases are analyzed using JavaScript Engines Specification Tester (JEST) for code coverage and Stryker for mutation testing while adopting both quantitative and qualitative analysis. The findings reveal that Claude 3.5 consistently outperforms the other models against test success rate, statement coverage, and mutation score with the achieved accuracy of 93.33%, 98.01%, and 89.23% respectively. The results also provide insights into the capabilities of LLMs for automated unit test generation and their integration into the continuous software integration pipeline. Further, the findings authenticated the importance of systematically comparing LLMs for test case generation.","","979-8-3315-3098-3","10.1109/ICARC64760.2025.10962997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10962997","large language models;software unit testing;test automation;mutation testing;javascript","Measurement;Accuracy;Large language models;Pipelines;Software;Test pattern generators;Standards;Optimization;Testing;Software development management","","3","","20","IEEE","16 Apr 2025","19-20 Feb. 2025","19-20 Feb. 2025","IEEE","IEEE Conferences"
"Retrieval-Augmented Generation for Software Requirement-Based Test Case Generation","Z. Wang; X. Guo; T. Tsuchiya","The University of Osaka, Suita, Osaka, Japan; The University of Osaka, Suita, Osaka, Japan; The University of Osaka, Suita, Osaka, Japan","2025 25th International Conference on Software Quality, Reliability and Security (QRS)","29 Sep 2025","2025","","","108","119","Testers often need to manually write black-box test cases based on software artifacts such as requirement documents. In agile development, this process is often time-consuming and is further complicated by frequent requirement changes, leading to continuous maintenance overhead. Automating this process is therefore essential. Given the strong natural language understanding and generation capabilities of large language models (LLMs), combined with Retrieval-Augmented Generation (RAG), we propose a RAG-based framework for automated test case generation. Before generation, we embed software artifacts to construct a vectorbased knowledge database. At runtime, software requirements are used as queries to retrieve relevant context, which is integrated into a prompt and passed to the LLM for test case generation. This approach addresses several shortcomings of LLMs, including limited context length, attention dilution over large inputs, and the tendency to hallucinate or over-look key domain-specific constraints. By providing query-specific external knowledge, RAG enhances both accuracy and efficiency. We deploy the framework with different models locally and conduct experiments on two open-source datasets. Compared with the manually written benchmark test cases, our method achieves full requirement coverage with fewer test cases, improved efficiency, reduced error potential, and realized better readability.","2693-9177","978-1-6654-7771-0","10.1109/QRS65678.2025.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11173474","Test Case Generation;Retrieval-Augmented Generation;Large Language Model;Software Requirement","Runtime;Databases;Large language models;Retrieval augmented generation;Closed box;Software quality;Natural language processing;Software reliability;Maintenance;Security","","","","33","IEEE","29 Sep 2025","16-20 July 2025","16-20 July 2025","IEEE","IEEE Conferences"
"ISO/IEC/IEEE International Standard - Systems and software engineering -- Vocabulary","",,ISO/IEC/IEEE 24765:2010(E),"8 Feb 2018","2010","","","1","418","The systems and software engineering disciplines are continuing to mature while information technology advances. This International Standard was prepared to collect and standardize terminology. Its purpose is to identify terms currently in use in the field and standard definitions for these terms. It is intended to serve as a useful reference for those in the Information Technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute (PMI). This International Standard replaces IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, which was contributed by the IEEE as a source document. The approach and lexical exactitude of IEEE Std 610.12-1990 served as a model for this International Standard. Nevertheless, approximately two thirds of the definitions in this International Standard are new since IEEE Std 610.12 was last updated in 1990, a reflection of the continued evolution in the field.;ISO/IEC/IEEE 24765:2010 provides a common vocabulary applicable to all systems and software engineering work. It was prepared to collect and standardize terminology. ISO/IEC/IEEE 24765:2010 is intended to serve as a useful reference for those in the information technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute. ISO/IEC/IEEE 24765:2010 includes references to the active source standards for each definition so that the use of the term can be further explored.","","978-0-7381-6205-8","10.1109/IEEESTD.2010.5733835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5733835","computer;dictionary;information technology;software engineering;systems engineering;terminology;vocabulary","IEEE standards;ISO standards;IEC standards;Software engineering;Dictionaries","","41","1","128","","8 Feb 2018","15 Dec. 2010","","IEEE","IEEE Standards"
