@article{abtahi2025augmenting,
  author = {Abtahi, Seyed Moein and Azim, Akramul},
  title  = {Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements},
  year   = {2025}
}
@article{applis2025unified,
  author = {Applis, Leonhard et al.},
  title  = {Unified Software Engineering agent as AI Software Engineer},
  year   = {2025}
}
@inproceedings{hong2023metagpt,
  title     = {MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework},
  author    = {Hong, Sirui et al.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024}
}
@inproceedings{qian2023chatdev,
  title     = {ChatDev: Communicative Agents for Software Development},
  author    = {Qian, Chen et al.},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2024}
}

@inproceedings{alshahwan2024automated,
  title     = {Automated Unit Test Improvement using Large Language Models at Meta},
  author    = {Alshahwan, Nadia et al.},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year      = {2024}
}
@article{he2025zspace,
  author = {He, Qingsong et al.},
  title  = {Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation},
  year   = {2025}
}
@inproceedings{huang2023agentcoder,
  author    = {Huang, Dong et al.},
  title     = {AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year      = {2024}
}
@article{jin2024llms,
  author = {Jin, Haolin et al.},
  title  = {From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future},
  year   = {2024}
}
@inproceedings{jin2024rgd,
  author = {Jin, Haolin et al.},
  title  = {RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance},
  year   = {2024}
}
@article{chen2025when,
  author = {Chen, Jingyi et al.},
  title  = {When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation Just as It Helps Developers?},
  year   = {2025}
}
@inproceedings{kim2024multi,
  author    = {Kim, Myeongsoo and Stennett, Tyler and Sinha, Saurabh and Orso, Alessandro},
  title     = {A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year      = {2025}
}
@article{liu2024large,
  author = {Liu, Junwei et al.},
  title  = {Large Language Model-Based Agents for Software Engineering: A Survey},
  year   = {2024}
}
@article{meng2024empirical,
  author = {Meng, Xiangxin et al.},
  title  = {An Empirical Study on LLM-based Agents for Automated Bug Fixing},
  year   = {2024}
}
@article{mundler2024code,
  author = {Mündler, Niels et al.},
  title  = {Code Agents are State of the Art Software Testers},
  year   = {2024}
}
@article{nunez2024autosafecoder,
  author = {Nunez, Ana et al.},
  title  = {AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing},
  year   = {2024}
}
@article{oueslati2025refagent,
  author = {Oueslati, Khouloud et al.},
  title  = {RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring},
  year   = {2025}
}
@article{pan2025codecor,
  author = {Pan, Ruwei et al.},
  title  = {CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code Generation},
  year   = {2025}
}
@inproceedings{shamim2024methodology,
  author = {Shamim, Isha and Singhal, Rekha},
  title  = {Methodology for Quality Assurance Testing of LLM-based Multi-Agent Systems},
  year   = {2024}
}
@article{tawosi2025almas,
  author = {Tawosi, Vali et al.},
  title  = {ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework},
  year   = {2025}
}
@article{badertdinov2025swe,
  author = {Badertdinov, Ibragim et al.},
  title  = {SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents},
  year   = {2025}
}
@article{ugarte2025astral,
  author = {Ugarte, Miriam et al.},
  title  = {ASTRAL: Automated Safety Testing of Large Language Models},
  year   = {2025}
}

@article{wu2025curriculumpt,
  author = {Wu, Xingyu et al.},
  title  = {CurriculumPT: LLM-Based Multi-Agent Autonomous Penetration Testing with Curriculum-Guided Task Scheduling},
  year   = {2025}
}
@book{winters2020software,
  title  = {Software Engineering at Google: Lessons Learned from Programming Over Time},
  author = {Winters, Titus et al.},
  year   = {2020}
}
@inproceedings{vaswani2017attention,
  title  = {Attention is all you need},
  author = {Vaswani, Ashish et al.},
  year   = {2017}
}
@inproceedings{yao2022react,
  title  = {ReAct: Synergizing Reasoning and Acting in Language Models},
  author = {Yao, Shunyu et al.},
  year   = {2023}
}
@inproceedings{wei2022chain,
  title  = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author = {Wei, Jason et al.},
  year   = {2022}
}
@inproceedings{xia2024agentless,
  author    = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  title     = {Agentless: Demystifying LLM-based Software Engineering Agents},
  booktitle = {International Symposium on the Foundations of Software Engineering (FSE)},
  year      = {2025}
}
@inproceedings{jimenez2024swebench,
  title     = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author    = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Yin, Kexin and Narasimhan, Karthik},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024}
}
@article{moher2009preferred,
  title     = {Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement},
  author    = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G and Prisma Group and others},
  journal   = {PLoS medicine},
  volume    = {6},
  number    = {7},
  pages     = {e1000097},
  year      = {2009},
  publisher = {Public Library of Science}
}
@article{page2021prisma,
  title     = {The PRISMA 2020 statement: an updated guideline for reporting systematic reviews},
  author    = {Page, Matthew J et al.},
  journal   = {BMJ},
  volume    = {372},
  year      = {2021},
  publisher = {British Medical Journal Publishing Group}
}
@article{packer2023memgpt,
  title   = {MemGPT: Towards LLMs as Operating Systems},
  author  = {Packer, Charles and Fang, Vivian and Patil, Shishir G and Lin, Kevin and Wooders, Sarah and Gonzalez, Joseph E},
  journal = {arXiv preprint arXiv:2310.08560},
  year    = {2023}
}
@misc{eu_ai_act,
  title  = {Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)},
  author = {{European Union}},
  year   = {2024},
  url    = {http://data.europa.eu/eli/reg/2024/1689/oj}
}
@misc{acm_ethics,
  title  = {ACM Code of Ethics and Professional Conduct},
  author = {{Association for Computing Machinery}},
  year   = {2018},
  url    = {https://www.acm.org/code-of-ethics}
}

@inproceedings{owotogbe2025assessing,
  title     = {Assessing and Enhancing the Robustness of LLM-Based Multi-Agent Systems Through Chaos Engineering},
  author    = {Owotogbe, J.},
  booktitle = {Proc. CAIN},
  year      = {2025}
}
@inproceedings{bradbury2025addressing,
  title     = {Addressing Data Leakage in HumanEval Using Combinatorial Test Design},
  author    = {Bradbury, J. S. and others},
  booktitle = {Proc. ICST},
  year      = {2025}
}
@inproceedings{traykov2024framework,
  title     = {A Framework for Security Testing of Large Language Models},
  author    = {Traykov, K.},
  booktitle = {Proc. IS},
  year      = {2024}
}
@inproceedings{chandrasekaran2025evaluating,
  title     = {Evaluating Large Language Model Robustness using Combinatorial Testing},
  author    = {Chandrasekaran, J. and others},
  booktitle = {Proc. ICSTW},
  year      = {2025}
}
@inproceedings{loevenich2025agentic,
  title     = {Agentic Generative AI for Automation of Cyber Security Attack Chains in Tactical MANETs},
  author    = {Loevenich, J. F. and others},
  booktitle = {Proc. LCN},
  year      = {2025}
}
@inproceedings{harman2025mutation,
  title     = {Mutation-Guided LLM-based Test Generation at Meta},
  author    = {Harman, M. and others},
  booktitle = {Proc. FSE},
  year      = {2025}
}
@article{shang2025finetuning,
  title   = {A Large-Scale Empirical Study on Fine-Tuning Large Language Models for Unit Testing},
  author  = {Shang, Y. and others},
  journal = {Proc. ACM Softw. Eng.},
  year    = {2025}
}
% Referenced in Chapter 4
@inproceedings{sternak2025automating,
  title     = {Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach},
  author    = {Sternak, T. and others},
  booktitle = {Proc. MIPRO},
  year      = {2025}
}

% Referenced in Chapter 4
@inproceedings{nayyeri2025privacy,
  title     = {A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model},
  author    = {Nayyeri, A. and others},
  booktitle = {Proc. ICCKE},
  year      = {2025}
}

% Referenced in Chapter 4
@inproceedings{zhao2024models,
  title     = {Models Are Codes: Towards Measuring Malicious Code Poisoning Attacks on Pre-trained Model Hubs},
  author    = {Zhao, J. and others},
  booktitle = {Proc. ASE},
  year      = {2024}
}
% Referenced in Chapter 4
@inproceedings{wang2024agents,
  title     = {Do Agents Behave Aligned with Human Instructions? - An Automated Assessment Approach},
  author    = {Wang, K. and others},
  booktitle = {Proc. QRS-C},
  year      = {2024}
}
% Referenced in Chapter 4
@inproceedings{srinivasan2025dura,
  title     = {DURA-CPS: A Multi-Role Orchestrator for Dependability Assurance in LLM-Enabled Cyber-Physical Systems},
  author    = {Srinivasan, T. and others},
  booktitle = {Proc. DSN-W},
  year      = {2025}
}

% Referenced in Chapter 4
@inproceedings{sallou2024breaking,
  title     = {Breaking the Silence: the Threats of Using LLMs in Software Engineering},
  author    = {Sallou, J. and others},
  booktitle = {Proc. ICSE-NIER},
  year      = {2024}
}


% Referenced in Chapter 4
@inproceedings{hoffmann2024generating,
  title     = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
  author    = {Hoffmann, J. and Frister, D.},
  booktitle = {Proc. AST},
  year      = {2024}
}

% Referenced in Chapter 4
@article{wang2023software,
  author = {Wang, Junjie et al.},
  title  = {Software Testing with Large Language Model: Survey, Landscape, and Vision},
  year   = {2023}
}

% Referenced in Chapter 4
@inproceedings{yang2024sweagent,
  title     = {SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author    = {Yang, John et al.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2024}
}

@inproceedings{jimenez2024swebench,
  title     = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author    = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://arxiv.org/abs/2310.06770}
}
% --- New SLR Entries ---
@inproceedings{ahammad2025magister,
  author    = {A. Ahammad and M. El Bajta and M. Radgui},
  title     = {MAGISTER: LLM-Based Test Generation with Role-Specialized Agents},
  booktitle = {2025 International Conference on Intelligent Systems: Theories and Applications (SITA)},
  year      = {2025},
  abstract  = {Automated test generation is one of the most critical topics in software testing, with numerous challenges due to the need for deep code understanding and the creation of meaningful assertions. Traditional approaches often generate low-quality and difficult-to-read tests that lack real code understanding and rely on statistical and dynamic analysis. With the recent advances in Large Language Models (LLMs), new opportunities emerge for generating unit tests that prioritize readability and context...}
}

@inproceedings{salman2025a,
  author    = {I. Salman and M. Waseem and V. Mandić and R. D. De Alwis},
  title     = {A Vision for Debiasing Confirmation Bias in Software Testing via LLM},
  booktitle = {2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  year      = {2025},
  abstract  = {Background: Large language models (LLM) suffer from various forms of biases due to the biased datasets used to train the models. At the same time, human cognitive biases have an equal propensity to express themselves when using LLMs for software engineering tasks. Software testing is a critical phase of the software development life cycle. Confirmation bias is reported to have deteriorated software testing by designing more specification-consistent test cases compared to specificationinconsisten...}
}

@inproceedings{tomic2025evaluation,
  author    = {S. Tomic and E. Alégroth and M. Isaac},
  title     = {Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation},
  booktitle = {2025 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  year      = {2025},
  abstract  = {Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present Path...}
}

@inproceedings{maklad2025multifuzz,
  author    = {Y. Maklad and F. Wael and A. Hamdi and W. Elsersy and K. Shaban},
  title     = {MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing},
  booktitle = {2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)},
  year      = {2025},
  abstract  = {Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, a...}
}

  title = {MCM: A Multi-Agent Collaborative Multimodal Framework For Traditional Chinese Medicine Diagnosis},
  booktitle = {2025 IEEE International Conference on Image Processing (ICIP)},
  year = {2025},
  abstract = {The advancement of information technology and the rise of generative AI have paved the way for the development of Large Language Models (LLMs) tailored for TCM diagnostics. However, existing LLMs in the field of TCM face challenges in interpretability, limited modality in interaction, and robustness. To address these limitations, we propose MCM, a Multi-Agent Collaborative Multimodal Framework for TCM Diagnosis. This framework enables robust and interpretable multimodal diagnosis through multi-a...}
}

@inproceedings{karanjai2025hpcagenttester,
  author    = {R. Karanjai and L. Xu and W. Shi},
  title     = {HPCAgentTester: a Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation},
  booktitle = {2025 2nd IEEE/ACM International Conference on AI-powered Software (AIware)},
  year      = {2025},
  abstract  = {Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow wh...}
}

@inproceedings{hardgrove2025liblmfuzz,
  author    = {I. Hardgrove and J. D. Hastings},
  title     = {LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-Box Libraries},
  booktitle = {2025 Cyber Awareness and Research Symposium (CARS)},
  year      = {2025},
  abstract  = {A fundamental problem in cybersecurity and computer science is determining whether a program is free of bugs and vulnerabilities. Fuzzing, a popular approach to discovering vulnerabilities in programs, has several advantages over alternative strategies, although it has investment costs in the form of initial setup and continuous maintenance. The choice of fuzzing is further complicated when only a binary library is available, such as the case of closed-source and proprietary software. In respons...}
}

@inproceedings{kanagaraj2025llmdriven,
  author    = {K. Kanagaraj and D. Handa and K. K. Nikhil and S. Duvarakanath and S. M.},
  title     = {LLM-Driven Smart Test Case Generation for Scalable Software Testing},
  booktitle = {2025 2nd International Conference on Software, Systems and Information Technology (SSITCON)},
  year      = {2025},
  abstract  = {Software testing is vital to modern development; however, traditional test case generation remains limited by imprecision, heavy manual intervention, and a lack of scalability. This study proposes a robust framework that integrates Large Language Models (LLMs) with multi-agent systems to generate fully automated, end-to-end test cases. By combining Deep Learning with Chain-of-Thought reasoning, the framework intelligently analyzes back-end APIs and front-end UIs, producing context-aware and comp...}
}

@inproceedings{garlapati2024aipowered,
  author    = {A. Garlapati and M. N. V. Satya Sai Muni Parmesh and Savitha and J. S},
  title     = {AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s},
  booktitle = {2024 5th IEEE Global Conference for Advancement in Technology (GCAT)},
  year      = {2024},
  abstract  = {Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensur...}
}

@inproceedings{tiwari2025intellitest,
  author    = {A. S. Tiwari and S. Ganesh Nutan Dev C and P. M. Patole and G. Ponnamreddy and S. Chinthalapudi and D. P. Kattamanchi},
  title     = {IntelliTest: An Intelligent Framework for Agentic Functional Test Generation Using Multimodal Data and Domain Knowledge},
  booktitle = {2025 IEEE Future Networks World Forum (FNWF)},
  year      = {2025},
  abstract  = {IntelliTest is an end-to-end, locally deployable framework that automates functional test generation and selection for complex embedded systems from two input modes: Software Change Artifacts (SCAs)—multimodal changelist data combining code diffs with textual metadata—and Natural-Language Queries (NLQs) for direct test requests. It employs a domainagnostic ontology defining procedural semantics, compiled into a structured procedural database and a specification knowledge graph for retrieval-augm...}
}

@inproceedings{ding2025multiagent,
  author    = {Y. Ding and J. Yu and A. Twabi and L. Zhang and T. Kondo and H. Sato},
  title     = {Multi-Agent Auditing for Smart Contracts*},
  booktitle = {2025 9th International Symposium on Computer Science and Intelligent Control (ISCSIC)},
  year      = {2025},
  abstract  = {Smart contracts underpin contemporary decentralized systems, yet their immutability and perpetual execution amplify the consequences of latent defects. Despite progress in manual audits, static analysis, fuzzing, and formal verification, auditors face a widening gap between the scalability and desired assurance due to limited automation capability. Recent large language models (LLMs) with tool-use capabilities promise greater automation, but monolithic single-agent auditors struggle with coverag...}
}

@inproceedings{sulaiman2025an,
  author    = {N. A. A. Sulaiman and H. Haron and N. M. Mahfuz and N. A. M. Saat and S. N. Daud and S. Alias},
  title     = {An Agentic Reasoning-Based Feedback System for Programming Assignments},
  booktitle = {2025 IEEE 11th International Conference on Computing, Engineering and Design (ICCED)},
  year      = {2025},
  abstract  = {This study introduces Explain-then-Grade, an automated feedback framework designed for programming lab assignments using agent reasoning and large language models (LLMs). Unlike conventional auto-graders, which provide marks directly and general feedback, the proposed framework requires the agent to explain detected errors prior to assigning marks, thereby enhancing pedagogical value and transparency. The framework integrates sandboxed execution, on-the-fly test generation, and structured, stepw...}
}

@inproceedings{bose2025llms,
  author    = {D. B. Bose and Y. B. Alebachew and C. Brown},
  title     = {LLMs in Debate: Does Arguing Make Them Better at Detecting Metamorphic Relations?},
  booktitle = {2025 40th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)},
  year      = {2025},
  abstract  = {Large Language Models (LLMs) are transforming software engineering, including mobile Augmented Reality (AR) applications. AR software behavior often depends on dynamic environmental factors, making it difficult to use conventional testing and verification approaches. Metamorphic Testing (MT) offers an alternative by assessing whether expected transformations hold across varied conditions. However, there is limited work exploring how well LLMs can detect these transformations-Metamorphic Relation...}
}

@inproceedings{pasuksmit2025humanintheloop,
  author    = {J. Pasuksmit and W. Takerngsaksiri and P. Thongtanunam and C. Tantithamthavorn and R. Zhang and S. Wang and F. Jiang and J. Li and E. Cook and K. Chen and M. Wu},
  title     = {Human-In-The-Loop Software Development Agents: Challenges and Future Directions},
  booktitle = {2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)},
  year      = {2025},
  abstract  = {Multi-agent LLM-driven systems for software development are rapidly gaining traction, offering new opportunities to enhance productivity. At Atlassian, we deployed Human-in-the-Loop Software Development Agents to resolve Jira work items and evaluated the generated code quality using functional correctness testing and GPT-based similarity scoring. This paper highlights two major challenges: the high computational costs of unit testing and the variability in LLM-based evaluations. We also propose ...}
}

@article{liu2025seeing,
  author   = {Z. Liu and C. Li and C. Chen and J. Wang and M. Chen and B. Wu and Y. Wang and J. Hu and Q. Wang},
  title    = {Seeing is Believing: Vision-Driven Non-Crash Functional Bug Detection for Mobile Apps},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2025},
  abstract = {Mobile app GUI (Graphical User Interface) pages now contain rich visual information, with the visual semantics of each page helping users understand the application logic. However, these complex visual and functional logics present new challenges to software testing. Existing automated GUI testing methods, constrained by the lack of reliable testing oracles, are limited to detecting crash bugs with obvious abnormal signals. Consequently, many non-crash functional bugs, ranging from unexpected be...}
}

@inproceedings{huang2025research,
  author    = {J. Huang},
  title     = {Research on Multi-Model Fusion Machine Learning Demand Intelligent Forecasting System in Cloud Computing Environment},
  booktitle = {2025 2nd International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)},
  year      = {2025},
  abstract  = {Background: Large language models (LLMs) are increasingly being used for automated unit test generation, but reported performance varies across tasks and datasets, and key aspects such as assertion plausibility and target coverage are not well understood. Methods: We perform a structured evaluation of LLM-based test generation on recent benchmarks and settings, summarizing research results in hinting, static analysis guidance, multi-agent work frameworks, and oracle generation; we compare LLMs w...}
}

@inproceedings{stojanović2024unit,
  author    = {D. Stojanović and B. Pavković and N. Četić and M. Krunić and L. Vidaković},
  title     = {Unit Test Generation Multi-Agent AI System for Enhancing Software Documentation and Code Coverage},
  booktitle = {2024 32nd Telecommunications Forum (TELFOR)},
  year      = {2024},
  abstract  = {Software development necessitates a robust testing plan though test development can be laborious and nonappealing task. We explore the utilization of the application artificial intelligence agents for generating and executing unit tests, enhancing the “Mostly Basic Python Problems” dataset. We employ behavior-driven development within a three-agent system to generate user stories and unit tests. Empirical results indicate improvements in branch coverage, illustrating the effective utilization of...}
}

@article{wang2025supply,
  author   = {S. Wang and Y. Zhao and X. Hou and H. Wang},
  title    = {Large Language Model Supply Chain: A Research Agenda},
  journal  = {TOSEM '25},
  year     = {2025},
  abstract = {Defines and analyzes the "supply chain" of LLMs, including infrastructure, foundation models, and downstream applications like autonomous agents and testing tools.}
}

@article{jiang2024survey,
  author   = {J. Jiang and F. Wang and J. Shen and S. Kim and S. Kim},
  title    = {A Survey on Large Language Models for Code Generation},
  journal  = {TOSEM},
  year     = {2024},
  abstract = {A comprehensive survey on "Code LLMs," covering evolution, advanced techniques (autonomous agents, RAG), and comparative analysis on benchmarks like HumanEval, MBPP, and SWE-bench.}
}

@inproceedings{bouzenia2025repair,
  author    = {I. Bouzenia and P. Devanbu and M. Pradel},
  title     = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
  booktitle = {ICSE '25},
  year      = {2025},
  abstract  = {Introduces RepairAgent, an autonomous agent that uses an LLM and a finite state machine to plan and execute actions (invoking tools) to fix software bugs.}
}


% --- New SLR Entries ---
@inproceedings{ahammad2025magister,
  author    = {A. Ahammad and M. El Bajta and M. Radgui},
  title     = {MAGISTER: LLM-Based Test Generation with Role-Specialized Agents},
  booktitle = {2025 International Conference on Intelligent Systems: Theories and Applications (SITA)},
  year      = {2025},
  abstract  = {Automated test generation is one of the most critical topics in software testing, with numerous challenges due to the need for deep code understanding and the creation of meaningful assertions. Traditional approaches often generate low-quality and difficult-to-read tests that lack real code understanding and rely on statistical and dynamic analysis. With the recent advances in Large Language Models (LLMs), new opportunities emerge for generating unit tests that prioritize readability and context...}
}

@inproceedings{salman2025a,
  author    = {I. Salman and M. Waseem and V. Mandić and R. D. De Alwis},
  title     = {A Vision for Debiasing Confirmation Bias in Software Testing via LLM},
  booktitle = {2025 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  year      = {2025},
  abstract  = {Background: Large language models (LLM) suffer from various forms of biases due to the biased datasets used to train the models. At the same time, human cognitive biases have an equal propensity to express themselves when using LLMs for software engineering tasks. Software testing is a critical phase of the software development life cycle. Confirmation bias is reported to have deteriorated software testing by designing more specification-consistent test cases compared to specificationinconsisten...}
}

@inproceedings{tomic2025evaluation,
  author    = {S. Tomic and E. Alégroth and M. Isaac},
  title     = {Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation},
  booktitle = {2025 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  year      = {2025},
  abstract  = {Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present Path...}
}

@inproceedings{maklad2025multifuzz,
  author    = {Y. Maklad and F. Wael and A. Hamdi and W. Elsersy and K. Shaban},
  title     = {MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing},
  booktitle = {2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)},
  year      = {2025},
  abstract  = {Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, a...}
}

  title = {MCM: A Multi-Agent Collaborative Multimodal Framework For Traditional Chinese Medicine Diagnosis},
  booktitle = {2025 IEEE International Conference on Image Processing (ICIP)},
  year = {2025},
  abstract = {The advancement of information technology and the rise of generative AI have paved the way for the development of Large Language Models (LLMs) tailored for TCM diagnostics. However, existing LLMs in the field of TCM face challenges in interpretability, limited modality in interaction, and robustness. To address these limitations, we propose MCM, a Multi-Agent Collaborative Multimodal Framework for TCM Diagnosis. This framework enables robust and interpretable multimodal diagnosis through multi-a...}
}

@inproceedings{karanjai2025hpcagenttester,
  author    = {R. Karanjai and L. Xu and W. Shi},
  title     = {HPCAgentTester: a Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation},
  booktitle = {2025 2nd IEEE/ACM International Conference on AI-powered Software (AIware)},
  year      = {2025},
  abstract  = {Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow wh...}
}

@inproceedings{hardgrove2025liblmfuzz,
  author    = {I. Hardgrove and J. D. Hastings},
  title     = {LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-Box Libraries},
  booktitle = {2025 Cyber Awareness and Research Symposium (CARS)},
  year      = {2025},
  abstract  = {A fundamental problem in cybersecurity and computer science is determining whether a program is free of bugs and vulnerabilities. Fuzzing, a popular approach to discovering vulnerabilities in programs, has several advantages over alternative strategies, although it has investment costs in the form of initial setup and continuous maintenance. The choice of fuzzing is further complicated when only a binary library is available, such as the case of closed-source and proprietary software. In respons...}
}

@inproceedings{kanagaraj2025llmdriven,
  author    = {K. Kanagaraj and D. Handa and K. K. Nikhil and S. Duvarakanath and S. M.},
  title     = {LLM-Driven Smart Test Case Generation for Scalable Software Testing},
  booktitle = {2025 2nd International Conference on Software, Systems and Information Technology (SSITCON)},
  year      = {2025},
  abstract  = {Software testing is vital to modern development; however, traditional test case generation remains limited by imprecision, heavy manual intervention, and a lack of scalability. This study proposes a robust framework that integrates Large Language Models (LLMs) with multi-agent systems to generate fully automated, end-to-end test cases. By combining Deep Learning with Chain-of-Thought reasoning, the framework intelligently analyzes back-end APIs and front-end UIs, producing context-aware and comp...}
}

@inproceedings{garlapati2024aipowered,
  author    = {A. Garlapati and M. N. V. Satya Sai Muni Parmesh and Savitha and J. S},
  title     = {AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s},
  booktitle = {2024 5th IEEE Global Conference for Advancement in Technology (GCAT)},
  year      = {2024},
  abstract  = {Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensur...}
}

@inproceedings{tiwari2025intellitest,
  author    = {A. S. Tiwari and S. Ganesh Nutan Dev C and P. M. Patole and G. Ponnamreddy and S. Chinthalapudi and D. P. Kattamanchi},
  title     = {IntelliTest: An Intelligent Framework for Agentic Functional Test Generation Using Multimodal Data and Domain Knowledge},
  booktitle = {2025 IEEE Future Networks World Forum (FNWF)},
  year      = {2025},
  abstract  = {IntelliTest is an end-to-end, locally deployable framework that automates functional test generation and selection for complex embedded systems from two input modes: Software Change Artifacts (SCAs)—multimodal changelist data combining code diffs with textual metadata—and Natural-Language Queries (NLQs) for direct test requests. It employs a domainagnostic ontology defining procedural semantics, compiled into a structured procedural database and a specification knowledge graph for retrieval-augm...}
}

@inproceedings{ding2025multiagent,
  author    = {Y. Ding and J. Yu and A. Twabi and L. Zhang and T. Kondo and H. Sato},
  title     = {Multi-Agent Auditing for Smart Contracts*},
  booktitle = {2025 9th International Symposium on Computer Science and Intelligent Control (ISCSIC)},
  year      = {2025},
  abstract  = {Smart contracts underpin contemporary decentralized systems, yet their immutability and perpetual execution amplify the consequences of latent defects. Despite progress in manual audits, static analysis, fuzzing, and formal verification, auditors face a widening gap between the scalability and desired assurance due to limited automation capability. Recent large language models (LLMs) with tool-use capabilities promise greater automation, but monolithic single-agent auditors struggle with coverag...}
}

@inproceedings{sulaiman2025an,
  author    = {N. A. A. Sulaiman and H. Haron and N. M. Mahfuz and N. A. M. Saat and S. N. Daud and S. Alias},
  title     = {An Agentic Reasoning-Based Feedback System for Programming Assignments},
  booktitle = {2025 IEEE 11th International Conference on Computing, Engineering and Design (ICCED)},
  year      = {2025},
  abstract  = {This study introduces Explain-then-Grade, an automated feedback framework designed for programming lab assignments using agent reasoning and large language models (LLMs). Unlike conventional auto-graders, which provide marks directly and general feedback, the proposed framework requires the agent to explain detected errors prior to assigning marks, thereby enhancing pedagogical value and transparency. The framework integrates sandboxed execution, on-the-fly test generation, and structured, stepw...}
}

@inproceedings{bose2025llms,
  author    = {D. B. Bose and Y. B. Alebachew and C. Brown},
  title     = {LLMs in Debate: Does Arguing Make Them Better at Detecting Metamorphic Relations?},
  booktitle = {2025 40th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)},
  year      = {2025},
  abstract  = {Large Language Models (LLMs) are transforming software engineering, including mobile Augmented Reality (AR) applications. AR software behavior often depends on dynamic environmental factors, making it difficult to use conventional testing and verification approaches. Metamorphic Testing (MT) offers an alternative by assessing whether expected transformations hold across varied conditions. However, there is limited work exploring how well LLMs can detect these transformations-Metamorphic Relation...}
}

@inproceedings{pasuksmit2025humanintheloop,
  author    = {J. Pasuksmit and W. Takerngsaksiri and P. Thongtanunam and C. Tantithamthavorn and R. Zhang and S. Wang and F. Jiang and J. Li and E. Cook and K. Chen and M. Wu},
  title     = {Human-In-The-Loop Software Development Agents: Challenges and Future Directions},
  booktitle = {2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)},
  year      = {2025},
  abstract  = {Multi-agent LLM-driven systems for software development are rapidly gaining traction, offering new opportunities to enhance productivity. At Atlassian, we deployed Human-in-the-Loop Software Development Agents to resolve Jira work items and evaluated the generated code quality using functional correctness testing and GPT-based similarity scoring. This paper highlights two major challenges: the high computational costs of unit testing and the variability in LLM-based evaluations. We also propose ...}
}

@article{liu2025seeing,
  author   = {Z. Liu and C. Li and C. Chen and J. Wang and M. Chen and B. Wu and Y. Wang and J. Hu and Q. Wang},
  title    = {Seeing is Believing: Vision-Driven Non-Crash Functional Bug Detection for Mobile Apps},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2025},
  abstract = {Mobile app GUI (Graphical User Interface) pages now contain rich visual information, with the visual semantics of each page helping users understand the application logic. However, these complex visual and functional logics present new challenges to software testing. Existing automated GUI testing methods, constrained by the lack of reliable testing oracles, are limited to detecting crash bugs with obvious abnormal signals. Consequently, many non-crash functional bugs, ranging from unexpected be...}
}

@inproceedings{huang2025research,
  author    = {J. Huang},
  title     = {Research on Multi-Model Fusion Machine Learning Demand Intelligent Forecasting System in Cloud Computing Environment},
  booktitle = {2025 2nd International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS)},
  year      = {2025},
  abstract  = {Background: Large language models (LLMs) are increasingly being used for automated unit test generation, but reported performance varies across tasks and datasets, and key aspects such as assertion plausibility and target coverage are not well understood. Methods: We perform a structured evaluation of LLM-based test generation on recent benchmarks and settings, summarizing research results in hinting, static analysis guidance, multi-agent work frameworks, and oracle generation; we compare LLMs w...}
}

@inproceedings{stojanović2024unit,
  author    = {D. Stojanović and B. Pavković and N. Četić and M. Krunić and L. Vidaković},
  title     = {Unit Test Generation Multi-Agent AI System for Enhancing Software Documentation and Code Coverage},
  booktitle = {2024 32nd Telecommunications Forum (TELFOR)},
  year      = {2024},
  abstract  = {Software development necessitates a robust testing plan though test development can be laborious and nonappealing task. We explore the utilization of the application artificial intelligence agents for generating and executing unit tests, enhancing the “Mostly Basic Python Problems” dataset. We employ behavior-driven development within a three-agent system to generate user stories and unit tests. Empirical results indicate improvements in branch coverage, illustrating the effective utilization of...}
}

@inproceedings{n/a2025large,
  author    = {N/A},
  title     = {Large Language Model Supply Chain: A Research Agenda},
  booktitle = {TOSEM '25},
  year      = {2025},
  abstract  = {Defines and analyzes the "supply chain" of LLMs, including infrastructure, foundation models, and downstream applications like autonomous agents and testing tools.}
}

@inproceedings{n/a2026a,
  author    = {N/A},
  title     = {A Survey on Large Language Models for Code Generation},
  booktitle = {TOSEM},
  year      = {2026},
  abstract  = {A comprehensive survey on "Code LLMs," covering evolution, advanced techniques (autonomous agents, RAG), and comparative analysis on benchmarks like HumanEval, MBPP, and SWE-bench.}
}

@inproceedings{n/a2025repairagent,
  author    = {N/A},
  title     = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
  booktitle = {ICSE '25},
  year      = {2025},
  abstract  = {Introduces RepairAgent, an autonomous agent that uses an LLM and a finite state machine to plan and execute actions (invoking tools) to fix software bugs.}
}

@inproceedings{n/a2025demystifying,
  author    = {N/A},
  title     = {Demystifying LLM-Based Software Engineering Agents},
  booktitle = {PACMSE (FSE)},
  year      = {2025},
  abstract  = {A study analyzing "Agentless" approaches versus complex agentic frameworks. It shows that simple, multi-phase LLM approaches can outperform complex agents on benchmarks like SWE-bench.}
}

@inproceedings{n/a2025a,
  author    = {N/A},
  title     = {A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs},
  booktitle = {ICSE '25},
  year      = {2025},
  abstract  = {Introduces AutoRestTest, which uses a multi-agent reinforcement learning approach combined with LLMs to generate inputs and test REST APIs more effectively.}
}

