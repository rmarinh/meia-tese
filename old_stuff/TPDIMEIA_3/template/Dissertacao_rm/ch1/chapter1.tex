\chapter{Introduction}
\label{chap:introduction}

This chapter provides a comprehensive overview of the research context, defining the problem scope and the motivation behind using Multi-Agent Systems for software testing. It outlines the central hypothesis guiding the work, the specific research objectives, and the expected scientific and technical contributions of this dissertation.

\section{Contextualization}
\label{sec:contextualization}

Testing is the bottleneck of modern software delivery. While developers can deploy code in minutes, ensuring that code is correct remains a manual, error-prone struggle. \textcite{winters2020software} note that as codebases grow, the cost of maintenance often outstrips the cost of creation. We have moved from manual clicking to scripted automation (like Selenium), but we are now entering a new phase: Agentic Testing.

Traditionally, automation meant writing code to test code. This created the "maintenance trap": every time the application changed, the tests broke. The arrival of Large Language Models (LLMs) offers a way out. By understanding the semantics of code, LLMs can theoretically write tests for us. However, generating a test from a simple prompt rarely works for complex software. A single model is "blind" to the millions of lines of code in a repository and lacks the feedback loops that human developers rely on.

This dissertation argues that the solution is not a better model, but a better system. We propose moving from "Generative AI" to "Agentic AI"---a team of specialized agents that can plan, write, execute, and fix tests iteratively, just as a human engineer would.

\subsubsection{The Economic Reality}
Testing is expensive. Estimates suggest that up to 50\% of engineering time is spent on verification and maintenance. The industry needs a solution that decouples test coverage from human effort. If autonomous agents can reliability generate even 30\% of the required unit tests, the economic impact would be substantial.

\subsubsection{Why Generative Agents?}
Old tools look for patterns; new agents "understand" intent. A fuzzer throws random patterns to find a crash. An LLM reads a function \texttt{calculate\_interest(rate)} and understands that \texttt{rate} implies specific boundary conditions (e.g., negative numbers). This semantic reasoning allows agents to generate tests that are meaningful, not just syntactically correct.

\section{Problem Definition}
\label{sec:problem_definition}

Despite the promise of agentic AI, the automated generation of reliable, executable test cases for enterprise software remains a complex and unsolved engineering challenge. The core problem lies in the disconnect between the generative capabilities of Large Language Models (LLMs) and the strict correctness requirements of software execution environments. While LLMs excel at pattern matching and generating syntactically plausible code, they lack an inherent understanding of the specific runtime constraints, internal dependencies, and business logic of proprietary codebases.

This disconnect manifests as a "Grounding Gap": the model operates in a probabilistic text space, while the compiler operates in a deterministic logic space. A single hallucinated method call or incorrect import renders an entire test suite useless. Furthermore, existing tools often treat test generation as a one-off "fire-and-forget" task, failing to mimic the human engineering process of writing, executing, analyzing error logs, and iteratively refining the code. The absence of this feedback loop prevents autonomous agents from self-correcting, leading to high-maintenance test artifacts that require significant human intervention to function.

The central problem addressed by this dissertation is the inability of current single-agent Large Language Model approaches to autonomously generate correct, executable, and high-coverage test suites for complex enterprise software systems. This failure stems from three specific deficiencies:
\begin{enumerate}
    \item The Oracle Problem: Single-prompt models cannot reliably determine the "correct" expected behavior of code without execution, leading to tests that assert incorrect values or hallucinate non-existent functionality.
    \item Contextual Blindness: Models lack access to the broader repository context (e.g., file structure, installed libraries, custom utilities), resulting in generated code that fails to compile due to missing dependencies or incorrect paths.
    \item Open-Loop Generation: Current systems lack a mechanism for iterative refinement based on compiler and runtime feedback, preventing them from correcting simple syntax errors or logic bugs that a human developer would fix immediately.
\end{enumerate}

\section{Objective and Hypothesis}
\label{sec:objective_hypothesis}

This research is driven by the central hypothesis that a Multi-Agent System, specifically one composed of specialized roles with access to an execution environment and iterative feedback loops, will significantly outperform single-prompt Large Language Models in the generation of valid, executable, and high-coverage test cases for complex software repositories.

The primary goal of this research is to design, implement, and evaluate a Multi-Agent System (MAS) Framework that orchestrates specialized autonomous agents to generate, validate, and refine software tests. The framework aims to bridge the "Grounding Gap" by integrating Agent-Computer Interfaces (ACI) that allow agents to interact with real execution environments, thereby achieving higher rates of functional correctness and code coverage than single-agent baselines.

To achieve this general objective, four specific objectives are defined. First, the research aims to define a taxonomy of specialized agent roles (e.g., Planner, Coder, Tester, Reviewer) and their interaction protocols to mimic a collaborative engineering workflow. Second, it seeks to design and implement an Agent-Computer Interface (ACI) that provides agents with safe, controlled access to external tools, including file system navigation, static linters, and test runners (e.g., PyTest). Third, the project will develop a "Self-Healing" feedback loop mechanism that enables agents to parse execution error logs (stderr) and iteratively refine their generated code to resolve compilation and logic errors. Finally, the proposed framework will be empirically evaluated using standard industry benchmarks (e.g., SWE-bench), measuring key performance indicators such as Pass Rate (Pass@1), Code Coverage percentage, and the reduction in human intervention required.

\section{Expected Results and Contributions}
\label{sec:contributions}

The contributions of this dissertation are multifaceted, addressing both the scientific advancement of Automated Software Engineering and the practical needs of the software industry.

\subsection{Scientific Contributions}
This research contributes to the scientific body of knowledge by:
\begin{itemize}
    \item \textbf{Taxonomy of Agentic Roles:} Defining a rigorous classification of agent responsibilities in the testing domain, moving beyond generic "chatbots" to role-specific prompting strategies (e.g., the specific cognitive load of a "Test Designer" vs. a "QA Auditor").
    \item \textbf{Feedback Loop Dynamics:} Providing empirical evidence on how compiler and runtime feedback signals influence the convergence rate of LLM-generated code. This helps quantify the value of "Grounding" in probabilistic generation.
    \item \textbf{Ethical Framework for Autonomous Development:} Offering a structured analysis of the ethical implications of replacing human verification with agentic consensus, contributing to the debate on "Human-in-the-Loop" governance.
\end{itemize}

\subsection{Technical Contributions}
From a technical and informatic perspective, the dissertation delivers:
\begin{itemize}
    \item \textbf{MAS Framework Implementation:} A reusable, modular framework enabling the orchestration of multiple LLM agents (using tools like Letta or LangGraph) to autonomously navigate a repository and generate tests.
    \item \textbf{Agent-Computer Interface (ACI) Design:} A concrete implementation of an ACI tailored for software testing, including safe sandboxing of agent-executed code and structured error parsing.
    \item \textbf{Self-Healing Pipeline:} A robust CI/CD-compatible pipeline where agents act as "maintenance bots," automatically attempting to fix broken tests before alerting human developers.
\end{itemize}

\section{Expected Results}
The expected outcomes of this research include:
\begin{itemize}
    \item A fully functional prototype capable of generating unit tests for Python repositories with a compilation success rate exceeding 80\% (compared to <50\% for zero-shot baselines).
    \item A comprehensive benchmark dataset comparing the proposed MAS approach against standard baselines (e.g., GPT-4o, GitHub Copilot) on the SWE-bench Light dataset.
    \item A reduction in the "Human Effort" metric, measured by the number of manual edits required to make a generated test suite passable.
\end{itemize}

\section{Document Structure}
\label{sec:structure}

The remainder of this document is organized as follows:
\begin{itemize}
    \item Chapter 2: State of the Art provides a systematic literature review, analyzing the evolution of MAS in testing, current architectural patterns, and validation methodologies.
    \item Chapter 3: Methodology \& Tools details the proposed system architecture, the selection of the Letta framework, and the design of the experimental setup.
    \item Chapter 4: Ethical, Legal, and Privacy Considerations analyzes the specific privacy risks identified in the SLR, discusses compliance with the EU AI Act, and proposes a governance framework (DPIA).
    \item Chapter 5: Experimentation [Future Work] will present the results of the empirical evaluation and the analysis of the collected data.
    \item Chapter 6: Conclusions and Future Work summarizes the research proposal and outlines the roadmap for the completion of the dissertation.
\end{itemize}
