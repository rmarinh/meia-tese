\chapter{Experimentation}
\label{chap:experimentation}

This chapter details the experimental protocol designed to validate the central hypothesis of this dissertation: that a Multi-Agent System (MAS) grounded in execution feedback significantly outperforms single-prompt Large Language Models in automated test generation. As this is a dissertation proposal, this chapter describes the \textit{planned} evaluation methodology, including the research questions, dataset selection, baseline comparisons, and the metrics for success.

\section{Experimental Questions}
\label{sec:exp_questions}

To guide the evaluation of the proposed framework, the following Experimental Questions (EQs) are formulated, directly mapping to the specific objectives defined in Chapter 1:

\begin{itemize}
    \item \textbf{EQ1 (Effectiveness):} Does the proposed MAS framework achieve a higher Pass@1 rate on the SWE-bench Light dataset compared to standard zero-shot LLM baselines?
    \item \textbf{EQ2 (Quality):} Do the tests generated by the "Planner-Coder" agentic loop achieve higher Line and Branch Coverage than those generated by a single model?
    \item \textbf{EQ3 (Efficiency):} What is the "Self-Correction Rate" of the system? Specifically, what percentage of initially failing tests are successfully repaired by the agent after analyzing the execution stderr?
    \item \textbf{EQ4 (Cost):} What is the computational cost (token usage and latency) of the multi-agent approach compared to the single-agent baseline, and is the performance gain justified?
\end{itemize}

\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Dataset Selection}
The evaluation will be conducted using the \textbf{SWE-bench Light} dataset \parencite{jimenez2024swebench}. This dataset is chosen over algorithmic benchmarks (like HumanEval) because it consists of real-world GitHub issues from popular Python libraries (e.g., Django, scikit-learn, flask). 

\begin{itemize}
    \item \textbf{Composition:} The dataset contains 300 verified "Fail-to-Pass" test cases paired with the corresponding code fixes.
    \item \textbf{Task:} The agent will be tasked with taking an issue description and the codebase as input, and generating a reproduction script (test) that fails on the buggy version and passes on the fixed version.
    \item \textbf{Gold Standard:} Success will be measured by running the generated test against the ground truth "Gold Patch" provided by the dataset maintainers.
\end{itemize}

\subsection{Baselines}
To isolate the impact of the agentic architecture, the proposed solution will be compared against two distinct baselines:

\begin{enumerate}
    \item \textbf{Baseline A (Zero-Shot GPT-4o):} A standard, stateless prompt asking the model to "Write a reproduction script for this issue" without access to the repository structure or execution tools. This represents the current state of "Chat with Code" interfaces.
    \item \textbf{Baseline B (RAG-Only):} An agent augmented with Retrieval-Augmented Generation (RAG) to access file contents but lacking the "Executor" role. This baseline verifies whether context alone is sufficient, or if execution feedback is truly necessary.
\end{enumerate}

\section{Evaluation Metrics}
\label{sec:metrics}

The performance of the system will be quantified using the following metrics:

\begin{itemize}
    \item \textbf{Pass@1 (\%):} The percentage of issues for which the agent generates a valid, passing reproduction script on the first attempt (or after the allowed self-correction turns).
    \item \textbf{Code Coverage (\%):} Measured using \texttt{coverage.py}, this metric will calculate the percentage of lines in the modified files that are executed by the generated test suite.
    \item \textbf{Human Effort (Edits):} A proxy metric estimated by counting the Levenshtein distance between the agent's final generated code and a working solution. Lower distance implies less manual intervention required by the developer.
\end{itemize}

\section{Execution Procedure}
\label{sec:procedure}

The experiment will follow a strict pipeline executed within a Dockerized sandbox to ensuring reproducibility and safety:

\begin{enumerate}
    \item \textbf{Ingestion:} The "Planner" agent will read the repository structure and the issue description.
    \item \textbf{Generation:} The "Coder" agent will draft an initial test file.
    \item \textbf{Validation Loop (The Intervention):}
    \begin{itemize}
        \item The "Executor" will run the test with \texttt{pytest}.
        \item If the test fails or errors (e.g., \texttt{ImportError}), the stdout/stderr will be fed back to the Coder.
        \item The Coder will attempt to fix the error (Max 5 iterations).
    \end{itemize}
    \item \textbf{Adjudication:} The final test case will be executed against the Gold Patch. If it passes, the instance will be marked as "Resolved."
\end{enumerate}

\section{Threats to Validity}
\label{sec:threats}

\subsection{Internal Validity}
The non-deterministic nature of LLMs means results may vary between runs. To mitigate this, experiments will be repeated three times (Temperature = 0.7), and the average Pass@1 reported.

\subsection{External Validity}
While SWE-bench represents real-world Python code, the results may not generalize to other languages (e.g., Java, C++) or proprietary logic which differs from open-source patterns. Future work will address this by extending the benchmark to multi-language repositories.
