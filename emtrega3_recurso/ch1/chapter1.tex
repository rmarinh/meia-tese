% Chapter 1 - Introduction

\chapter{Introduction}
\label{chap:introduction}

%----------------------------------------------------------------------------------------

\section{Contextualisation}
\label{sec:contextualisation}

This section situates the dissertation within the broader landscape of software testing, tracing the evolution from manual testing practices to LLM-powered multi-agent approaches and motivating the need for the research presented in subsequent chapters.

Software testing remains one of the most critical yet resource-intensive activities in the \acrfull{SDLC}. Industry estimates consistently indicate that testing activities consume between 30\% and 50\% of total development effort \parencite{ammann2016introduction}, while inadequate testing continues to cost the global economy billions annually through software failures, security breaches, and system downtime. According to the Consortium for Information and Software Quality (CISQ), the cost of poor software quality in the United States alone exceeded \$2.4 trillion in 2022, with a substantial portion attributable to inadequate testing practices \parencite{krasner2022cost}. Organizations allocate an average of 23\% of their IT budgets to quality assurance and testing activities, yet many report dissatisfaction with their testing coverage and effectiveness.

The rise of agile and DevOps practices has intensified these challenges. Modern development methodologies emphasize rapid iteration, continuous integration, and frequent releases, often multiple times per day. This acceleration places enormous pressure on testing processes, which must keep pace with development velocity while maintaining quality standards. Traditional manual testing approaches simply cannot scale to meet these demands, and even script-based automation requires substantial upfront investment and ongoing maintenance as the \acrfull{SUT} evolves.

The emergence of \acrfull{LLM} technology has opened fundamentally new possibilities for addressing these challenges. Models such as GPT-4, Claude, and open-weight alternatives like LLaMA \parencite{brown2020language} have demonstrated remarkable capabilities in understanding and generating code, comprehending natural language specifications, and reasoning about software behaviour \parencite{fan2023large}. Unlike prior automated test generation techniques, such as search-based approaches (EvoSuite \parencite{fraser2011evosuite}, Randoop), which operate on syntactic or structural properties and suffer from the ``oracle problem'', LLM-based approaches can understand code semantics, infer intended behaviour from context, and generate human-readable test code with meaningful assertions.

However, the complexity of comprehensive software testing often exceeds the capabilities of single-model approaches. Real-world testing involves multiple interrelated activities: understanding the target application, identifying testable surfaces, generating appropriate test cases, executing tests, and validating results. This complexity motivates the exploration of \acrfull{MAS} architectures, where multiple specialised agents collaborate to address different aspects of the testing challenge. Recent frameworks such as MetaGPT \parencite{hong2023metagpt}, ChatDev \parencite{qian2024chatdev}, and SWE-agent \parencite{yang2024sweagent} have demonstrated the viability of multi-agent approaches for software engineering tasks, employing hierarchical or peer-to-peer coordination models with role-based specialisation.

This dissertation investigates how a multi-agent system, combining LLM-powered test generation with two complementary input strategies, learning from existing test examples and observing application behaviour, can automate the production of high-quality integration and end-to-end tests. The central contribution is \textbf{TestForge}, a modular platform that implements both approaches and integrates persistent agent memory for progressive learning across sessions.

%----------------------------------------------------------------------------------------

\section{Description of the Problem}
\label{sec:problem_description}

This section identifies the key challenges that current automated test generation approaches face, from shallow test quality and lack of application context to input modality constraints and privacy concerns, establishing the problem space that this dissertation addresses.

Despite significant advances in LLM capabilities, automated test generation faces several fundamental challenges that limit practical adoption. The most immediate concern is the quality of generated tests: most existing tools produce shallow ``smoke tests'' that verify basic connectivity, for example, confirming that an endpoint returns a 200 status code, without probing state integrity, boundary conditions, business logic violations, or data consistency. The resulting tests find few real bugs and provide a false sense of coverage.

A closely related issue is the lack of application context. Single-shot generation approaches treat each invocation independently, with no memory of previous interactions, discovered endpoints, or known patterns. This prevents the system from building a progressively deeper understanding of the target application over time. The problem is compounded by what can be termed the cold-start challenge: when no reference tests exist, generators lack information about project conventions, assertion styles, fixture patterns, and the testing framework in use. Conversely, when high-quality reference tests are available, current tools fail to leverage them effectively as few-shot examples.

Beyond test quality, existing approaches suffer from input modality limitations. Most tools require explicit endpoint definitions, OpenAPI specifications, or direct source code access. Many real-world applications, however, lack formal API documentation, making it difficult to determine what needs testing without observing the application in operation. Furthermore, comprehensive test generation requires multiple distinct activities (analysis, generation, execution, and validation) that benefit from specialised treatment, yet monolithic single-agent systems struggle to coordinate this complexity effectively. Finally, organisations handling sensitive data face legitimate concerns about transmitting proprietary code to cloud-based LLM providers, and the evolving regulatory landscape surrounding the GDPR and the EU AI Act adds further constraints that motivate local execution capabilities.

This dissertation addresses these challenges through a dual-pipeline approach implemented as a multi-agent platform. The \textbf{Golden Examples Pipeline} analyses existing high-quality test files using \acrfull{AST} parsing to extract patterns, conventions, and testing strategies, then uses few-shot prompting to generate new tests that follow the same style while targeting untested scenarios. Complementing this, the \textbf{Black-Box Observer Pipeline} captures HTTP traffic from a running application (via proxy interception or HAR file import), maps discovered endpoints and their schemas, and generates tests based on observed behaviour without requiring source code access. Both pipelines are orchestrated by a multi-agent system where specialised agents handle analysis, generation, execution, and validation. A persistent memory layer, implemented through the Letta framework, enables the system to accumulate knowledge about target applications across sessions, progressively improving test quality.

%----------------------------------------------------------------------------------------

\section{Research Questions and Objectives}
\label{sec:research_questions}

This section presents the central research question that drives the thesis, the objective that the dissertation aims to fulfil, and the milestones that structure the research programme.

The thesis is structured around an overarching investigation question that guides the systematic review and the entire research programme:

\begin{quote}
\textit{How can a multi-agent system, powered by large language models and informed by both existing test examples and observed application behaviour, automate the generation of high-quality integration and end-to-end tests while preserving data privacy?}
\end{quote}

This question will be answered progressively throughout the dissertation: Chapter~\ref{chap:literature_review} surveys the state of the art and decomposes the question into specific review themes, Chapter~\ref{chap:data_collection} and Chapter~\ref{chap:methods_tools} describe the data strategies and architectural design adopted, Chapter~\ref{chap:implementation} presents the prototype implementation, and Chapter~\ref{chap:experimentation} evaluates the results empirically.

\subsection{Thesis Objective}

The objective of this dissertation is to design, implement, and evaluate a multi-agent platform, TestForge, that automates the generation of high-quality integration and end-to-end tests for web applications. The platform must support two complementary input strategies: a golden examples pipeline that learns testing patterns from existing high-quality test files through \acrfull{AST} analysis and few-shot prompting, and a black-box observer pipeline that discovers API endpoints by capturing HTTP traffic from a running application and generates tests from observed behaviour without requiring source code access. The system must coordinate multiple specialised agents across analysis, generation, execution, and validation stages, while incorporating persistent memory so that the platform accumulates knowledge about target applications across sessions. Critically, the entire platform, including LLM inference, must be capable of running on local infrastructure using open-weight models, ensuring that organisations retain full control over their proprietary code and comply with data protection regulations such as the GDPR and the EU AI Act. The thesis validates this objective through a systematic literature review following \acrfull{PRISMA} methodology and an empirical evaluation against a real-world application, measuring test quality, bug detection capability, and practical deployment feasibility.

\subsection{Milestones}

The research programme is organised around six milestones, each corresponding to a verifiable deliverable:

\begin{enumerate}
    \item \textbf{M1: Systematic Literature Review}: Completion of the PRISMA-based review covering MAS architectures, LLM-driven testing, and security considerations, resulting in a synthesis of 55 studies and identification of research gaps.

    \item \textbf{M2: Platform Architecture}: Definition of the dual-pipeline architecture with six specialised agents, typed inter-agent communication, and persistent memory integration.

    \item \textbf{M3: Golden Examples Pipeline}: Implementation and validation of AST-based pattern extraction (Tree-sitter) and LLM few-shot test generation from reference test files.

    \item \textbf{M4: Black-Box Observer Pipeline}: Implementation and validation of HTTP traffic capture, endpoint mapping, and LLM-driven test generation from observed behaviour.

    \item \textbf{M5: Experimental Evaluation}: End-to-end evaluation of the platform against a real-world application, measuring test quality, bug detection, and generation effectiveness.

    \item \textbf{M6: Practical Deployment Assessment}: Evaluation of local execution feasibility, privacy compliance, and cost-effectiveness compared to cloud-based alternatives.
\end{enumerate}

%----------------------------------------------------------------------------------------

\section{Contributions of the Dissertation}
\label{sec:contributions}

This section summarises the main contributions that emerge from the research programme.

The first contribution is a systematic literature review of 55 studies, conducted following PRISMA methodology, covering MAS architectures for testing, LLM-driven test generation, security considerations, and practical deployment challenges. This review identifies research gaps and establishes the theoretical foundation for the platform design.

The second contribution is the golden examples approach, a method for extracting testing patterns from existing test suites using Tree-sitter AST parsing and leveraging them as few-shot examples for LLM-based generation. This approach produces insightful tests that target state integrity, boundary conditions, and business logic rather than shallow smoke tests. 

The third contribution is the black-box observer approach, which automatically maps API endpoints from captured HTTP traffic and generates test suites without requiring source code access or formal API specifications.

The fourth and central contribution is TestForge itself: a modular, open-source multi-agent platform implementing both approaches with six specialised agents (Observer, Mapper, Analyser, Generator, Executor, Validator), persistent memory via Letta, and support for fully local execution using open-weight LLMs. The fifth contribution is an empirical evaluation demonstrating the platform's effectiveness on a real-world application, with analysis of test quality, bug detection capabilities, and the impact of progressive learning. Finally, the sixth contribution provides a practical assessment of privacy, performance, and cost trade-offs for deploying LLM-based test generation on local infrastructure, including considerations related to the GDPR and the EU AI Act regulatory framework.

%----------------------------------------------------------------------------------------

\section{Document Structure}
\label{sec:document_structure}

This section provides a roadmap of the dissertation, outlining the purpose and content of each chapter so the reader can navigate the document effectively.

This thesis is organised into eight chapters that progressively address the research questions and objectives. Following this introduction, Chapter~\ref{chap:literature_review} presents a systematic review of the literature following PRISMA methodology, covering MAS architectures for testing, LLM-driven test generation, example-based learning approaches, black-box testing techniques, and security considerations. Chapter~\ref{chap:data_collection} describes the data sources used by both pipelines, including golden test examples with AST-based pattern extraction, HTTP traffic capture and HAR file processing, and the data representations applied before generation. Chapter~\ref{chap:methods_tools} presents the system architecture, including the dual-pipeline design, the six-agent multi-agent architecture, LLM integration strategy, and persistent memory via Letta. Chapter~\ref{chap:implementation} details the prototype implementation of TestForge, covering the development environment, implementation of each agent, the Streamlit web interface, and challenges encountered. Chapter~\ref{chap:experimentation} presents the experimental evaluation, including the setup, metrics, results from the Golden Examples pipeline, and discussion of findings in relation to the research questions. Chapter~\ref{chap:data_protection} discusses privacy-by-design considerations, security measures, GDPR compliance, and ethical implications. Finally, Chapter~\ref{chap:conclusions} summarises the contributions, answers the research questions, discusses limitations, and outlines directions for future research. Appendices provide supplementary materials including PRISMA data extraction tables, extended code listings, and additional experimental results.
