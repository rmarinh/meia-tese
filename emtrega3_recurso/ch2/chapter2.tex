% Chapter 2 - Systematic Review of the Literature

\chapter{Systematic Review of the Literature}
\label{chap:literature_review}

This chapter presents a comprehensive systematic literature review following the \acrfull{PRISMA} methodology to ensure rigour, transparency, and reproducibility. The review is guided by its own overarching research question, distinct from the thesis question presented in Chapter~\ref{chap:introduction}, which surveys how artificial intelligence techniques have been applied to automated software test generation. This question is decomposed into five research questions (RQ1--RQ5) that structure the search strategy, thematic analysis, and synthesis. The chapter begins with a detailed description of the review methodology (Section~\ref{sec:methodology}), presents the results organised by research question with the full thematic analysis for each (Section~\ref{sec:results}), provides a critical discussion of the findings and identification of research gaps (Section~\ref{sec:critical_discussion}), and concludes with a summary of the review, its limitations, and directions for future work (Section~\ref{sec:review_conclusion}).

%----------------------------------------------------------------------------------------

\section{Methodology}
\label{sec:methodology}

This systematic literature review follows the PRISMA 2020 guidelines \parencite{page2021prisma} and incorporates recommendations from established software engineering research methodology \parencite{kitchenham2007guidelines}. The methodology encompasses research question formulation, search strategy definition, study selection criteria, quality assessment, and data extraction procedures.

\subsection{Review Question and Research Questions}

The systematic review is guided by its own overarching research question, distinct from the thesis question presented in Chapter~\ref{chap:introduction}. While the thesis question concerns the design and evaluation of a specific platform (TestForge), the review question surveys the broader state of the art in AI-driven test generation to establish the theoretical foundation upon which the thesis builds:

\begin{quote}
\textit{How have artificial intelligence techniques, particularly large language models and multi-agent systems, been applied to automated software test generation, and what are the current capabilities, limitations, and open challenges?}
\end{quote}

To answer this question systematically, the review decomposes it into five research questions (RQ). Each question targets a distinct aspect of the problem space, and together they provide a comprehensive mapping of the existing literature that informs the design decisions made in subsequent chapters.

RQ1 asks: \textit{What techniques have been used to leverage existing test examples and few-shot prompting for AI-driven test generation, and what quality improvements do they achieve?} This question examines how reference tests, code demonstrations, and in-context learning strategies have been employed to guide LLMs toward generating higher-quality tests, covering retrieval-based example selection, mutation-guided feedback, and AST-based pattern extraction.

RQ2 asks: \textit{How have black-box testing and traffic analysis methods been combined with AI for automatic endpoint discovery and test suite creation?} This question surveys techniques for testing APIs without source code access, including stateful fuzzing, specification inference, and the emerging use of LLMs for generating tests from observed HTTP exchanges.

RQ3 asks: \textit{What multi-agent system architectures have been proposed for software engineering and testing tasks, and how do coordination models, role specialisation, and communication patterns affect effectiveness?} This question investigates the design space for multi-agent frameworks, comparing hierarchical, peer-to-peer, and hybrid coordination models, and identifying the role decomposition patterns most relevant to automated testing.

RQ4 asks: \textit{How have persistent memory mechanisms been used in LLM-based agent systems, and what is their potential for improving test generation quality across sessions?} This question explores memory architectures for language agents, including virtual context management, reflective memory, and retrieval-augmented generation, and evaluates whether they have been applied to software testing.

RQ5 asks: \textit{What are the practical considerations, including privacy, security, cost, and regulatory compliance, for deploying AI-based test generation systems, particularly with local execution?} This question addresses the deployment landscape, covering the trade-offs between proprietary and open-weight models, security threats specific to agent-based systems, mitigation strategies, and the requirements imposed by the GDPR and the EU AI Act.

The search strategy, inclusion criteria, and thematic analysis sections that follow are organised around these five research questions. The Results section consolidates the findings and thematic analysis for each question, the Critical Discussion provides a critical analysis per question, and the Conclusions section summarises the state of the field and identifies the research gaps that motivate this thesis.

\subsection{Search Strategy}

The literature search employed multiple complementary search strings targeting different aspects of the research questions. Searches were conducted across IEEE Xplore, ACM Digital Library, Scopus, and arXiv to capture both peer-reviewed publications and recent preprints in this rapidly evolving field.

The first search string (S1) targeted multi-agent systems and test generation, addressing RQ1 and RQ3:

\begin{verbatim}
("Multi-Agent" OR "MAS" OR "LLM" OR "Large Language Model"
 OR "GPT" OR "Code Generation")
AND
("Software Testing" OR "Test Generation" OR "Automated Testing"
 OR "Unit Testing" OR "Integration Testing" OR "End-to-End Testing")
\end{verbatim}

The second search string (S2) focused on few-shot and example-driven testing for RQ1:

\begin{verbatim}
("Few-Shot" OR "In-Context Learning" OR "Example-Driven"
 OR "Example-Based" OR "Prompt Engineering")
AND
("Test Generation" OR "Test Case" OR "Unit Test" OR "Code Generation")
AND
("LLM" OR "Large Language Model" OR "GPT" OR "Codex" OR "Copilot")
\end{verbatim}

The third search string (S3) targeted black-box testing and API analysis for RQ2:

\begin{verbatim}
("Black-Box" OR "Black Box" OR "REST API" OR "API Testing"
 OR "Traffic Analysis" OR "HTTP" OR "API Discovery")
AND
("Test Generation" OR "Automated Testing" OR "Fuzzing"
 OR "Specification Inference" OR "OpenAPI")
\end{verbatim}

The fourth search string (S4) addressed agent memory and stateful systems for RQ4:

\begin{verbatim}
("Agent Memory" OR "Persistent Memory" OR "Long-Term Memory"
 OR "Stateful Agent" OR "MemGPT" OR "Memory-Augmented")
AND
("LLM" OR "Large Language Model" OR "Agent" OR "Multi-Agent")
\end{verbatim}

The fifth search string (S5) covered architecture, effectiveness, and deployment for RQ3 and RQ5:

\begin{verbatim}
("Multi-Agent" OR "Agent Framework" OR "LLM Agent")
AND
("Architecture" OR "Effectiveness" OR "Performance"
 OR "Benchmark" OR "Evaluation" OR "Security" OR "Privacy"
 OR "Local Deployment" OR "Open-Weight")
\end{verbatim}

The search period covered publications from January 2020 to January 2026, capturing the emergence of modern LLMs (beginning with GPT-3) through to current developments. Searches were conducted across IEEE Xplore, ACM Digital Library, Scopus, and arXiv. Reference lists of included studies were manually screened to identify additional relevant works (snowballing).

\subsection{Inclusion and Exclusion Criteria}

To ensure that only relevant and methodologically sound studies were retained, a set of inclusion and exclusion criteria was defined prior to screening. The inclusion criteria were designed to capture the full breadth of the five research questions, encompassing multi-agent architectures, LLM-based test generation, example-driven approaches, black-box testing, agent memory, and practical deployment concerns. The exclusion criteria filtered out studies that fell outside the scope of the review or that lacked the methodological rigour necessary for meaningful synthesis. Both sets of criteria were applied consistently at the title/abstract screening and full-text assessment stages. Table~\ref{tab:inclusion_criteria} and Table~\ref{tab:exclusion_criteria} present the complete criteria.

\begin{table}[ht]
\caption{Inclusion criteria}
\label{tab:inclusion_criteria}
\centering
\small
\begin{tabular}{c p{11cm}}
\toprule
\tabhead{ID} & \tabhead{Criterion} \\
\midrule
IC1 & Studies presenting MAS architectures for software testing or development \\
IC2 & Studies with empirical evaluation of LLM-based testing tools \\
IC3 & Studies reporting quantitative metrics (coverage, bug detection, pass@k) \\
IC4 & Framework papers describing multi-agent systems (MetaGPT, ChatDev, SWE-agent, etc.) \\
IC5 & Studies on few-shot, example-driven, or in-context learning for code/test generation \\
IC6 & Studies on black-box testing, API discovery, or traffic-based test generation \\
IC7 & Studies on agent memory architectures or stateful LLM agents \\
IC8 & Studies addressing security, privacy, or practical deployment concerns in LLM-based systems \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Exclusion criteria}
\label{tab:exclusion_criteria}
\centering
\small
\begin{tabular}{c p{11cm}}
\toprule
\tabhead{ID} & \tabhead{Criterion} \\
\midrule
EC1 & Single-agent LLM approaches without multi-agent coordination (unless providing essential baseline comparisons) \\
EC2 & Studies lacking empirical validation or technical depth \\
EC3 & Non-English publications \\
EC4 & Opinion pieces, editorials, or short papers without substantive technical content \\
EC5 & Studies focused exclusively on non-testing applications (e.g., pure code generation without testing) \\
EC6 & Duplicate publications or extended versions superseded by later work \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Study Selection Process}

The study selection followed a multi-stage screening process designed to progressively narrow the pool of candidate studies while maintaining transparency and reproducibility. In the first stage, titles and abstracts were reviewed against the inclusion and exclusion criteria defined above; this screening was performed independently by two reviewers, with disagreements resolved through discussion. Studies that passed the initial screening proceeded to a full-text assessment, during which each article was read in its entirety and evaluated against the complete set of criteria, with explicit documentation of the reason for any exclusion. Following this, a quality assessment was applied to the remaining studies to evaluate methodological rigour, clarity of reporting, and validity of conclusions. Finally, a snowballing step, encompassing both forward and backward reference searching from the included studies, was conducted to identify additional relevant works that may have been missed by the database searches.

\subsection{Quality Assessment}

Each included study was assessed using an adapted quality checklist comprising seven criteria (Table~\ref{tab:quality_criteria}): whether the study presents a clear statement of research objectives, whether it employs an appropriate research methodology, whether the experimental setup is described in sufficient detail, whether the evaluation metrics are valid and reliable, whether appropriate statistical analysis is applied where applicable, whether the authors discuss limitations and threats to validity, and whether the results are reproducible. Each criterion was scored as fully met (1), partially met (0.5), or not met (0), yielding a maximum score of 7. Studies scoring below 4 were flagged for careful consideration of their contribution weight in the synthesis, though none were excluded solely on this basis.

\begin{table}[ht]
\caption{Quality assessment criteria}
\label{tab:quality_criteria}
\centering
\small
\begin{tabular}{c l l}
\toprule
\tabhead{ID} & \tabhead{Criterion} & \tabhead{Scoring} \\
\midrule
QC1 & Clear research objectives & 0/0.5/1 \\
QC2 & Appropriate methodology & 0/0.5/1 \\
QC3 & Adequate experimental setup description & 0/0.5/1 \\
QC4 & Valid and reliable metrics & 0/0.5/1 \\
QC5 & Appropriate statistical analysis & 0/0.5/1 \\
QC6 & Discussion of limitations & 0/0.5/1 \\
QC7 & Reproducibility of results & 0/0.5/1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Extraction}

A structured data extraction form was applied to each included study (Table~\ref{tab:extraction_form}). For every study, the form captured bibliographic details (authors, year, venue, and publication type), the review themes addressed, and the research methodology employed (empirical study, framework proposal, survey, or case study). Where applicable, the form recorded the agent architecture, including the coordination model, the number and roles of agents, and the communication patterns used. The specific LLMs employed, model names, versions, and configurations, were noted alongside the evaluation metrics and quantitative results reported. Security and privacy concerns identified by the authors were extracted together with any mitigation strategies proposed. Finally, the form recorded the limitations acknowledged by each study and the key findings relevant to the review themes.

\begin{table}[ht]
\caption{Data extraction form template}
\label{tab:extraction_form}
\centering
\small
\begin{tabular}{l p{8cm}}
\toprule
\tabhead{Field} & \tabhead{Description} \\
\midrule
Study ID & Unique identifier (S01--S55) \\
Authors & Author names \\
Year & Publication year \\
Venue & Conference/journal name \\
Type & Conference paper / Journal article / Preprint \\
\midrule
RQs Addressed & Which of RQ1--RQ5 the study addresses \\
Methodology & Empirical study / Framework / Survey / Case study \\
\midrule
Agent Architecture & Coordination model, number of agents, roles \\
LLMs Used & Model names, versions, configurations \\
Evaluation Metrics & Coverage, pass@k, mutation score, etc. \\
Benchmarks Used & HumanEval, SWE-bench, Defects4J, etc. \\
\midrule
Security Concerns & Identified risks and vulnerabilities \\
Mitigations Proposed & Countermeasures and defensive techniques \\
\midrule
Key Findings & Primary results and conclusions \\
Limitations & Acknowledged limitations \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Results}
\label{sec:results}

This section presents the findings of the systematic review, organised by the five research questions introduced in the methodology. The first subsection describes the study selection process and the characteristics of the 55 included studies. The subsequent subsections present the full thematic analysis for each research question, synthesising the relevant evidence from the corpus.

\subsection{Study Selection and Characteristics}
\label{sec:study_selection}

The systematic search identified a substantial number of potentially relevant records across the searched databases. After removing duplicates, records underwent title and abstract screening. Studies clearly not meeting inclusion criteria were excluded, with the remaining records proceeding to full-text assessment. Figure~\ref{fig:prisma_flow} illustrates the complete study selection process following the PRISMA 2020 flow diagram, and Table~\ref{tab:prisma_flow} summarizes the numerical breakdown at each stage. Table~\ref{tab:search_results_detail} presents the detailed search results from each database, broken down by search string group.

\begin{figure}[htbp]
\centering
\resizebox{0.92\textwidth}{!}{%
\begin{tikzpicture}[
    >=stealth,
    node distance=1.0cm and 1.2cm,
    every node/.style={align=center, font=\small},
    flowbox/.style={
        rectangle, draw=black, thick,
        text width=7cm, minimum height=1.0cm, inner sep=6pt,
        fill=white
    },
    exclbox/.style={
        rectangle, draw=black, thick,
        text width=5.5cm, minimum height=0.8cm, inner sep=6pt,
        fill=gray!8
    },
    addbox/.style={
        rectangle, draw=black, thick,
        text width=5cm, minimum height=0.8cm, inner sep=6pt,
        fill=white
    },
    resultbox/.style={
        rectangle, draw=black, thick,
        text width=7cm, minimum height=1.0cm, inner sep=6pt,
        fill=white, line width=1.5pt
    },
    phase/.style={
        font=\small\bfseries, rotate=90, anchor=south
    },
]

\node[flowbox] (identified) {
    \textbf{Records identified through database searching}\\[3pt]
    (n\,=\,1{,}055)\\[2pt]
    {\scriptsize IEEE Xplore (n\,=\,234) $\cdot$ ACM DL (n\,=\,198)
    $\cdot$ Scopus (n\,=\,267) $\cdot$ arXiv (n\,=\,356)}
};

\node[exclbox, right=1.2cm of identified] (duplicates) {
    \textbf{Records removed before screening}\\[3pt]
    Duplicate records removed (n\,=\,287)
};

\node[flowbox, below=1.4cm of identified] (screened) {
    \textbf{Records screened}\\[3pt]
    (n\,=\,768)
};

\node[exclbox, right=1.2cm of screened] (excluded1) {
    \textbf{Records excluded}\\[3pt]
    (n\,=\,581)
};

\node[flowbox, below=1.4cm of screened] (fulltext) {
    \textbf{Reports assessed for eligibility}\\[3pt]
    (n\,=\,187)
};

\node[exclbox, right=1.2cm of fulltext, text width=5.5cm] (excluded2) {
    \textbf{Reports excluded} (n\,=\,140)\\[3pt]
    {\scriptsize Insufficient depth/no evaluation: 52\\
    Out of scope (no testing focus): 41\\
    Single-agent only: 28\\
    Non-English or inaccessible: 11\\
    Superseded by later version: 8}
};

\node[addbox, left=1.2cm of fulltext] (snowball) {
    \textbf{Additional records identified}\\
    \textbf{through snowballing}\\[3pt]
    (n\,=\,8)
};

\node[resultbox, below=1.4cm of fulltext] (included) {
    \textbf{Studies included in review}\\[3pt]
    \textbf{(n\,=\,55)}
};

\draw[->, thick] (identified) -- (screened);
\draw[->, thick] (identified) -- (duplicates);
\draw[->, thick] (screened) -- (fulltext);
\draw[->, thick] (screened) -- (excluded1);
\draw[->, thick] (fulltext) -- (included);
\draw[->, thick] (fulltext) -- (excluded2);
\draw[->, thick] (snowball) -- (fulltext);

\node[phase, left=0.5cm of identified.west, anchor=south]
    {\textsc{Identification}};
\node[phase, left=0.5cm of screened.west, anchor=south]
    {\textsc{Screening}};
\node[phase, left=0.5cm of fulltext.west, anchor=south]
    {\textsc{Eligibility}};
\node[phase, left=0.5cm of included.west, anchor=south]
    {\textsc{Included}};

\coordinate (sep1) at ($(identified.south)!0.5!(screened.north)$);
\coordinate (sep2) at ($(screened.south)!0.5!(fulltext.north)$);
\coordinate (sep3) at ($(fulltext.south)!0.5!(included.north)$);

\draw[dashed, gray!50] ([xshift=-2.0cm]sep1) -- ([xshift=9.5cm]sep1);
\draw[dashed, gray!50] ([xshift=-2.0cm]sep2) -- ([xshift=9.5cm]sep2);
\draw[dashed, gray!50] ([xshift=-2.0cm]sep3) -- ([xshift=9.5cm]sep3);

\end{tikzpicture}%
}% end resizebox
\caption{PRISMA 2020 flow diagram of the study selection process}
\label{fig:prisma_flow}
\end{figure}

\begin{table}[ht]
\caption{PRISMA study selection summary}
\label{tab:prisma_flow}
\centering
\begin{tabular}{l r}
\toprule
\tabhead{Stage} & \tabhead{Records} \\
\midrule
\textbf{Identification} & \\
\quad Records from IEEE Xplore & 234 \\
\quad Records from ACM Digital Library & 198 \\
\quad Records from Scopus & 267 \\
\quad Records from arXiv & 356 \\
\quad Total records identified & 1055 \\
\quad Duplicates removed & (287) \\
\midrule
\textbf{Screening} & \\
\quad Records screened (title/abstract) & 768 \\
\quad Records excluded & (581) \\
\midrule
\textbf{Eligibility} & \\
\quad Full-text articles assessed & 187 \\
\quad Full-text articles excluded & (140) \\
\quad Articles from snowballing & 8 \\
\midrule
\textbf{Included} & \\
\quad Studies included in review & \textbf{55} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Detailed search results by database and query}
\label{tab:search_results_detail}
\centering
\small
\begin{tabular}{l l r r}
\toprule
\tabhead{Database} & \tabhead{Search String} & \tabhead{Results} & \tabhead{After Dedup} \\
\midrule
\multirow{3}{*}{IEEE Xplore} & S1 (MAS + Testing) & 102 & 97 \\
 & S2--S4 (Few-Shot, Black-Box, Memory) & 78 & 68 \\
 & S5 (Architecture + Deployment) & 54 & 46 \\
\midrule
\multirow{3}{*}{ACM DL} & S1 (MAS + Testing) & 86 & 79 \\
 & S2--S4 (Few-Shot, Black-Box, Memory) & 68 & 56 \\
 & S5 (Architecture + Deployment) & 44 & 37 \\
\midrule
\multirow{3}{*}{Scopus} & S1 (MAS + Testing) & 112 & 94 \\
 & S2--S4 (Few-Shot, Black-Box, Memory) & 93 & 74 \\
 & S5 (Architecture + Deployment) & 62 & 49 \\
\midrule
\multirow{3}{*}{arXiv} & S1 (MAS + Testing) & 148 & 121 \\
 & S2--S4 (Few-Shot, Black-Box, Memory) & 126 & 97 \\
 & S5 (Architecture + Deployment) & 82 & 50 \\
\midrule
\textbf{Total} & & \textbf{1055} & \textbf{768} \\
\bottomrule
\end{tabular}
\end{table}

The primary reasons for exclusion at the full-text stage are detailed in Table~\ref{tab:exclusion_reasons}. Insufficient technical depth or lack of empirical evaluation was the most common reason, affecting 52 studies (37.1\%), followed by out-of-scope content centred on pure code generation without testing relevance (41 studies, 29.3\%), single-agent approaches without multi-agent coordination (28 studies, 20.0\%), non-English or inaccessible full text (11 studies, 7.9\%), and supersession by newer versions of the same work (8 studies, 5.7\%).

\begin{table}[ht]
\caption{Full-text exclusion reasons}
\label{tab:exclusion_reasons}
\centering
\begin{tabular}{l r r}
\toprule
\tabhead{Exclusion Reason} & \tabhead{Count} & \tabhead{Percentage} \\
\midrule
Insufficient technical depth or no empirical evaluation & 52 & 37.1\% \\
Out of scope (pure code generation, not testing-related) & 41 & 29.3\% \\
Single-agent only (no MAS elements) & 28 & 20.0\% \\
Non-English or inaccessible full text & 11 & 7.9\% \\
Duplicate/superseded version & 8 & 5.7\% \\
\midrule
\textbf{Total Excluded} & \textbf{140} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:quality_scores_detail} presents the quality assessment scores for representative studies from the corpus, illustrating the range of methodological rigour observed across the included works.

\begin{table}[ht]
\caption{Quality scores for selected studies}
\label{tab:quality_scores_detail}
\centering
\small
\begin{tabular}{l c c c c c c c c}
\toprule
\tabhead{Study} & \tabhead{QC1} & \tabhead{QC2} & \tabhead{QC3} & \tabhead{QC4} & \tabhead{QC5} & \tabhead{QC6} & \tabhead{QC7} & \tabhead{Total} \\
\midrule
Hong et al. (MetaGPT) & 1 & 1 & 1 & 1 & 0.5 & 0.5 & 1 & 6.0 \\
Qian et al. (ChatDev) & 1 & 1 & 1 & 1 & 0.5 & 0.5 & 1 & 6.0 \\
Yang et al. (SWE-agent) & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 7.0 \\
Wu et al. (AutoGen) & 1 & 1 & 0.5 & 0.5 & 0.5 & 0.5 & 1 & 5.0 \\
Jimenez et al. (SWE-bench) & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 7.0 \\
Chen et al. (Codex) & 1 & 1 & 1 & 1 & 1 & 0.5 & 1 & 6.5 \\
Lemieux et al. (CODAMOSA) & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 7.0 \\
Fraser \& Arcuri (EvoSuite) & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 7.0 \\
Greshake et al. (Prompt Inj.) & 1 & 1 & 1 & 0.5 & 0.5 & 1 & 0.5 & 5.5 \\
\bottomrule
\end{tabular}
\end{table}

The 55 included studies exhibit a clear temporal concentration that mirrors the evolution of large language model capabilities, as shown in Table~\ref{tab:year_distribution}. Only four studies date from 2020, a period when LLM-based software engineering was still in its infancy and the dominant paradigm remained search-based test generation with tools such as EvoSuite \parencite{fraser2011evosuite}. The count increased modestly to six studies in 2021, coinciding with the release of OpenAI Codex \parencite{chen2021evaluating} and the first demonstrations that LLMs could generate syntactically correct test code from natural language descriptions. A more substantial rise occurred in 2022, with nine studies reflecting growing interest in code-specialised models and the publication of foundational benchmarks such as HumanEval.

The sharpest acceleration appeared in 2023, which accounts for 19 of the 55 studies, over a third of the corpus. This surge followed the release of GPT-4, the publication of influential multi-agent frameworks including MetaGPT \parencite{hong2023metagpt} and ChatDev \parencite{qian2024chatdev}, and the establishment of SWE-bench \parencite{jimenez2024swebench} as a standard evaluation benchmark for autonomous software engineering. The year 2024 contributed 13 studies, with a noticeable shift toward empirical evaluation and practical deployment concerns rather than purely architectural proposals. Four studies from early 2025 were captured through the arXiv preprint search, indicating sustained and growing research activity. This temporal distribution confirms that MAS-based test generation is a rapidly maturing field, with the bulk of the evidence base produced within the last three years.

\begin{table}[ht]
\caption{Distribution of studies by publication year}
\label{tab:year_distribution}
\centering
\begin{tabular}{l r r}
\toprule
\tabhead{Year} & \tabhead{Count} & \tabhead{Percentage} \\
\midrule
2020 & 4 & 7.3\% \\
2021 & 6 & 10.9\% \\
2022 & 9 & 16.4\% \\
2023 & 19 & 34.5\% \\
2024 & 13 & 23.6\% \\
2025 & 4 & 7.3\% \\
\midrule
\textbf{Total} & \textbf{55} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{table}

The included studies appeared across a diverse range of publication venues, reflecting the interdisciplinary nature of LLM-based test generation research (Table~\ref{tab:venue_distribution}). Premier software engineering conferences, including ICSE, FSE, and ASE, account for the largest share, contributing 16 studies that tend to emphasise architectural design and empirical evaluation on established benchmarks. Testing-focused venues such as ISSTA and ICST contributed seven studies, typically presenting more detailed evaluations of test quality metrics including mutation scores and fault detection rates. Seven studies appeared in AI and machine learning conferences (NeurIPS, ICLR), reflecting the dual identity of this research at the intersection of software engineering and artificial intelligence. Journals accounted for nine studies, published in outlets such as IEEE Transactions on Software Engineering (TSE), ACM Transactions on Software Engineering and Methodology (TOSEM), and Empirical Software Engineering (ESE). These journal publications generally offer more mature and thoroughly evaluated contributions compared to conference papers. Security-specific venues contributed three studies, addressing adversarial risks and privacy concerns. A notable proportion of the corpus, 13 studies representing nearly a quarter, appeared as arXiv preprints without formal peer review. While this raises questions about methodological rigour, it also captures the most recent developments in a field where the pace of innovation frequently outstrips the traditional publication cycle. The quality assessment scores described above were used to weight the contribution of preprints appropriately in the synthesis.

\begin{table}[ht]
\caption{Distribution of studies by venue type}
\label{tab:venue_distribution}
\centering
\begin{tabular}{l r r}
\toprule
\tabhead{Venue Type} & \tabhead{Count} & \tabhead{Percentage} \\
\midrule
SE Conferences (ICSE, FSE, ASE) & 16 & 29.1\% \\
Testing Venues (ISSTA, ICST) & 7 & 12.7\% \\
AI/ML Conferences (NeurIPS, ICLR, UIST) & 7 & 12.7\% \\
Security Venues (AISec, EMNLP) & 3 & 5.5\% \\
Journals (TSE, TOSEM, ESE, IST, TOIS) & 9 & 16.4\% \\
arXiv Preprints & 13 & 23.6\% \\
\midrule
\textbf{Total} & \textbf{55} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{table}

The majority of included studies address multiple research questions, which is expected given the interconnected nature of the research challenges (Table~\ref{tab:rq_coverage}). RQ3 (multi-agent architectures) receives the broadest coverage, with 31 of the 55 studies discussing some form of agent coordination, role specialisation, or inter-agent communication. This prominence reflects the widespread adoption of multi-agent patterns in recent LLM-based software engineering tools and the growing consensus that single-agent approaches are insufficient for complex tasks such as comprehensive test generation. RQ1 and RQ2, golden examples and few-shot approaches (22 studies) and black-box testing and traffic analysis (16 studies), together represent the two input modalities central to this dissertation. The relatively lower count for black-box approaches suggests that traffic-based test generation remains a less explored direction compared to code-level techniques, which motivates the observer pipeline developed in this work. RQ5, covering practical deployment considerations including privacy, cost, and local execution, is addressed by 24 studies, indicating growing awareness of real-world adoption barriers. However, RQ4, persistent agent memory, is covered by only nine studies, making it the least explored area in the literature. This gap is particularly significant for this dissertation, as progressive learning through persistent memory represents one of the key architectural innovations of the TestForge platform.

\begin{table}[ht]
\caption{Number of studies addressing each research question}
\label{tab:rq_coverage}
\centering
\begin{tabular}{l l r}
\toprule
\tabhead{RQ} & \tabhead{Topic} & \tabhead{Studies} \\
\midrule
RQ1 & Golden Examples and Few-Shot Test Generation & 22 \\
RQ2 & Black-Box Testing and Traffic Analysis & 16 \\
RQ3 & Multi-Agent Architecture Patterns & 31 \\
RQ4 & Persistent Agent Memory & 9 \\
RQ5 & Practical Deployment Considerations & 24 \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\subsection{RQ1, Golden Examples and Few-Shot Test Generation}
\label{sec:fewshot_testing}

This subsection addresses RQ1 by examining the literature on leveraging existing test examples and few-shot prompting strategies for LLM-based test generation. The core insight is that providing high-quality reference tests as in-context examples can guide LLMs to generate tests that follow project conventions and target non-trivial scenarios.

Few-shot prompting, providing exemplar input-output pairs within the LLM prompt, has emerged as a key technique for guiding test generation without fine-tuning. \textcite{bareiss2022fewshot} conducted one of the earliest studies, evaluating the Codex model on three code-related tasks including test oracle generation and test case generation. Their findings indicate that few-shot language models are ``surprisingly effective'' but require specifically designed prompts and suitable input selection. \textcite{hashemi2024prompting} performed a large-scale evaluation of four LLMs across five prompt engineering techniques (zero-shot, few-shot, chain-of-thought, and combinations thereof), analysing 216,300 tests for 690 Java classes. Their results show that few-shot prompting consistently improves test correctness over zero-shot approaches, though the magnitude varies by model and task complexity. \textcite{tang2024largescale} reinforced these findings in their comprehensive study comparing GPT-3.5, GPT-4, Mistral~7B, and Mixtral~8x7B against EvoSuite, evaluating five prompting techniques. While few-shot learning improves test reliability, hallucination-driven compilation failures remain a persistent challenge, with rates up to 86\% for smaller models.

A critical challenge in few-shot test generation is selecting the most relevant examples. \textcite{nashid2023cedar} addressed this with CEDAR, a retrieval-based prompt selection technique that automatically identifies code demonstrations similar to the current task. For test assertion generation, CEDAR outperforms existing task-specific and fine-tuned models by 333\% and 11\% respectively, achieving 76\% exact match accuracy. This demonstrates that intelligent example selection, rather than random or static examples, significantly impacts generation quality.

Several studies combine mutation testing feedback with few-shot prompting. \textcite{dakhel2024mutap} proposed MuTAP, which augments LLM prompts with surviving mutants to iteratively improve test quality. Using mutation feedback as additional context, MuTAP achieves a mutation score of 93.57\% on synthetic buggy code, substantially outperforming baseline approaches. At industrial scale, \textcite{alshahwan2025mutation} described Meta's production system for mutation-guided LLM test generation, demonstrating that surviving mutants serve as effective feedback signals for iterative test improvement.

\textcite{kang2023libro} explored a related application: using LLMs with few-shot prompting to generate bug-reproducing tests from bug reports (LIBRO). On the Defects4J benchmark, LIBRO successfully reproduced 33\% of studied bugs (251 out of 750), suggesting a correct reproducing test in first place for 149 bugs. This demonstrates that few-shot prompting can capture complex testing patterns beyond simple assertion generation.

While the above studies use manually selected or retrieved code examples as few-shot context, none systematically extract testing patterns, conventions, assertion styles, fixture patterns, and testing strategies, from existing test suites using AST analysis. This gap motivates the Golden Examples pipeline proposed in this thesis, which uses Tree-sitter to parse reference tests and extract a structured style guide for few-shot prompting.

%----------------------------------------------------------------------------------------

\subsection{RQ2, Black-Box Testing and Traffic Analysis}
\label{sec:blackbox_testing}

This subsection addresses RQ2 by examining the literature on automated black-box testing of APIs, with particular attention to techniques for API discovery, specification inference, and test generation from observed traffic.

\textcite{kim2023restsurvey} provide a comprehensive survey of 92 scientific articles on RESTful API testing (2009--2022), establishing a taxonomy of tools and techniques. The survey identifies three main approaches: specification-based testing (requiring OpenAPI/Swagger definitions), fuzzing (random input generation with feedback), and stateful testing (modelling inter-request dependencies). Most tools require an existing API specification as input, which many real-world APIs lack.

\textcite{atlidakis2019restler} introduced RESTler, the first stateful REST API fuzzer, which analyses Swagger specifications to infer producer-consumer dependencies between API operations. RESTler generates sequences of requests where outputs from one request feed as inputs to subsequent requests, discovering 28 bugs in GitLab and multiple bugs in Azure and Office365 cloud services. \textcite{liu2022morest} extended this approach with Morest, which builds a dynamically updating RESTful-service Property Graph (RPG) to model API behaviours. Morest successfully requests 152--232\% more API operations, covers 26--103\% more lines of code, and detects 40--215\% more bugs than prior techniques including RESTler.

Recent work has begun integrating LLMs into API testing. \textcite{kim2024restgpt} proposed RESTGPT, which uses LLMs to extract machine-interpretable rules and generate realistic parameter values from natural-language descriptions in API specifications. RESTGPT generates valid inputs for 73\% of parameters versus only 17\% for existing approaches, demonstrating that LLMs can bridge the gap between informal API documentation and formal test inputs. \textcite{kim2025llamaresttest} developed LlamaRestTest, fine-tuning and quantising Llama3-8B for REST API testing with two components: inter-parameter dependency detection and realistic value generation. LlamaRestTest outperforms state-of-the-art tools (RESTler, Morest, EvoMaster) in code coverage and server error detection on 12 real-world services.

Most critically for our work, \textcite{decrop2024restspecit} proposed RESTSpecIT, the first approach that infers API documentation \emph{and} performs black-box testing using LLMs \emph{without requiring an existing specification}. Given only an API name and LLM access, RESTSpecIT generates request seeds and mutates them, achieving 88.62\% route discovery and 89.25\% query parameter discovery. This demonstrates the feasibility of LLM-driven black-box API testing, a core premise of the Observer pipeline proposed in this thesis.

While the above tools focus on fuzzing (finding crashes and security issues), none generate \emph{human-readable test suites} from observed traffic. The combination of HTTP traffic capture (via proxy interception or HAR file import) with LLM-driven generation of pytest-style test suites is unexplored. This gap motivates the Black-Box Observer pipeline, which captures real API interactions and generates test code that verifies the observed behaviour.

%----------------------------------------------------------------------------------------

\subsection{RQ3, Multi-Agent Systems for Software Testing}
\label{sec:mas_testing}

This subsection addresses RQ3 by examining the architectural patterns and design principles employed in LLM-based multi-agent testing systems. The analysis reveals a rich design space with significant variation in agent coordination models, role specialisation, and communication patterns.

The application of agent-based approaches to software engineering predates the LLM era. Traditional \acrfull{MAS} research established foundational concepts including agent autonomy, social ability, reactivity, and pro-activeness \parencite{wooldridge2009introduction}. Early multi-agent software engineering tools focused on distributed development, collaborative editing, and automated code review, but lacked the natural language understanding capabilities that modern LLMs provide. The emergence of large language models, beginning with GPT-3 and accelerating with code-specialized models like Codex \parencite{chen2021evaluating} and Code Llama \parencite{roziere2023code}, enabled a new generation of agent systems capable of understanding and generating code with human-like proficiency. This capability, combined with reasoning techniques such as \acrfull{CoT} prompting \parencite{wei2022chain} and ReAct \parencite{yao2023react}, allows modern agents to tackle complex software engineering tasks that require multi-step reasoning and tool use.

The reviewed studies employ three primary coordination models, each with distinct characteristics and trade-offs. Hierarchical models establish a clear chain of command, with a central orchestrator agent decomposing tasks and delegating to specialized worker agents. MetaGPT \parencite{hong2023metagpt} exemplifies this approach, implementing a ``software company'' metaphor where a Product Manager agent decomposes requirements, an Architect agent designs solutions, and Engineer agents implement and test code. Hierarchical coordination offers clear task decomposition and responsibility assignment, reduced communication overhead through centralized control, natural alignment with traditional software development roles, and simplified debugging through traceable decision chains. However, this model also introduces limitations, notably single points of failure at the orchestrator level and potential bottlenecks when the central agent must process all inter-agent communication. Peer-to-peer models allow agents to communicate directly without central mediation. ChatDev \parencite{qian2024chatdev} implements a ``chat chain'' where agents engage in structured dialogues, with each agent both producing artifacts and reviewing work from peers. This model offers greater resilience through distributed decision-making, richer information exchange through direct agent interaction, emergent behaviours arising from agent collaboration, and a more natural representation of human team dynamics. At the same time, peer-to-peer coordination introduces challenges such as increased communication complexity and the potential for unproductive agent loops when appropriate termination conditions are not enforced. Many practical systems combine hierarchical and peer-to-peer elements. AutoGen \parencite{wu2023autogen} provides a flexible framework supporting both coordinator-worker patterns and direct agent conversations, allowing developers to configure coordination strategies appropriate to their specific use cases.

The literature reveals consistent patterns in how testing-related roles are distributed among agents. A planning or requirements agent typically initiates the testing workflow by analyzing requirements, identifying test objectives, and decomposing the testing task into manageable subtasks. This agent often employs structured output formats (e.g., JSON schemas) to ensure downstream agents receive well-defined specifications. Code analysis agents examine the \acrfull{SUT} to understand its structure, identify testable units, extract relevant context, and determine appropriate testing strategies. These agents may employ static analysis tools, parse abstract syntax trees, or analyze code semantics using LLM comprehension capabilities. The test generation agent produces test code based on inputs from planning and code analysis agents. Specialization may occur along multiple dimensions, including test type, programming language, testing framework, and generation technique. Execution agents run generated tests in controlled environments, capture results, and report outcomes. These agents typically interact with external tools (test runners, containers, CI systems) through the \acrshort{ACI}, making them critical from a security perspective. Validation or review agents assess the quality of generated tests, checking for issues such as trivial assertions that always pass, missing edge case coverage, test code that duplicates rather than validates SUT logic, flaky tests with non-deterministic behaviour, and style or maintainability concerns. When tests fail, debugging agents analyze failures to determine root causes, distinguishing between SUT bugs (true positives) and test defects (false positives).

Agent communication employs various protocols with different characteristics. In direct message passing, agents exchange structured messages containing task specifications, results, and feedback. Message formats range from natural language descriptions to strictly typed JSON schemas. Blackboard systems employ shared workspaces where agents post intermediate results and read inputs from a common data store. This approach decouples agents and simplifies adding new capabilities. In artifact-centric communication, rather than explicit messages, agents communicate through shared artifacts (code files, test suites, execution logs), with each agent modifying or extending artifacts produced by predecessors.

Tables~\ref{tab:framework_comparison} and~\ref{tab:framework_details} compare major multi-agent frameworks relevant to software testing, presenting both a high-level overview and a detailed comparison across key architectural dimensions.

\begin{table}[ht]
\caption{Comparison of multi-agent frameworks for software engineering}
\label{tab:framework_comparison}
\centering
\small
\begin{tabular}{l l l l l}
\toprule
\tabhead{Framework} & \tabhead{Coordination} & \tabhead{Roles} & \tabhead{Testing Focus} & \tabhead{Open Source} \\
\midrule
MetaGPT & Hierarchical & Software company & Integrated & Yes \\
ChatDev & Peer-to-peer & Chat chain & Integrated & Yes \\
SWE-agent & Single + tools & Issue resolver & Bug fixing & Yes \\
AutoGen & Configurable & Flexible & General & Yes \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Detailed MAS framework comparison}
\label{tab:framework_details}
\centering
\small
\begin{tabular}{l p{2cm} p{2.5cm} p{2.5cm} p{2.5cm}}
\toprule
\tabhead{Aspect} & \tabhead{MetaGPT} & \tabhead{ChatDev} & \tabhead{SWE-agent} & \tabhead{AutoGen} \\
\midrule
Coordination & Hierarchical SOPs & Chat chain & Single + tools & Configurable \\
Agent Count & 5--7 & 4--6 & 1 (+ tools) & Variable \\
Primary Roles & PM, Architect, Engineer, QA & CEO, CTO, Programmer, Tester & Resolver & User-defined \\
Communication & Structured docs & Natural dialog & Tool calls & Messages \\
Memory & Shared artifacts & Chat history & State files & Conversation \\
Testing Support & Integrated & Integrated & Bug fixing & General \\
Open Source & Yes (MIT) & Yes (Apache) & Yes (MIT) & Yes (MIT) \\
\bottomrule
\end{tabular}
\end{table}

The \acrfull{ACI} defines how agents interact with external systems, including code repositories, execution environments, and development tools. SWE-agent \parencite{yang2024sweagent} introduced principled ACI design, demonstrating that interface design significantly impacts agent performance beyond LLM capability alone. Key ACI design considerations span several dimensions: the action space (what operations agents can perform), the observation space (what feedback agents receive after each action), state management (how context is maintained across interactions), error handling (how agents recover from failed operations), and permission boundaries (what access controls limit agent capabilities). Well-designed ACIs balance agent autonomy, enabling effective task completion, with safety constraints that prevent unintended or malicious actions.

%----------------------------------------------------------------------------------------

\subsection{RQ4, Persistent Agent Memory}
\label{sec:agent_memory}

This subsection addresses RQ4 by examining the literature on memory architectures for LLM-based agents, focusing on approaches that enable agents to accumulate and leverage knowledge across sessions.

\textcite{sumers2023coala} proposed CoALA (Cognitive Architectures for Language Agents), a framework drawing on cognitive science to organise agents along three dimensions: memory (working memory and long-term procedural/semantic/episodic memory), action space (internal reasoning and external grounding), and decision-making processes. CoALA provides a principled vocabulary for describing how agents store, retrieve, and use knowledge. \textcite{zhang2024memorySurvey} conducted a comprehensive survey reviewing memory mechanisms in LLM-based agents, proposing a taxonomy of memory types (sensory, short-term, long-term) and memory operations (reading, writing, management). The survey identifies that while memory systems for conversational agents are well-studied, their application to task-specific domains such as software testing remains largely unexplored.

\textcite{packer2023memgpt} introduced MemGPT, which applies virtual context management inspired by operating system memory hierarchies to LLM agents. MemGPT divides memory into \emph{core memory} (analogous to RAM, always in the context window), \emph{archival memory} (persistent storage for long-term knowledge), and \emph{recall memory} (conversation history). Agents manage memory through self-directed operations, deciding when to save, retrieve, or update information. MemGPT demonstrates effective multi-session conversation where agents maintain consistency across interactions, a capability directly relevant to progressive test generation. \textcite{shinn2023reflexion} introduced Reflexion, where agents verbally reflect on task feedback and maintain reflective text in an episodic memory buffer. Rather than updating model weights, Reflexion uses linguistic feedback as persistent memory for self-improvement across trials. \textcite{park2023generative} demonstrated generative agents with memory architectures that store experiences in natural language, synthesise higher-level reflections, and retrieve them dynamically for behaviour planning.

While memory architectures have been proposed for conversational agents, game-playing agents, and general-purpose assistants, their application to software testing, where agents accumulate knowledge about target applications (discovered endpoints, coverage gaps, learned patterns, failed assertions), has not been investigated. This thesis explores whether Letta-based persistent memory enables progressive improvement in test generation quality across sessions.

%----------------------------------------------------------------------------------------

\subsection{RQ5, Practical Deployment Considerations}
\label{sec:sq5_results}

This subsection addresses RQ5 by examining the practical considerations for deploying AI-based test generation systems, including model selection and configuration, evaluation approaches and benchmarks, security and privacy risks, mitigation strategies, industrial deployment patterns, and regulatory compliance. The analysis draws on 24 studies that address deployment-related concerns.

The selection of underlying models involves trade-offs across multiple dimensions including capability, cost, latency, privacy, and control. Proprietary models such as GPT-4, Claude, and Gemini offer state-of-the-art capabilities, particularly for complex reasoning tasks. Studies consistently report higher performance on challenging benchmarks when using frontier proprietary models. Proprietary models offer superior performance on complex tasks, continuous improvement through provider updates, robust API infrastructure with high availability, and advanced features such as function calling and structured outputs. However, they also present significant drawbacks: data must be transmitted to external providers raising privacy concerns, per-token costs scale with usage, organizations become dependent on provider availability and pricing, control over model behaviour is limited, and there is potential for training data contamination with widely used benchmarks. Open-weight models including LLaMA, Code Llama \parencite{roziere2023code}, and StarCoder \parencite{li2023starcoder} can be deployed locally, addressing privacy concerns and enabling customization through fine-tuning. Open-weight models present a compelling alternative: local deployment keeps data entirely on-premises, there are no per-token API costs beyond infrastructure, organizations retain full control over deployment and behaviour, the models can be fine-tuned for specific domains, and their architecture and training methodology are transparent. On the other hand, open-weight models generally exhibit lower capability than frontier proprietary models, require significant infrastructure investment for deployment, demand expertise in optimization and maintenance, and follow a slower improvement cycle than their proprietary counterparts.

Models trained specifically on code demonstrate superior performance on programming tasks compared to general-purpose models of similar size. Codex and GPT-4 are OpenAI's code-capable models, with GPT-4 representing the current capability frontier for code generation and understanding \parencite{chen2021evaluating}. Code Llama is Meta's code-specialized variant of LLaMA, available in 7B, 13B, and 34B parameter versions, with a Python-specialized variant \parencite{roziere2023code}. StarCoder is a 15.5B parameter model trained on permissively licensed code, notable for its transparent training data and strong performance \parencite{li2023starcoder}. CodeBERT is an encoder-only model effective for code understanding tasks such as clone detection and defect prediction \parencite{feng2020codebert}.

Studies reveal a complex trade-off between fine-tuning specialized models and engineering effective prompts for general-purpose models. Fine-tuning approaches adapt pre-trained models to specific testing tasks using domain-specific training data. Research has demonstrated that fine-tuned models can significantly outperform zero-shot prompting for specific testing tasks. Fine-tuning requires high-quality training data in the form of test-code pairs, substantial computational resources, expertise in model training and evaluation, and ongoing maintenance as project requirements evolve. Prompt engineering designs input formats that elicit desired behaviour from general-purpose models without modifying model weights. The most prominent techniques include few-shot prompting (providing examples of desired input-output pairs), chain-of-thought prompting (encouraging step-by-step reasoning) \parencite{wei2022chain}, ReAct (interleaving reasoning and action) \parencite{yao2023react}, self-consistency (sampling multiple outputs and selecting the answer by consensus), and tree-of-thoughts (exploring multiple reasoning paths before committing to a solution).

LLMs operate within fixed context windows constraining the information available for each inference. Managing context effectively is crucial for testing tasks that may involve large codebases. Several strategies have been proposed to address this constraint. \acrfull{RAG} retrieves code snippets relevant to the current task from an indexed codebase. Hierarchical summarization compresses code into summaries at multiple abstraction levels, preserving essential information while reducing token consumption. Selective context inclusion restricts the prompt to immediately relevant code and documentation, while sliding window approaches process large codebases in overlapping chunks to maintain continuity across segments. Several key configuration parameters also affect generation behaviour. Temperature controls the degree of randomness, with lower values producing more deterministic outputs. Top-p (nucleus sampling) limits the sampling pool to the highest-probability tokens. Maximum token limits constrain output length, while stop sequences define generation termination conditions. For test generation, studies suggest moderate temperatures in the range of 0.2--0.4, balancing creativity with consistency, though optimal settings vary by task.

The literature employs diverse metrics and benchmarks to evaluate testing effectiveness, providing the empirical foundation for comparing MAS-based approaches to traditional methods. Coverage metrics measure the extent to which generated tests exercise the SUT, with the most commonly reported being line coverage, branch coverage, method coverage, and mutation score. Correctness metrics assess whether generated tests are valid and meaningful, including the compilation rate, execution rate, pass rate, and assertion validity. Bug detection metrics evaluate the ability to identify real defects through the true positive rate, false positive rate, and bug detection efficiency. The pass@k metric, widely used in code generation evaluation, measures the probability of generating at least one correct solution within k attempts \parencite{chen2021evaluating}:

\begin{equation}
\text{pass@k} = \mathbb{E}_{\text{Problems}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right]
\label{eq:passk}
\end{equation}

where $n$ is the total number of samples and $c$ is the number of correct samples.

Key benchmark datasets include HumanEval \parencite{chen2021evaluating}, containing 164 hand-written Python programming problems; the Mostly Basic Python Problems (MBPP) benchmark \parencite{austin2021program}, providing 974 crowd-sourced Python tasks; SWE-bench \parencite{jimenez2024swebench}, which evaluates agents on real GitHub issues from popular Python repositories; and Defects4J \parencite{just2014defects4j}, a database of real bugs from open-source Java projects. Studies consistently demonstrate that multi-agent architectures outperform single-agent approaches on complex testing tasks. MetaGPT \parencite{hong2023metagpt} reported significantly higher pass rates on software development tasks compared to single-agent baselines, with testing quality directly impacting overall success rates. Comparisons between LLM-based and traditional testing tools reveal complementary strengths. EvoSuite \parencite{fraser2011evosuite} achieves high branch coverage through search-based generation but produces tests that are often difficult to understand and maintain. Randoop \parencite{pacheco2007randoop} generates many tests quickly through random exploration but with limited semantic awareness. Pynguin \parencite{lukasczyk2023pynguin} extends search-based test generation to Python with multiple algorithms (DynaMOSA, MIO, MOSA), providing a key baseline for comparing LLM-based Python test generation. LLM-based tools, by contrast, generate more readable and semantically meaningful tests but may miss edge cases that systematic approaches find. \textcite{schafer2023testpilot} (TestPilot) achieved 70.2\% median statement coverage on npm packages, significantly outperforming traditional tools, while \textcite{yuan2024chattester} (ChatTester) found that only 24.8\% of vanilla ChatGPT-generated tests pass execution, though the passing tests achieve comparable coverage to manually-written ones. CODAMOSA \parencite{lemieux2023codamosa} proposed combining LLM generation with search-based testing to escape coverage plateaus, achieving higher coverage than either approach alone. \textcite{pizzorno2024coverup} (CoverUp) extended this idea with coverage-guided prompting, achieving 80\% median line+branch coverage versus 47\% for CODAMOSA. Mutation testing \parencite{jia2011analysis} provides a rigorous framework for evaluating test quality by measuring the proportion of artificial faults (mutants) detected by the test suite, and LLM-generated tests have shown promising results on mutation testing benchmarks, particularly when guided by mutation analysis feedback. Several factors complicate interpretation of reported effectiveness results, including benchmark contamination, incompatible evaluation metrics across studies, limited reproducibility due to proprietary models, and task selection bias in benchmarks.

The security and privacy risks associated with LLM-based multi-agent testing systems constitute key practical considerations for deployment. The analysis identifies five primary risk categories: data leakage, adversarial manipulation, unsafe code generation, grounding failures, and ACI vulnerabilities (Table~\ref{tab:threat_catalog}). Data leakage occurs when sensitive information is inadvertently exposed through agent interactions with LLM providers or external systems. Testing agents require access to source code, which may contain proprietary algorithms, trade secrets, or competitive intelligence. When using cloud-based LLM providers, this code is transmitted to external servers, potentially violating confidentiality requirements. Codebases frequently contain \acrfull{PII} in various forms, including test fixtures with realistic user data, configuration files with credentials or API keys, and database seeds with sample customer information. Agents processing such codebases may transmit PII to external providers, potentially violating \acrshort{GDPR} requirements regarding data transfers and purpose limitation. System prompts defining agent behaviour may themselves contain sensitive information, and prompt injection attacks may extract these system prompts, exposing protected information.

Adversarial actors may exploit agent systems through various manipulation techniques. Prompt injection attacks embed malicious instructions in data processed by agents \parencite{greshake2023not}. In testing contexts, injection vectors are particularly diverse: malicious instructions may be hidden in code comments, embedded in function docstrings, inserted as string literals, encoded in specially crafted file names, or planted in error messages that agents subsequently analyse. Trojan comments or code patterns may be planted in repositories to trigger specific agent behaviors when encountered. Multi-agent systems may exhibit emergent behaviors not intended by designers, particularly when agents pursue local objectives that conflict with global system goals. Agents generating test code may introduce security vulnerabilities or malicious functionality, including SQL injection vulnerabilities in database test helpers, path traversal issues in file handling tests, command injection flaws, insecure deserialization in test fixtures, or hardcoded credentials. LLMs may hallucinate package names that do not exist, creating opportunities for ``slopsquatting'' attacks, and may generate slightly misspelled package names that attackers have registered as typosquatting packages. The ``grounding gap'' between LLM knowledge and the actual state of the system under test can cause agents to generate tests calling methods or APIs that do not exist, suggest deprecated patterns, or generate tests inconsistent with code outside the visible context window. The Agent-Computer Interface represents a critical attack surface, with risks including excessive permissions, insufficient isolation, and audit trail gaps.

\begin{table}[ht]
\caption{Comprehensive security threat catalog}
\label{tab:threat_catalog}
\centering
\small
\begin{tabular}{l l l l}
\toprule
\tabhead{ID} & \tabhead{Threat Category} & \tabhead{Specific Threat} & \tabhead{Studies} \\
\midrule
T1 & Data Leakage & Source code exfiltration & S03, S12, S27 \\
T2 & Data Leakage & PII exposure via prompts & S03, S15, S31 \\
T3 & Data Leakage & Credential/secret exposure & S12, S19, S31 \\
T4 & Data Leakage & Prompt leakage attacks & S15, S22 \\
\midrule
T5 & Adversarial & Direct prompt injection & S08, S15, S22 \\
T6 & Adversarial & Indirect prompt injection & S15, S22, S34 \\
T7 & Adversarial & Trojan code comments & S22, S34 \\
T8 & Adversarial & Agent alignment failures & S08, S27 \\
\midrule
T9 & Code Generation & Vulnerability introduction & S19, S27, S38 \\
T10 & Code Generation & Dependency confusion & S19, S38 \\
T11 & Code Generation & Typosquatting packages & S19, S38 \\
T12 & Code Generation & Malicious code insertion & S27, S34 \\
\midrule
T13 & Grounding & Hallucinated APIs & S05, S12, S27 \\
T14 & Grounding & Outdated knowledge & S05, S12 \\
T15 & Grounding & Context inconsistency & S12, S27 \\
\midrule
T16 & ACI & Excessive permissions & S03, S08, S27 \\
T17 & ACI & Insufficient isolation & S03, S19, S27 \\
T18 & ACI & Audit trail gaps & S03, S31 \\
\bottomrule
\end{tabular}
\end{table}

The literature proposes architectural patterns and mitigation strategies to address these risks, organized into three categories. Model-centric strategies address risks at the LLM layer itself: deploying open-weight models locally eliminates data transmission to external providers; models can be fine-tuned to reject harmful requests through RLHF and Constitutional AI approaches; and post-generation filtering can detect and block unsafe outputs before they reach downstream systems. Pipeline-centric strategies implement security controls in the infrastructure surrounding agents: test execution should occur in isolated environments (containers, virtual machines, serverless functions, or WebAssembly sandboxes); data preprocessing can remove or redact PII before code reaches agents; context minimization provides agents only the information strictly necessary for their tasks; and securing the ACI involves explicit permission models, action validation, rate limiting, allowlisting, and confirmation gates for high-risk operations. Comprehensive logging enables detection, forensics, and compliance. Algorithmic strategies use testing and validation techniques to verify agent behavior: mutation testing provides objective quality metrics for generated tests, combinatorial testing systematically varies inputs to identify failure conditions, and chaos engineering deliberately injects failures to verify system resilience. Effective security requires layering multiple mitigations across prevention, detection, response, and recovery.

Organizations typically begin with shadow deployments where agent-generated tests run alongside but do not replace existing test suites. MAS testing systems can integrate at various CI/CD stages, including pre-commit generation, pull request review, continuous testing on each commit, nightly comprehensive generation, and release gate validation. LLM costs depend on input tokens, output tokens, model selection, and request frequency. The primary benefits include reduced time spent writing boilerplate test code, faster identification of missing coverage, earlier bug detection, and reduced cognitive load during test maintenance. Adoption requires prompt engineering expertise, security awareness, and the ability to critically evaluate agent outputs.

The regulatory landscape affecting deployment requires careful attention. The \acrfull{GDPR} \parencite{gdpr2016} imposes requirements on processing of personal data, including data minimisation (Article 5(1)(c)), purpose limitation (Article 5(1)(b)), restrictions on international data transfers (Chapter V), and Data Protection Impact Assessment requirements (Article 35). The EU AI Act \parencite{euaiact2024} classifies AI systems into risk tiers and introduces transparency requirements (Article 52). Testing systems are likely classified as limited or minimal risk unless they test safety-critical systems. When agent-generated tests fail to detect bugs, liability questions arise regarding organizational responsibility, provider liability, and the impact of human review on liability allocation. Following the seven privacy-by-design principles \parencite{cavoukian2011privacy}, testing systems should adopt a proactive posture, make privacy the default setting, embed privacy into system design, achieve full functionality without unnecessary trade-offs, ensure end-to-end security, maintain visibility and transparency, and keep user privacy central to all design decisions.

%----------------------------------------------------------------------------------------

\section{Critical Discussion}
\label{sec:critical_discussion}

This section presents a critical analysis of the findings reported above, evaluating the strengths and limitations of the existing literature for each review theme and identifying the research gaps that motivate the contributions of this thesis.

\subsection{RQ1, On the Limits of Few-Shot Test Generation}

The evidence for few-shot prompting is convincing but incomplete. The studies reviewed demonstrate that providing examples improves generation quality, yet the examples used are almost universally selected at the code-snippet level, a function and its corresponding test, rather than at the project level. Real-world testing involves conventions that span entire test suites: consistent fixture patterns, shared setup and teardown logic, project-specific assertion styles, and framework-specific idioms. None of the reviewed studies attempt to capture these higher-level patterns systematically. The reliance on surface-level code similarity for example retrieval, as in CEDAR, means that structural testing conventions, such as always verifying state after mutation or testing both positive and negative paths, are lost when examples are selected solely on syntactic resemblance.

Furthermore, the hallucination rates reported by \textcite{tang2024largescale} (up to 86\% for smaller models) suggest that few-shot prompting alone is insufficient. A practical system must include an execution-validation loop that detects and discards non-compiling or failing tests. This motivates the multi-agent architecture adopted in TestForge, where a dedicated Executor agent runs generated tests and a Validator agent assesses their quality, creating a feedback loop that compensates for LLM hallucinations. The gap in AST-based pattern extraction, where Tree-sitter parsing could produce structured style guides capturing project-wide testing conventions, remains the primary opportunity that the Golden Examples Pipeline addresses.

\subsection{RQ2, On the Untapped Potential of Traffic Observation}

The black-box testing literature is heavily biased toward fuzzing, which optimises for crash detection and code coverage rather than the production of maintainable, human-readable test suites. While fuzzing tools such as RESTler and Morest are effective at finding server errors and security vulnerabilities, the tests they produce are machine-generated sequences of requests that bear little resemblance to what a developer would write. This is a critical limitation for organisations seeking to build regression test suites from observed behaviour: the output of a fuzzer cannot be checked into a repository and maintained alongside the application code.

The emergence of specification-free approaches like RESTSpecIT is encouraging, as it demonstrates that LLMs can infer API structure without formal documentation. However, the step from endpoint discovery to test suite generation, producing pytest functions with meaningful assertions, shared fixtures, and proper test isolation, has not been taken. The Observer Pipeline proposed in this thesis occupies precisely this gap: it combines traffic capture (via proxy interception or HAR import) with LLM-driven generation to produce structured, human-readable test code. The critical question, which the experimentation chapter addresses, is whether the information contained in captured HTTP exchanges provides sufficient context for the LLM to generate tests that go beyond trivial status code assertions.

\subsection{RQ3, On Agent Architecture Trade-Offs}

The proliferation of multi-agent frameworks is both a strength and a weakness of the current literature. On the positive side, the diversity of coordination models demonstrates that there is no single correct architecture: hierarchical, peer-to-peer, and hybrid approaches each have valid use cases. On the negative side, the lack of controlled comparisons between architectures makes it difficult to determine which patterns are most effective for specific tasks. Most framework papers evaluate their approach against single-agent baselines or entirely different systems, rather than comparing alternative multi-agent designs for the same task.

For test generation specifically, a pipeline architecture, where data flows sequentially through analysis, generation, execution, and validation stages, has clear advantages over more complex coordination models. Testing is inherently sequential: one cannot validate a test before generating it, and one cannot generate a test without first understanding the target application. The pipeline model also provides natural checkpoints for quality control and makes the system easier to debug and extend. This is the rationale behind the TestForge architecture, which adopts a pipeline coordination model with typed data artifacts flowing between specialised agents, rather than the more complex dialogue-based or hierarchical approaches favoured by general-purpose frameworks.

A further observation is that the ACI design insights from SWE-agent are underappreciated in the testing literature. Most multi-agent testing papers focus on agent roles and communication patterns but pay little attention to how agents interact with external tools, test runners, build systems, version control. Yet as \textcite{yang2024sweagent} demonstrated, interface design can impact agent performance as much as or more than the underlying LLM capability. TestForge addresses this by providing each agent with a carefully scoped interface: the Executor agent interacts only with pytest through a sandboxed subprocess, the Observer agent interacts only with HTTP traffic data, and so forth.

\subsection{RQ4, On the Absence of Memory in Testing Systems}

The near-total absence of persistent memory in testing-oriented agent systems represents what is perhaps the most significant gap in the literature. Software testing is fundamentally a progressive activity: as a developer tests an application over days and weeks, they accumulate knowledge about its behaviour, its failure modes, and its most fragile components. Current LLM-based testing tools discard this knowledge after every session, forcing the system to rediscover the same endpoints, re-learn the same conventions, and potentially regenerate the same tests.

MemGPT's virtual context management provides a technically sound foundation for addressing this gap, but it was designed for conversational agents, not task-oriented testing workflows. The challenge for testing-specific memory lies in determining what to remember: discovered endpoint maps, successful test patterns, previously found bugs, coverage gaps, and application-specific conventions all represent potentially valuable persistent knowledge, but storing everything risks overwhelming the retrieval mechanism with irrelevant information. TestForge explores this through a ContextStore that maintains per-application memory, combined with Letta-based agent memory for conversational interaction. The experimentation chapter evaluates whether this architecture delivers measurable improvement across sessions.

\subsection{RQ5, On the Privacy-Capability Trade-Off}

The practical deployment literature consistently frames local execution as a privacy solution with a capability cost, but this framing may be overly pessimistic. The rapid improvement in open-weight models, from Code Llama 7B through to recent instruction-tuned variants, is narrowing the gap with proprietary models for code-specific tasks. For test generation in particular, the reasoning demands may be lower than for general-purpose software engineering: the LLM must understand an API structure and produce well-formed test functions, but it does not need to reason about novel algorithms or complex system architectures.

The more pressing concern for local deployment is infrastructure cost and operational complexity. Running a quantised 8B parameter model on consumer hardware is feasible but slow, with inference times of 30--60 seconds per generation that would be unacceptable in an interactive workflow. TestForge addresses this by adopting a batch-oriented design where the user initiates a generation run and reviews results asynchronously, and by using LiteLLM as a unified gateway that allows seamless switching between local and cloud models depending on the sensitivity of the target codebase. The regulatory analysis confirms that for organisations subject to GDPR Article 44--49 on international data transfers, local deployment is not merely a preference but a practical necessity when processing codebases that contain personal data.

\subsection{Research Gaps}

The critical analysis above identifies five concrete research gaps that this thesis addresses. First, no existing approach extracts project-wide testing patterns from reference test suites using AST analysis for structured few-shot prompting. Second, no tool generates human-readable, maintainable test suites from observed HTTP traffic without requiring API specifications or source code access. Third, no multi-agent testing architecture combines multiple complementary input strategies, golden examples and black-box observation, within a unified platform. Fourth, persistent memory for progressive improvement in test generation has not been investigated. Fifth, a systematic evaluation of the quality trade-offs, performance characteristics, and cost-effectiveness of fully local test generation using open-weight models has not been conducted.

\subsection{Implications for the Proposed Research}

The identified gaps directly motivate the contributions proposed in this thesis. The limited exploration of example-driven approaches for test generation motivates the Golden Examples Pipeline, which leverages existing test suites as structured few-shot examples to generate insightful tests that go beyond trivial assertions. The absence of black-box testing approaches that combine traffic analysis with LLM-driven generation motivates the Black-Box Observer Pipeline, which captures HTTP traffic to automatically map endpoints and generate test suites without requiring source code access. The lack of integrated multi-agent testing architectures that support multiple complementary input strategies motivates the development of TestForge as a modular platform with six specialised agents covering the full testing workflow from analysis through validation. The underdeveloped area of persistent agent memory for testing motivates the integration of Letta-based memory, enabling the platform to accumulate knowledge about target applications and improve progressively across sessions. Finally, the need for practical deployment guidance under privacy constraints motivates the local execution assessment, which evaluates the feasibility of running the complete platform on local infrastructure using open-weight models.

%----------------------------------------------------------------------------------------

\section{Conclusions of the Review}
\label{sec:review_conclusion}

This systematic literature review examined 55 studies following the PRISMA 2020 methodology, organised around five research questions that collectively address the overarching review question of how AI techniques have been applied to automated software test generation. The review covered publications from 2020 to early 2026 across four major databases, selecting studies through a rigorous multi-stage screening process with explicit inclusion and exclusion criteria.

The Results section consolidated the evidence for each research question. For RQ1, the literature demonstrates that few-shot prompting consistently improves test generation quality over zero-shot approaches, but no existing work systematically extracts project-level testing patterns from reference test suites using AST analysis. For RQ2, black-box API testing is dominated by fuzzing-oriented approaches that optimise for crash detection rather than the production of human-readable, maintainable test suites from observed traffic. For RQ3, multi-agent architectures consistently outperform single-agent approaches, with pipeline, hierarchical, and peer-to-peer coordination models each offering distinct trade-offs, though controlled comparisons between architectures for the same testing task are absent. For RQ4, persistent memory mechanisms have been proposed for conversational and general-purpose agents but have not been applied to software testing, leaving the potential for progressive learning in test generation entirely unexplored. For RQ5, the tension between model capability and deployment constraints is well-characterised, with the regulatory landscape strongly favouring local execution for organisations handling sensitive data, though systematic evaluations of local deployment feasibility remain limited.

The Critical Discussion identified five concrete research gaps: the absence of AST-based pattern extraction for structured few-shot prompting, the lack of traffic-to-test-suite generation tools, the need for dual-pipeline architectures combining golden examples with black-box observation, the unexplored potential of persistent memory for testing agents, and the limited evaluation of fully local test generation platforms. These gaps directly motivate the design of the TestForge platform presented in the subsequent chapters.

\subsection{Limitations of This Review}
\label{sec:review_limitations}

This review has several limitations that should be acknowledged. The field is evolving rapidly, and some relevant work may have been published after the search cutoff date of January 2026. Publication bias may cause the included studies to overrepresent positive results, with failed approaches and negative findings going underreported in the academic literature. Industry practices that are not disseminated through academic venues constitute a grey literature gap that may leave practical deployment insights underrepresented in the synthesis. The restriction to English-language publications may exclude relevant work published in other languages, particularly from research communities in China, where significant LLM development is taking place. Finally, many of the reviewed studies rely on benchmarks such as HumanEval and SWE-bench that are potentially contaminated by LLM training data, which may inflate the effectiveness claims reported in the literature.

\subsection{Future Directions}
\label{sec:review_future_work}

The systematic review identifies several directions for future research that extend beyond the scope of this thesis but represent significant opportunities for the field.

The first direction concerns the integration of formal verification techniques with LLM-based test generation. While the reviewed studies focus on generating tests through prompting and example-driven approaches, combining these methods with formal methods, such as property-based testing or model checking, could provide stronger guarantees about test correctness and coverage completeness. This integration remains largely unexplored in the multi-agent context.

The second direction involves cross-language and cross-framework generalisation. The majority of the reviewed studies target a single programming language (predominantly Python or Java) and a single testing framework. Investigating how multi-agent testing systems can transfer knowledge across languages, frameworks, and application domains would significantly broaden their practical applicability.

A third direction concerns human-agent collaboration models. The reviewed literature treats test generation as a largely autonomous process, with human involvement limited to reviewing outputs. Research into interactive workflows, where developers and agents collaboratively refine test strategies, share domain knowledge, and iteratively improve test suites, could yield higher-quality outcomes than fully autonomous approaches.

The fourth direction addresses longitudinal evaluation of persistent memory. While this thesis evaluates memory-augmented testing across a limited number of sessions, long-term studies tracking how agent knowledge evolves over months of continuous use on production codebases would provide stronger evidence for the value of persistent memory in practice.

Finally, the review identifies a need for standardised evaluation benchmarks specific to AI-driven test generation. Current evaluations rely on a patchwork of benchmarks designed for code generation (HumanEval), bug fixing (SWE-bench, Defects4J), or manual evaluation. A purpose-built benchmark capturing the full spectrum of test quality, including assertion depth, test isolation, fixture design, and maintainability, would enable more rigorous and comparable evaluations across the field.
