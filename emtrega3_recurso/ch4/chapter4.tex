% Chapter 4 - Methods and Tools

\chapter{Methods and Tools}
\label{chap:methods_tools}

This chapter presents the system architecture of TestForge, describing the dual-pipeline design, the multi-agent architecture, the LLM integration strategy, and the persistent memory mechanism. It also describes the development stack and deployment environment.

%----------------------------------------------------------------------------------------

\section{System Overview and Architecture}
\label{sec:system_overview}

TestForge is organised around two complementary pipelines that can operate independently or in combination. The Golden Examples Pipeline analyses existing test files to learn patterns, then generates new tests in the same style while targeting untested scenarios. The Black-Box Observer Pipeline captures HTTP traffic from a running application, maps the API surface, and generates tests based on observed behaviour.

Both pipelines share a common downstream path through the Generator, Executor, and Validator agents. The architecture follows a pipeline pattern where data flows sequentially through specialised processing stages, with each stage implemented as an autonomous agent.

Figure~\ref{fig:architecture_overview} illustrates the high-level architecture.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=0.8cm and 1.4cm,
        box/.style={rectangle, draw, rounded corners, minimum width=2.2cm, minimum height=0.7cm, text centered, font=\footnotesize},
        arrow/.style={->, thick, >=stealth},
    ]
        % Golden pipeline (top row)
        \node[box, fill=blue!10] (golden) {Golden Examples};
        \node[box, fill=blue!20, right=of golden] (analyzer) {Analyser Agent};

        % Observer pipeline (second row)
        \node[box, fill=green!10, below=of golden] (traffic) {HTTP Traffic};
        \node[box, fill=green!20, right=of traffic] (observer) {Observer Agent};
        \node[box, fill=green!20, right=of observer] (mapper) {Mapper Agent};

        % Shared pipeline (third row, centred)
        \node[box, fill=orange!20, below=1.2cm of mapper] (generator) {Generator Agent};
        \node[box, fill=red!15, right=of generator] (executor) {Executor Agent};
        \node[box, fill=purple!15, right=of executor] (validator) {Validator Agent};
        \node[box, fill=gray!15, right=of validator] (results) {Results};

        % Arrows
        \draw[arrow] (golden) -- (analyzer);
        \draw[arrow] (analyzer.south) -- ++(0,-0.4) -| (generator.north);
        \draw[arrow] (traffic) -- (observer);
        \draw[arrow] (observer) -- (mapper);
        \draw[arrow] (mapper) -- (generator);

        \draw[arrow] (generator) -- (executor);
        \draw[arrow] (executor) -- (validator);
        \draw[arrow] (validator) -- (results);
    \end{tikzpicture}}
    \caption{High-level architecture of TestForge showing the dual-pipeline design.}
    \label{fig:architecture_overview}
\end{figure}

%----------------------------------------------------------------------------------------

\section{Multi-Agent Architecture}
\label{sec:multi_agent_architecture}

The platform employs six specialised agents, each implemented as a subclass of a common \texttt{BaseAgent} abstract class with typed input and output models (Pydantic v2) \parencite{pydantic2017}. This design ensures clear contracts between agents and enables independent testing of each component.

\subsection{Agent Roles and Responsibilities}

\begin{description}
    \item[Observer Agent] Captures HTTP traffic from running applications via three modes: pre-captured exchange data, HAR file import, or live mitmproxy interception. Outputs normalised \texttt{HTTPExchange} records.

    \item[Mapper Agent] Transforms captured HTTP exchanges into a structured \texttt{EndpointMap}. Performs path normalisation, schema inference, authentication pattern detection, and dependency chain identification.

    \item[Analyser Agent] Parses golden test files using Tree-sitter \parencite{treesitter} AST analysis. Extracts imports, fixtures, test functions, assertions, and helper functions. Aggregates patterns into a \texttt{TestStyleGuide}.

    \item[Generator Agent] Produces new test code using LLM few-shot prompting \parencite{brown2020language}. Accepts a style guide (from the Analyser), an endpoint map (from the Mapper), or both. Constructs a detailed system prompt instructing the LLM to generate insightful tests across eight categories, and includes golden examples as few-shot context.

    \item[Executor Agent] Writes generated tests to disk alongside an auto-generated \texttt{conftest.py} with common fixtures (\texttt{base\_url}, \texttt{created\_user}, \texttt{sample\_user}). Runs pytest as a subprocess and parses the output to collect per-test pass/fail results.

    \item[Validator Agent] Assesses the quality of generated tests along four dimensions: assertion count and type distribution, coverage breadth (number of distinct endpoints targeted), readability (test length, docstring presence), and execution results (pass rate). Produces a \texttt{ValidationResult} with per-test quality scores and an overall summary.
\end{description}

\subsection{Agent Communication}
\label{sec:agent_communication}

Agents communicate through typed data models rather than free-text messages. Each agent defines an \texttt{Input} and \texttt{Output} model (Pydantic \texttt{BaseModel}), and the orchestration layer is responsible for routing outputs to inputs. This approach provides type safety, as invalid data is rejected at agent boundaries, and testability, since each agent can be tested in isolation with mock inputs. It also supports extensibility, allowing new agents to be added simply by defining their input/output contracts, and ensures transparency, as the data flowing between agents is fully inspectable and serialisable.

\subsection{Base Agent Design}
\label{sec:base_agent}

All agents inherit from \texttt{BaseAgent[InputT, OutputT]}, a generic abstract class that provides a unique agent name and dedicated logger, an abstract \texttt{run(input\_data: InputT) -> OutputT} method that each agent implements, and an \texttt{execute()} wrapper that handles validation, error catching, and result packaging into an \texttt{AgentResult} with success/error/warning status.

%----------------------------------------------------------------------------------------

\section{Golden Examples Pipeline Design}
\label{sec:golden_pipeline}

The Golden Examples pipeline follows a four-stage sequence. In the first stage, the Analyser Agent parses golden test files and produces a \texttt{TestStyleGuide}. The Generator Agent then receives the style guide and golden examples as few-shot context and produces new test code via LLM completion. Optionally, the Executor Agent writes the tests to disk, generates a \texttt{conftest.py}, and runs pytest against the target application. Finally, the Validator Agent scores the generated tests based on assertion quality, coverage, readability, and execution results.

Figure~\ref{fig:golden_pipeline} illustrates the data flow through the Golden Examples pipeline.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=0.15cm and 0.35cm,
        agent/.style={rectangle, draw, rounded corners=3pt, minimum width=1.6cm, minimum height=0.65cm, text centered, font=\footnotesize\bfseries},
        artifact/.style={rectangle, draw, rounded corners=6pt, minimum width=1.2cm, minimum height=0.45cm, text centered, font=\scriptsize, fill=white},
        arrow/.style={->, thick, >=stealth},
    ]
        % Input artifact
        \node[artifact, fill=blue!8] (input) {Golden Test Files};

        % Analyser Agent
        \node[agent, fill=blue!25, right=0.6cm of input] (analyser) {Analyser};

        % Style guide artifact
        \node[artifact, fill=blue!8, right=0.6cm of analyser] (styleguide) {TestStyleGuide};

        % Generator Agent
        \node[agent, fill=orange!25, right=0.6cm of styleguide] (generator) {Generator};

        % Test code artifact
        \node[artifact, fill=orange!8, right=0.6cm of generator] (testcode) {Test Code};

        % Executor Agent
        \node[agent, fill=red!20, right=0.6cm of testcode] (executor) {Executor};

        % Test result artifact
        \node[artifact, fill=red!8, right=0.6cm of executor] (testresult) {TestResult};

        % Validator Agent
        \node[agent, fill=purple!20, right=0.6cm of testresult] (validator) {Validator};

        % Validation result artifact
        \node[artifact, fill=purple!8, right=0.6cm of validator] (valresult) {\makecell{Validation\\[-1pt]Result}};

        % Arrows
        \draw[arrow] (input) -- (analyser);
        \draw[arrow] (analyser) -- (styleguide);
        \draw[arrow] (styleguide) -- (generator);
        \draw[arrow] (generator) -- (testcode);
        \draw[arrow] (testcode) -- (executor);
        \draw[arrow] (executor) -- (testresult);
        \draw[arrow] (testresult) -- (validator);
        \draw[arrow] (validator) -- (valresult);
    \end{tikzpicture}}
    \caption{Data flow through the Golden Examples pipeline. Blue-tinted nodes represent stages specific to this pipeline; orange, red, and purple nodes represent shared stages.}
    \label{fig:golden_pipeline}
\end{figure}

\subsection{Few-Shot Prompting Strategy}
\label{sec:few_shot_prompting}

The Generator Agent constructs a multi-part LLM prompt. The system prompt defines the agent's role as a ``senior QA engineer and security tester'' and lists eight categories of insightful tests to generate: state integrity, boundary probing, business logic, error quality, concurrency hints, authorisation boundaries, data leakage, and regression traps. The style context includes the \texttt{TestStyleGuide} summary, covering framework, HTTP client, naming conventions, common imports and fixtures, so that generated tests match existing project conventions. The prompt also includes the full source code of golden test files as few-shot examples, enabling the LLM to replicate the style and assertion patterns. When available, application context from the \texttt{AppContext} provides information about previously tested endpoints, untested endpoints, and coverage gaps to direct generation toward new areas. Finally, the generation instruction specifies the target number of tests and any endpoint-specific requirements.

%----------------------------------------------------------------------------------------

\section{Black-Box Observer Pipeline Design}
\label{sec:observer_pipeline}

The Observer pipeline follows a five-stage sequence. The Observer Agent first collects HTTP exchanges from the configured data source. The Mapper Agent then transforms these exchanges into an \texttt{EndpointMap} with normalised paths, inferred schemas, and detected dependencies. The Generator Agent receives the endpoint map (and optionally a style guide if golden examples are also available) and produces test code. The Execute and Validate stages follow the same process as the Golden Examples pipeline.

When no golden examples are available, the Generator Agent constructs a minimal style guide (pytest + requests) from defaults, ensuring that tests are always generated with proper framework conventions.

Figure~\ref{fig:observer_pipeline} illustrates the data flow through the Observer pipeline.

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=0.15cm and 0.3cm,
        agent/.style={rectangle, draw, rounded corners=3pt, minimum width=1.4cm, minimum height=0.65cm, text centered, font=\footnotesize\bfseries},
        artifact/.style={rectangle, draw, rounded corners=6pt, minimum width=1.1cm, minimum height=0.45cm, text centered, font=\scriptsize, fill=white},
        arrow/.style={->, thick, >=stealth},
    ]
        % Input artifact
        \node[artifact, fill=green!8] (input) {HTTP Traffic};

        % Observer Agent
        \node[agent, fill=green!25, right=0.5cm of input] (observer) {Observer};

        % HTTPExchange artifact
        \node[artifact, fill=green!8, right=0.5cm of observer] (exchanges) {\makecell{HTTP\\[-1pt]Exchange[]}};

        % Mapper Agent
        \node[agent, fill=green!25, right=0.5cm of exchanges] (mapper) {Mapper};

        % EndpointMap artifact
        \node[artifact, fill=green!8, right=0.5cm of mapper] (endpointmap) {EndpointMap};

        % Generator Agent
        \node[agent, fill=orange!25, right=0.5cm of endpointmap] (generator) {Generator};

        % Test code artifact
        \node[artifact, fill=orange!8, right=0.5cm of generator] (testcode) {Test Code};

        % Executor Agent
        \node[agent, fill=red!20, right=0.5cm of testcode] (executor) {Executor};

        % Test result artifact
        \node[artifact, fill=red!8, right=0.5cm of executor] (testresult) {TestResult};

        % Validator Agent
        \node[agent, fill=purple!20, right=0.5cm of testresult] (validator) {Validator};

        % Validation result artifact
        \node[artifact, fill=purple!8, right=0.5cm of validator] (valresult) {\makecell{Validation\\[-1pt]Result}};

        % Arrows
        \draw[arrow] (input) -- (observer);
        \draw[arrow] (observer) -- (exchanges);
        \draw[arrow] (exchanges) -- (mapper);
        \draw[arrow] (mapper) -- (endpointmap);
        \draw[arrow] (endpointmap) -- (generator);
        \draw[arrow] (generator) -- (testcode);
        \draw[arrow] (testcode) -- (executor);
        \draw[arrow] (executor) -- (testresult);
        \draw[arrow] (testresult) -- (validator);
        \draw[arrow] (validator) -- (valresult);
    \end{tikzpicture}}
    \caption{Data flow through the Black-Box Observer pipeline. Green-tinted nodes represent stages specific to this pipeline; orange, red, and purple nodes represent shared stages.}
    \label{fig:observer_pipeline}
\end{figure}

%----------------------------------------------------------------------------------------

\section{Persistent Memory via Letta}
\label{sec:letta_integration}

A key differentiator of TestForge is its integration with Letta (formerly MemGPT) \parencite{packer2023memgpt}, an agent framework that provides persistent memory across sessions. This enables the platform to progressively learn about target applications rather than treating each generation run as independent.

\subsection{Memory Architecture}

The Letta agent maintains three types of memory:

\begin{description}
    \item[Core Memory] Editable memory blocks that the agent can update during conversation. TestForge uses three blocks:
    \begin{itemize}
        \item \textbf{Persona}: The agent's identity and capabilities.
        \item \textbf{Human}: Information about the user and their preferences.
        \item \textbf{App Context}: Discovered information about the target application (endpoints, coverage, patterns).
    \end{itemize}

    \item[Archival Memory] Long-term storage for knowledge that may be relevant across many sessions. Seeded with testing best practices and pattern knowledge.

    \item[Recall Memory] Conversation history, enabling the agent to reference previous interactions.
\end{description}

\subsection{Custom Tools}

The Letta agent is equipped with seven custom tools that bridge conversational interaction with the TestForge pipelines:

\begin{enumerate}
    \item \texttt{analyze\_golden\_tests}: Invokes the AST analyser on specified test files.
    \item \texttt{generate\_tests\_from\_golden}: Runs the full Golden Examples pipeline.
    \item \texttt{generate\_tests\_from\_endpoints}: Runs the Observer pipeline from endpoint definitions.
    \item \texttt{run\_tests}: Executes a test file with pytest and returns results.
    \item \texttt{save\_test\_file}: Writes generated test code to a specified path.
    \item \texttt{list\_files}: Lists files matching a glob pattern in a directory.
    \item \texttt{read\_file}: Reads the contents of a file.
\end{enumerate}

This design allows users to interact with TestForge conversationally, describing their application, providing golden examples, requesting test generation, and reviewing results, while the agent maintains context across the entire session and beyond.

\subsection{Local Execution}

Both the Letta server and the LLM inference run locally. Letta is deployed via Docker with bundled PostgreSQL for memory persistence, while the LLM runs through Ollama using \texttt{llama3.1:8b} for text generation and \texttt{mxbai-embed-large} for embeddings. This ensures that no application code or test data leaves the local environment, addressing the privacy concerns discussed in Chapter~\ref{chap:data_protection}.

%----------------------------------------------------------------------------------------

\section{Orchestration Engine}
\label{sec:orchestration}

The orchestration layer provides a unified interface for invoking either pipeline or a combined mode:

\begin{description}
    \item[Golden mode] Accepts golden test file paths (or source code strings) and runs the Analyser $\rightarrow$ Generator $\rightarrow$ Executor $\rightarrow$ Validator sequence.

    \item[Observer mode] Accepts captured HTTP exchanges (or a HAR file path) and runs the Observer $\rightarrow$ Mapper $\rightarrow$ Generator $\rightarrow$ Executor $\rightarrow$ Validator sequence.

    \item[Combined mode] Uses golden examples for the style guide and observer data for the endpoint map, then runs the shared Generator $\rightarrow$ Executor $\rightarrow$ Validator sequence. This mode produces tests that follow the project's conventions (from golden examples) while covering the full API surface (from observer data).
\end{description}

The orchestration engine is exposed through three interfaces: a FastAPI REST API with endpoints such as \texttt{/api/pipeline/run}, \texttt{/api/golden/upload}, and \texttt{/api/observer/import-har}; a CLI offering commands like \texttt{testforge generate} and \texttt{testforge chat}; and a Streamlit web interface with tabs for each mode.

%----------------------------------------------------------------------------------------

\section{Development Stack and Deployment Environment}
\label{sec:dev_stack}

Table~\ref{tab:tech_stack} summarises the technology stack.

\begin{table}[htbp]
\centering
\caption{Technology stack of the TestForge platform.}
\label{tab:tech_stack}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Technology} & \textbf{Rationale} \\
\midrule
Language & Python 3.11+ & LLM ecosystem, async support, testing tools \\
Backend API & FastAPI & Async, typed, auto-generated OpenAPI docs \\
Web UI & Streamlit & Rapid prototyping for data-oriented interfaces \\
HTTP Proxy & mitmproxy & Mature, scriptable, handles TLS \\
Code Parsing & Tree-sitter & Multi-language AST, handles partial code \\
LLM Gateway & LiteLLM & Unified API for OpenAI/Anthropic/Ollama \\
Agent Memory & Letta (Docker) & Persistent memory with archival/recall \\
Local LLM & Ollama & Local inference for privacy \\
Data Models & Pydantic v2 & Runtime validation, serialisation \\
Test Runner & pytest & Industry standard, rich plugin ecosystem \\
\bottomrule
\end{tabular}
\end{table}

The backend uses FastAPI \parencite{fastapi2018} for its async capabilities and automatic OpenAPI documentation generation, while Streamlit \parencite{streamlit2019} provides rapid prototyping for the web interface.

\subsection{LLM Configuration}
\label{sec:llm_config}

The LLM gateway (LiteLLM) \parencite{litellm} supports multiple providers through a unified interface. For the prototype evaluation, the model used was \texttt{ollama/llama3.1:8b} (8 billion parameters, running locally via Ollama \parencite{ollama2023}) with a temperature of 0.7 to balance creativity with consistency and a maximum token limit of 4096, sufficient for generating 8--12 test functions. The embedding model was \texttt{ollama/mxbai-embed-large}, used for Letta archival memory search. The architecture is model-agnostic: switching to a different LLM (e.g., GPT-4, Claude, Code Llama) requires only changing the model identifier in the configuration, with no code changes.
