% Chapter 4 - Implementation

\chapter{Implementation}
\label{chap:implementation}

This chapter details the prototype implementation of the secure multi-agent testing architecture proposed in Chapter~\ref{chap:architecture}. The implementation serves as a proof-of-concept demonstrating the feasibility of the proposed design while providing a foundation for the experimental evaluation presented in Chapter~\ref{chap:evaluation}. The chapter covers technology stack selection, core component implementation, security controls, workflow design, and integration with development environments.

%----------------------------------------------------------------------------------------

\section{Technology Stack}
\label{sec:tech_stack}

The technology stack was selected to balance implementation efficiency, ecosystem maturity, and alignment with the architectural requirements defined in Section~\ref{sec:requirements}.

\subsection{Programming Language}

Python 3.11+ serves as the primary implementation language, selected for several reasons:

\begin{itemize}
    \item \textbf{LLM Ecosystem}: Python dominates the LLM tooling ecosystem, with first-class support from major providers (OpenAI, Anthropic, Hugging Face) and comprehensive libraries (LangChain, LlamaIndex).

    \item \textbf{Asynchronous Support}: Python's \texttt{asyncio} library enables efficient handling of concurrent LLM API calls and agent interactions, critical for system performance.

    \item \textbf{Testing Ecosystem}: Python's testing tools (pytest, coverage.py, mutmut) are mature and widely adopted, simplifying integration.

    \item \textbf{Type Safety}: Type hints with \texttt{mypy} static checking improve code quality and enable better IDE support for the complex agent interactions.
\end{itemize}

\subsection{Project Structure}

The implementation follows a modular structure organized by architectural layer:

\begin{verbatim}
mas-testing/
+-- src/
|   +-- agents/           # Agent implementations
|   |   +-- base.py       # Base agent class
|   |   +-- planning.py   # Planning agent
|   |   +-- analysis.py   # Code analysis agent
|   |   +-- generation.py # Test generation agent
|   |   +-- execution.py  # Execution agent
|   |   +-- validation.py # Validation agent
|   |   +-- security.py   # Security agent
|   +-- orchestration/    # Workflow orchestration
|   |   +-- engine.py     # Workflow engine
|   |   +-- scheduler.py  # Task scheduler
|   |   +-- state.py      # State management
|   +-- llm/              # LLM integration
|   |   +-- gateway.py    # LLM gateway
|   |   +-- providers/    # Provider implementations
|   |   +-- prompts/      # Prompt templates
|   +-- security/         # Security controls
|   |   +-- sandbox.py    # Sandbox management
|   |   +-- scrubber.py   # PII scrubbing
|   |   +-- permissions.py# Permission enforcement
|   |   +-- audit.py      # Audit logging
|   +-- integration/      # External integrations
|   |   +-- git.py        # Git repository connector
|   |   +-- ci.py         # CI/CD integration
|   |   +-- cli.py        # Command-line interface
|   +-- utils/            # Shared utilities
+-- config/               # Configuration files
+-- docker/               # Docker configurations
+-- tests/                # Test suite
+-- docs/                 # Documentation
\end{verbatim}

\subsection{Dependencies}

Table~\ref{tab:dependencies} lists the primary dependencies and their purposes.

\begin{table}[ht]
\caption{Primary implementation dependencies}
\label{tab:dependencies}
\centering
\small
\begin{tabular}{l l l}
\toprule
\tabhead{Package} & \tabhead{Version} & \tabhead{Purpose} \\
\midrule
langchain & 0.1.x & LLM abstraction and chains \\
openai & 1.x & OpenAI API client \\
anthropic & 0.x & Anthropic API client \\
ollama & 0.1.x & Local model integration \\
docker & 7.x & Container management \\
pytest & 8.x & Test execution framework \\
coverage & 7.x & Coverage measurement \\
mutmut & 2.x & Mutation testing \\
pydantic & 2.x & Data validation and settings \\
structlog & 24.x & Structured logging \\
aiohttp & 3.x & Async HTTP client \\
gitpython & 3.x & Git repository operations \\
tree-sitter & 0.21.x & Code parsing \\
presidio-analyzer & 2.x & PII detection \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LLM Provider Integration}

The implementation supports multiple LLM providers through a unified interface:

\subsubsection{OpenAI Integration}

OpenAI's GPT-4 and GPT-3.5 models provide high-capability cloud-based inference. The implementation uses the official Python client with support for:
\begin{itemize}
    \item Chat completions with function calling
    \item Structured JSON output mode
    \item Streaming responses for long generations
    \item Automatic retry with exponential backoff
\end{itemize}

\subsubsection{Anthropic Integration}

Claude models offer an alternative cloud provider with strong code understanding capabilities. Integration supports:
\begin{itemize}
    \item Messages API with tool use
    \item Extended context windows (up to 200K tokens)
    \item XML-structured prompting patterns
\end{itemize}

\subsubsection{Local Model Integration}

Ollama enables local deployment of open-weight models for privacy-sensitive operations:
\begin{itemize}
    \item Code Llama (7B, 13B, 34B variants)
    \item Mistral and Mixtral models
    \item DeepSeek Coder
    \item Custom fine-tuned models
\end{itemize}

\subsection{Containerization}

Docker provides isolated execution environments with the following base configuration:

\begin{verbatim}
# Base sandbox image
FROM python:3.11-slim

# Security hardening
RUN useradd -m -s /bin/bash sandbox && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        git && \
    rm -rf /var/lib/apt/lists/*

# Non-root execution
USER sandbox
WORKDIR /workspace

# Resource limits applied at runtime
# CPU: 2 cores, Memory: 2GB, Time: 300s
\end{verbatim}

%----------------------------------------------------------------------------------------

\section{Core Components Implementation}
\label{sec:core_implementation}

\subsection{Agent Base Class}

All agents inherit from a common base class that provides shared functionality:

\begin{verbatim}
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from pydantic import BaseModel
import structlog

class AgentConfig(BaseModel):
    """Configuration for agent instances."""
    agent_id: str
    role: str
    llm_provider: str = "openai"
    model: str = "gpt-4-turbo"
    temperature: float = 0.2
    max_tokens: int = 4096
    tools: List[str] = []
    permissions: Dict[str, Any] = {}

class AgentMessage(BaseModel):
    """Message exchanged between agents."""
    sender: str
    recipient: str
    message_type: str
    content: Dict[str, Any]
    metadata: Dict[str, Any] = {}

class BaseAgent(ABC):
    """Base class for all agents in the system."""

    def __init__(self, config: AgentConfig,
                 llm_gateway: "LLMGateway",
                 security_context: "SecurityContext"):
        self.config = config
        self.llm = llm_gateway
        self.security = security_context
        self.logger = structlog.get_logger().bind(
            agent_id=config.agent_id,
            role=config.role
        )
        self._tools = self._register_tools()
        self._memory: List[Dict] = []

    @abstractmethod
    def _register_tools(self) -> Dict[str, callable]:
        """Register tools available to this agent."""
        pass

    @abstractmethod
    async def process(self, message: AgentMessage) -> AgentMessage:
        """Process incoming message and produce response."""
        pass

    async def invoke_llm(self, prompt: str,
                         system: Optional[str] = None,
                         tools: Optional[List] = None) -> str:
        """Invoke LLM with security controls."""
        # Scrub PII from prompt
        scrubbed_prompt = self.security.scrub_pii(prompt)

        # Log the interaction
        self.security.audit_log(
            event="llm_invocation",
            agent=self.config.agent_id,
            prompt_length=len(scrubbed_prompt)
        )

        # Make LLM call
        response = await self.llm.complete(
            prompt=scrubbed_prompt,
            system=system,
            model=self.config.model,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            tools=tools
        )

        return response

    async def execute_tool(self, tool_name: str,
                           **kwargs) -> Any:
        """Execute a tool with permission checking."""
        # Verify permission
        if not self.security.check_permission(
            self.config.agent_id,
            tool_name,
            kwargs
        ):
            raise PermissionError(
                f"Agent {self.config.agent_id} denied "
                f"permission for {tool_name}"
            )

        # Execute tool
        tool = self._tools.get(tool_name)
        if not tool:
            raise ValueError(f"Unknown tool: {tool_name}")

        result = await tool(**kwargs)

        # Audit log
        self.security.audit_log(
            event="tool_execution",
            agent=self.config.agent_id,
            tool=tool_name,
            success=True
        )

        return result
\end{verbatim}

\subsection{Planning Agent Implementation}

The Planning Agent analyzes testing requirements and creates structured task plans:

\begin{verbatim}
class TestPlan(BaseModel):
    """Structured test plan output."""
    project_summary: str
    testing_objectives: List[str]
    tasks: List[TestTask]
    priority_order: List[str]
    estimated_coverage_targets: Dict[str, float]

class TestTask(BaseModel):
    """Individual testing task."""
    task_id: str
    description: str
    target_files: List[str]
    test_type: str  # unit, integration, property
    assigned_agent: str
    dependencies: List[str] = []
    context_requirements: List[str] = []

class PlanningAgent(BaseAgent):
    """Agent responsible for test planning."""

    SYSTEM_PROMPT = """You are a test planning specialist.
    Analyze the provided codebase information and create
    a comprehensive test plan.

    For each testing task, specify:
    - Clear description of what to test
    - Target files and functions
    - Type of tests needed
    - Dependencies on other tasks
    - Required context from codebase

    Prioritize tasks by:
    1. Critical business logic
    2. Complex code paths
    3. Recently changed code
    4. Low current coverage areas
    """

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "list_files": self._list_files,
            "read_file_summary": self._read_file_summary,
            "get_coverage_report": self._get_coverage_report,
            "get_git_history": self._get_git_history,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Create test plan from requirements."""
        requirements = message.content.get("requirements", "")
        project_path = message.content.get("project_path")

        # Gather project context
        context = await self._gather_context(project_path)

        # Generate plan using LLM
        prompt = self._build_planning_prompt(requirements, context)

        response = await self.invoke_llm(
            prompt=prompt,
            system=self.SYSTEM_PROMPT
        )

        # Parse structured output
        plan = self._parse_plan(response)

        return AgentMessage(
            sender=self.config.agent_id,
            recipient="orchestrator",
            message_type="test_plan",
            content=plan.model_dump()
        )

    async def _gather_context(self, project_path: str) -> Dict:
        """Gather project context for planning."""
        files = await self.execute_tool(
            "list_files",
            path=project_path,
            pattern="**/*.py"
        )

        coverage = await self.execute_tool(
            "get_coverage_report",
            path=project_path
        )

        history = await self.execute_tool(
            "get_git_history",
            path=project_path,
            days=30
        )

        return {
            "files": files,
            "coverage": coverage,
            "recent_changes": history
        }
\end{verbatim}

\subsection{Code Analysis Agent Implementation}

The Code Analysis Agent examines source code to extract context for test generation:

\begin{verbatim}
class CodeAnalysis(BaseModel):
    """Result of code analysis."""
    file_path: str
    module_summary: str
    classes: List[ClassInfo]
    functions: List[FunctionInfo]
    dependencies: List[str]
    complexity_metrics: Dict[str, float]
    suggested_test_cases: List[str]

class FunctionInfo(BaseModel):
    """Information about a function."""
    name: str
    signature: str
    docstring: Optional[str]
    line_start: int
    line_end: int
    complexity: int
    parameters: List[ParameterInfo]
    return_type: Optional[str]
    raises: List[str]
    calls: List[str]

class CodeAnalysisAgent(BaseAgent):
    """Agent for analyzing source code."""

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "read_file": self._read_file,
            "parse_ast": self._parse_ast,
            "get_dependencies": self._get_dependencies,
            "calculate_complexity": self._calculate_complexity,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Analyze code files for test generation."""
        files = message.content.get("target_files", [])
        context_hints = message.content.get("context", {})

        analyses = []
        for file_path in files:
            analysis = await self._analyze_file(
                file_path,
                context_hints
            )
            analyses.append(analysis)

        return AgentMessage(
            sender=self.config.agent_id,
            recipient=message.content.get("reply_to", "orchestrator"),
            message_type="code_analysis",
            content={"analyses": [a.model_dump() for a in analyses]}
        )

    async def _analyze_file(self, file_path: str,
                            hints: Dict) -> CodeAnalysis:
        """Perform detailed analysis of a single file."""
        # Read and parse the file
        content = await self.execute_tool("read_file", path=file_path)
        ast_info = await self.execute_tool("parse_ast", code=content)

        # Extract structural information
        classes = self._extract_classes(ast_info)
        functions = self._extract_functions(ast_info)

        # Calculate metrics
        complexity = await self.execute_tool(
            "calculate_complexity",
            code=content
        )

        # Use LLM for semantic understanding
        summary_prompt = f"""Analyze this Python code and provide:
        1. A brief summary of the module's purpose
        2. Key functionality and business logic
        3. Edge cases and error conditions to test
        4. Suggested test scenarios

        Code:
        ```python
        {content[:8000]}  # Truncate for context limits
        ```
        """

        llm_analysis = await self.invoke_llm(summary_prompt)

        return CodeAnalysis(
            file_path=file_path,
            module_summary=self._extract_summary(llm_analysis),
            classes=classes,
            functions=functions,
            dependencies=ast_info.get("imports", []),
            complexity_metrics=complexity,
            suggested_test_cases=self._extract_suggestions(llm_analysis)
        )
\end{verbatim}

\subsection{Test Generation Agent Implementation}

The Test Generation Agent produces executable test code:

\begin{verbatim}
class GeneratedTest(BaseModel):
    """A generated test case."""
    test_name: str
    test_code: str
    target_function: str
    test_type: str
    rationale: str
    assertions: List[str]
    fixtures_needed: List[str]

class TestGenerationAgent(BaseAgent):
    """Agent for generating test code."""

    SYSTEM_PROMPT = """You are an expert test engineer.
    Generate high-quality pytest test cases that:

    1. Test one behavior per test function
    2. Use descriptive test names (test_<function>_<scenario>)
    3. Include meaningful assertions
    4. Handle edge cases and error conditions
    5. Use appropriate fixtures and mocking
    6. Follow the Arrange-Act-Assert pattern

    Output valid, executable Python code using pytest.
    Include necessary imports and fixtures.
    """

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "read_file": self._read_file,
            "validate_syntax": self._validate_syntax,
            "check_imports": self._check_imports,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Generate tests based on analysis."""
        analysis = message.content.get("analysis")
        task = message.content.get("task")

        generated_tests = []

        for function in analysis.get("functions", []):
            tests = await self._generate_tests_for_function(
                function,
                analysis,
                task
            )
            generated_tests.extend(tests)

        # Validate all generated tests
        validated = await self._validate_tests(generated_tests)

        return AgentMessage(
            sender=self.config.agent_id,
            recipient="orchestrator",
            message_type="generated_tests",
            content={
                "tests": [t.model_dump() for t in validated],
                "task_id": task.get("task_id")
            }
        )

    async def _generate_tests_for_function(
        self,
        function: Dict,
        analysis: Dict,
        task: Dict
    ) -> List[GeneratedTest]:
        """Generate tests for a single function."""

        prompt = f"""Generate pytest tests for this function:

Function: {function['name']}
Signature: {function['signature']}
Docstring: {function.get('docstring', 'None')}

Module context:
{analysis.get('module_summary', '')}

Suggested test scenarios:
{analysis.get('suggested_test_cases', [])}

Generate tests covering:
1. Normal/happy path behavior
2. Edge cases (empty inputs, boundaries)
3. Error handling (invalid inputs, exceptions)
4. Any specific scenarios from the docstring

Return as JSON array with structure:
[{{"test_name": "...", "test_code": "...",
   "rationale": "...", "assertions": [...]}}]
"""

        response = await self.invoke_llm(
            prompt=prompt,
            system=self.SYSTEM_PROMPT
        )

        tests = self._parse_generated_tests(response, function['name'])
        return tests

    async def _validate_tests(
        self,
        tests: List[GeneratedTest]
    ) -> List[GeneratedTest]:
        """Validate generated tests for syntax and imports."""
        validated = []

        for test in tests:
            # Check syntax
            syntax_ok = await self.execute_tool(
                "validate_syntax",
                code=test.test_code
            )

            if not syntax_ok:
                self.logger.warning(
                    "Invalid syntax in generated test",
                    test_name=test.test_name
                )
                continue

            # Check imports are resolvable
            imports_ok = await self.execute_tool(
                "check_imports",
                code=test.test_code
            )

            if imports_ok:
                validated.append(test)
            else:
                self.logger.warning(
                    "Unresolvable imports in test",
                    test_name=test.test_name
                )

        return validated
\end{verbatim}

\subsection{Execution Agent Implementation}

The Execution Agent runs tests in sandboxed environments:

\begin{verbatim}
class ExecutionResult(BaseModel):
    """Result of test execution."""
    test_name: str
    status: str  # passed, failed, error, skipped
    duration_ms: float
    output: str
    error_message: Optional[str] = None
    coverage: Optional[Dict[str, float]] = None

class ExecutionAgent(BaseAgent):
    """Agent for executing tests in sandboxes."""

    def __init__(self, config: AgentConfig,
                 llm_gateway: "LLMGateway",
                 security_context: "SecurityContext",
                 sandbox_manager: "SandboxManager"):
        super().__init__(config, llm_gateway, security_context)
        self.sandbox = sandbox_manager

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "create_sandbox": self._create_sandbox,
            "run_in_sandbox": self._run_in_sandbox,
            "collect_results": self._collect_results,
            "cleanup_sandbox": self._cleanup_sandbox,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Execute tests and collect results."""
        tests = message.content.get("tests", [])
        project_path = message.content.get("project_path")

        # Create isolated sandbox
        sandbox_id = await self.execute_tool(
            "create_sandbox",
            base_image="python-test-runner:3.11",
            project_path=project_path,
            resource_limits={
                "cpu": 2,
                "memory": "2g",
                "timeout": 300
            }
        )

        try:
            # Write tests to sandbox
            test_file = self._create_test_file(tests)

            # Execute tests with coverage
            raw_results = await self.execute_tool(
                "run_in_sandbox",
                sandbox_id=sandbox_id,
                command=[
                    "pytest",
                    test_file,
                    "--tb=short",
                    "-v",
                    "--cov=.",
                    "--cov-report=json"
                ]
            )

            # Collect and parse results
            results = await self.execute_tool(
                "collect_results",
                sandbox_id=sandbox_id,
                output=raw_results
            )

        finally:
            # Always cleanup
            await self.execute_tool(
                "cleanup_sandbox",
                sandbox_id=sandbox_id
            )

        return AgentMessage(
            sender=self.config.agent_id,
            recipient="orchestrator",
            message_type="execution_results",
            content={
                "results": [r.model_dump() for r in results],
                "summary": self._summarize_results(results)
            }
        )

    def _create_test_file(self, tests: List[Dict]) -> str:
        """Create a test file from generated tests."""
        imports = set()
        test_code = []

        for test in tests:
            code = test.get("test_code", "")
            # Extract imports
            for line in code.split("\n"):
                if line.startswith("import ") or \
                   line.startswith("from "):
                    imports.add(line)
                else:
                    test_code.append(line)

        full_code = "\n".join(sorted(imports)) + "\n\n"
        full_code += "\n\n".join(test_code)

        return full_code
\end{verbatim}

\subsection{Validation Agent Implementation}

The Validation Agent assesses test quality:

\begin{verbatim}
class ValidationResult(BaseModel):
    """Test validation result."""
    test_name: str
    quality_score: float  # 0-1
    issues: List[ValidationIssue]
    recommendations: List[str]
    mutation_score: Optional[float] = None

class ValidationIssue(BaseModel):
    """A quality issue found in a test."""
    severity: str  # error, warning, info
    category: str  # assertion, coverage, style, flaky
    description: str
    line_number: Optional[int] = None

class ValidationAgent(BaseAgent):
    """Agent for validating test quality."""

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "analyze_assertions": self._analyze_assertions,
            "check_coverage": self._check_coverage,
            "run_mutation_testing": self._run_mutation_testing,
            "detect_flakiness": self._detect_flakiness,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Validate test quality."""
        tests = message.content.get("tests", [])
        execution_results = message.content.get("results", [])

        validations = []

        for test in tests:
            result = await self._validate_test(
                test,
                execution_results
            )
            validations.append(result)

        # Overall quality assessment
        overall = self._calculate_overall_quality(validations)

        return AgentMessage(
            sender=self.config.agent_id,
            recipient="orchestrator",
            message_type="validation_results",
            content={
                "validations": [v.model_dump() for v in validations],
                "overall_quality": overall,
                "recommendations": self._aggregate_recommendations(
                    validations
                )
            }
        )

    async def _validate_test(
        self,
        test: Dict,
        results: List[Dict]
    ) -> ValidationResult:
        """Validate a single test."""
        issues = []

        # Check assertion quality
        assertion_issues = await self.execute_tool(
            "analyze_assertions",
            test_code=test.get("test_code", "")
        )
        issues.extend(assertion_issues)

        # Check for trivial assertions
        if self._has_trivial_assertions(test.get("test_code", "")):
            issues.append(ValidationIssue(
                severity="warning",
                category="assertion",
                description="Test contains trivial assertions "
                           "(assert True, assert x == x)"
            ))

        # Check test isolation
        if self._has_isolation_issues(test.get("test_code", "")):
            issues.append(ValidationIssue(
                severity="warning",
                category="style",
                description="Test may have isolation issues "
                           "(shared state, external dependencies)"
            ))

        # Calculate quality score
        score = self._calculate_quality_score(issues)

        # Generate recommendations using LLM
        recommendations = await self._generate_recommendations(
            test, issues
        )

        return ValidationResult(
            test_name=test.get("test_name", "unknown"),
            quality_score=score,
            issues=issues,
            recommendations=recommendations
        )
\end{verbatim}

\subsection{Security Agent Implementation}

The Security Agent provides security oversight:

\begin{verbatim}
class SecurityScanResult(BaseModel):
    """Result of security scanning."""
    scan_type: str
    passed: bool
    findings: List[SecurityFinding]
    risk_level: str  # low, medium, high, critical

class SecurityFinding(BaseModel):
    """A security issue found."""
    severity: str
    category: str
    description: str
    location: Optional[str] = None
    recommendation: str

class SecurityAgent(BaseAgent):
    """Agent for security scanning and oversight."""

    DANGEROUS_PATTERNS = [
        (r"eval\s*\(", "Use of eval() is dangerous"),
        (r"exec\s*\(", "Use of exec() is dangerous"),
        (r"__import__\s*\(", "Dynamic import may be dangerous"),
        (r"subprocess\.(call|run|Popen)",
         "Subprocess execution requires review"),
        (r"os\.system\s*\(", "os.system() is dangerous"),
        (r"pickle\.loads?\s*\(", "Pickle deserialization is unsafe"),
        (r"yaml\.load\s*\([^)]*Loader\s*=\s*None",
         "Unsafe YAML loading"),
    ]

    def _register_tools(self) -> Dict[str, callable]:
        return {
            "scan_code": self._scan_code,
            "check_dependencies": self._check_dependencies,
            "scan_secrets": self._scan_secrets,
            "verify_sandbox_config": self._verify_sandbox_config,
        }

    async def process(self, message: AgentMessage) -> AgentMessage:
        """Perform security analysis."""
        scan_type = message.content.get("scan_type", "full")
        target = message.content.get("target")

        results = []

        if scan_type in ["full", "code"]:
            code_scan = await self._scan_code_security(target)
            results.append(code_scan)

        if scan_type in ["full", "dependencies"]:
            dep_scan = await self._scan_dependencies(target)
            results.append(dep_scan)

        if scan_type in ["full", "secrets"]:
            secret_scan = await self._scan_for_secrets(target)
            results.append(secret_scan)

        # Determine overall risk
        overall_risk = self._calculate_overall_risk(results)

        # Block if high-risk issues found
        should_block = overall_risk in ["high", "critical"]

        return AgentMessage(
            sender=self.config.agent_id,
            recipient="orchestrator",
            message_type="security_scan",
            content={
                "results": [r.model_dump() for r in results],
                "overall_risk": overall_risk,
                "should_block": should_block,
                "requires_review": overall_risk != "low"
            }
        )

    async def _scan_code_security(
        self,
        code: str
    ) -> SecurityScanResult:
        """Scan code for security issues."""
        findings = []

        # Pattern-based detection
        for pattern, description in self.DANGEROUS_PATTERNS:
            import re
            matches = re.finditer(pattern, code)
            for match in matches:
                findings.append(SecurityFinding(
                    severity="high",
                    category="dangerous_function",
                    description=description,
                    location=f"character {match.start()}",
                    recommendation="Review and remove or sandbox"
                ))

        # LLM-based analysis for subtle issues
        llm_findings = await self._llm_security_analysis(code)
        findings.extend(llm_findings)

        return SecurityScanResult(
            scan_type="code",
            passed=len([f for f in findings
                       if f.severity in ["high", "critical"]]) == 0,
            findings=findings,
            risk_level=self._determine_risk_level(findings)
        )
\end{verbatim}

%----------------------------------------------------------------------------------------

\section{Security Controls Implementation}
\label{sec:security_impl}

\subsection{Sandbox Manager}

The Sandbox Manager provisions and manages isolated execution environments:

\begin{verbatim}
class SandboxConfig(BaseModel):
    """Configuration for sandbox environment."""
    base_image: str
    cpu_limit: int = 2
    memory_limit: str = "2g"
    timeout_seconds: int = 300
    network_mode: str = "none"  # none, allowlist
    allowed_hosts: List[str] = []
    read_only_paths: List[str] = []
    writable_paths: List[str] = []

class SandboxManager:
    """Manages sandboxed execution environments."""

    def __init__(self, docker_client: docker.DockerClient):
        self.docker = docker_client
        self.active_sandboxes: Dict[str, Container] = {}

    async def create_sandbox(
        self,
        config: SandboxConfig,
        project_path: str
    ) -> str:
        """Create a new sandbox container."""
        sandbox_id = str(uuid.uuid4())[:8]

        # Security configuration
        security_opt = [
            "no-new-privileges:true",
            "seccomp=default.json"
        ]

        # Resource limits
        host_config = self.docker.api.create_host_config(
            mem_limit=config.memory_limit,
            cpu_period=100000,
            cpu_quota=config.cpu_limit * 100000,
            network_mode=config.network_mode,
            security_opt=security_opt,
            read_only=True,
            tmpfs={"/tmp": "size=100m,mode=1777"},
            cap_drop=["ALL"],
            cap_add=["CHOWN", "SETUID", "SETGID"],
        )

        # Create container
        container = self.docker.containers.create(
            config.base_image,
            name=f"sandbox-{sandbox_id}",
            user="sandbox",
            working_dir="/workspace",
            volumes={
                project_path: {
                    "bind": "/workspace",
                    "mode": "ro"  # Read-only by default
                }
            },
            host_config=host_config,
            detach=True
        )

        self.active_sandboxes[sandbox_id] = container
        container.start()

        return sandbox_id

    async def execute_in_sandbox(
        self,
        sandbox_id: str,
        command: List[str],
        timeout: int = 300
    ) -> Tuple[int, str, str]:
        """Execute command in sandbox with timeout."""
        container = self.active_sandboxes.get(sandbox_id)
        if not container:
            raise ValueError(f"Sandbox {sandbox_id} not found")

        try:
            exec_result = container.exec_run(
                command,
                user="sandbox",
                workdir="/workspace",
                demux=True
            )

            return (
                exec_result.exit_code,
                exec_result.output[0].decode() if exec_result.output[0] else "",
                exec_result.output[1].decode() if exec_result.output[1] else ""
            )

        except Exception as e:
            return (-1, "", str(e))

    async def cleanup_sandbox(self, sandbox_id: str):
        """Remove sandbox container."""
        container = self.active_sandboxes.pop(sandbox_id, None)
        if container:
            try:
                container.stop(timeout=5)
                container.remove(force=True)
            except Exception:
                pass  # Best effort cleanup
\end{verbatim}

\subsection{PII Scrubbing Pipeline}

The PII scrubber removes sensitive information before LLM transmission:

\begin{verbatim}
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
import re

class PIIScrubber:
    """Scrubs PII from text before LLM transmission."""

    # Patterns for code-specific secrets
    SECRET_PATTERNS = [
        (r'(?i)(api[_-]?key|apikey)\s*[=:]\s*["\']?[\w-]+',
         '<API_KEY_REDACTED>'),
        (r'(?i)(password|passwd|pwd)\s*[=:]\s*["\']?[^\s"\']+',
         '<PASSWORD_REDACTED>'),
        (r'(?i)(secret|token)\s*[=:]\s*["\']?[\w-]+',
         '<SECRET_REDACTED>'),
        (r'(?i)(aws_access_key_id)\s*[=:]\s*["\']?[A-Z0-9]+',
         '<AWS_KEY_REDACTED>'),
        (r'ghp_[a-zA-Z0-9]{36}', '<GITHUB_TOKEN_REDACTED>'),
        (r'sk-[a-zA-Z0-9]{48}', '<OPENAI_KEY_REDACTED>'),
    ]

    def __init__(self):
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()

    def scrub(self, text: str) -> Tuple[str, List[Dict]]:
        """Scrub PII from text, returning cleaned text and log."""
        scrub_log = []

        # First pass: regex-based secret detection
        cleaned = text
        for pattern, replacement in self.SECRET_PATTERNS:
            matches = re.findall(pattern, cleaned)
            if matches:
                cleaned = re.sub(pattern, replacement, cleaned)
                scrub_log.append({
                    "type": "secret",
                    "count": len(matches),
                    "replacement": replacement
                })

        # Second pass: Presidio NER-based PII detection
        results = self.analyzer.analyze(
            cleaned,
            entities=[
                "PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER",
                "CREDIT_CARD", "IP_ADDRESS", "LOCATION"
            ],
            language="en"
        )

        if results:
            anonymized = self.anonymizer.anonymize(
                cleaned,
                results
            )
            cleaned = anonymized.text

            for result in results:
                scrub_log.append({
                    "type": result.entity_type,
                    "score": result.score,
                    "start": result.start,
                    "end": result.end
                })

        return cleaned, scrub_log
\end{verbatim}

\subsection{Permission Enforcement}

The permission system enforces least-privilege access:

\begin{verbatim}
class PermissionPolicy(BaseModel):
    """Permission policy for an agent."""
    agent_id: str
    file_read: List[str] = []    # Glob patterns
    file_write: List[str] = []   # Glob patterns
    network: List[str] = []      # URL patterns
    execute: List[str] = []      # Command allowlist

class PermissionEnforcer:
    """Enforces permission policies for agent actions."""

    def __init__(self, policies: Dict[str, PermissionPolicy]):
        self.policies = policies
        self.audit_logger = AuditLogger()

    def check_permission(
        self,
        agent_id: str,
        action: str,
        resource: str
    ) -> bool:
        """Check if agent has permission for action."""
        policy = self.policies.get(agent_id)
        if not policy:
            self._log_denial(agent_id, action, resource,
                           "no policy found")
            return False

        allowed = False

        if action == "file_read":
            allowed = self._check_glob_match(
                resource, policy.file_read
            )
        elif action == "file_write":
            allowed = self._check_glob_match(
                resource, policy.file_write
            )
        elif action == "network":
            allowed = self._check_url_match(
                resource, policy.network
            )
        elif action == "execute":
            allowed = self._check_command_allowed(
                resource, policy.execute
            )

        if not allowed:
            self._log_denial(agent_id, action, resource,
                           "policy denied")
        else:
            self._log_allowed(agent_id, action, resource)

        return allowed

    def _check_glob_match(
        self,
        path: str,
        patterns: List[str]
    ) -> bool:
        """Check if path matches any allowed pattern."""
        from fnmatch import fnmatch
        return any(fnmatch(path, p) for p in patterns)

    def _log_denial(self, agent_id: str, action: str,
                    resource: str, reason: str):
        """Log permission denial."""
        self.audit_logger.log(
            event="permission_denied",
            agent_id=agent_id,
            action=action,
            resource=resource,
            reason=reason
        )
\end{verbatim}

\subsection{Audit Logging}

Comprehensive audit logging supports security monitoring and compliance:

\begin{verbatim}
class AuditLogger:
    """Structured audit logging for security events."""

    def __init__(self, log_path: str = "audit.log"):
        self.logger = structlog.get_logger("audit")
        self.log_path = log_path

    def log(self, event: str, **kwargs):
        """Log an audit event with structured data."""
        entry = AuditEntry(
            timestamp=datetime.utcnow().isoformat(),
            event=event,
            **kwargs
        )

        # Write to structured log
        self.logger.info(
            event,
            **entry.model_dump()
        )

        # Append to audit file (append-only)
        with open(self.log_path, "a") as f:
            f.write(entry.model_dump_json() + "\n")

    def log_llm_interaction(
        self,
        agent_id: str,
        prompt_hash: str,
        prompt_length: int,
        response_length: int,
        model: str,
        tokens_used: int
    ):
        """Log LLM API interaction."""
        self.log(
            event="llm_interaction",
            agent_id=agent_id,
            prompt_hash=prompt_hash,
            prompt_length=prompt_length,
            response_length=response_length,
            model=model,
            tokens_used=tokens_used
        )

    def log_tool_execution(
        self,
        agent_id: str,
        tool: str,
        parameters_hash: str,
        success: bool,
        error: Optional[str] = None
    ):
        """Log tool execution."""
        self.log(
            event="tool_execution",
            agent_id=agent_id,
            tool=tool,
            parameters_hash=parameters_hash,
            success=success,
            error=error
        )
\end{verbatim}

%----------------------------------------------------------------------------------------

\section{Workflow Implementation}
\label{sec:workflow_impl}

\subsection{Workflow Engine}

The workflow engine orchestrates agent interactions:

\begin{verbatim}
class WorkflowState(Enum):
    """Possible workflow states."""
    PENDING = "pending"
    PLANNING = "planning"
    ANALYZING = "analyzing"
    GENERATING = "generating"
    EXECUTING = "executing"
    VALIDATING = "validating"
    REVIEWING = "reviewing"
    COMPLETED = "completed"
    FAILED = "failed"

class TestingWorkflow:
    """Orchestrates the test generation workflow."""

    def __init__(
        self,
        agents: Dict[str, BaseAgent],
        state_manager: StateManager
    ):
        self.agents = agents
        self.state = state_manager

    async def run(
        self,
        project_path: str,
        requirements: str
    ) -> WorkflowResult:
        """Execute the full testing workflow."""
        workflow_id = str(uuid.uuid4())

        try:
            # Phase 1: Planning
            await self.state.transition(
                workflow_id, WorkflowState.PLANNING
            )
            plan = await self._run_planning(
                project_path, requirements
            )

            # Phase 2: Analysis
            await self.state.transition(
                workflow_id, WorkflowState.ANALYZING
            )
            analyses = await self._run_analysis(plan)

            # Phase 3: Generation
            await self.state.transition(
                workflow_id, WorkflowState.GENERATING
            )
            tests = await self._run_generation(plan, analyses)

            # Phase 4: Security Scan
            security_results = await self._run_security_scan(tests)
            if security_results.should_block:
                return WorkflowResult(
                    status="blocked",
                    reason="Security scan failed",
                    security_findings=security_results.findings
                )

            # Phase 5: Execution
            await self.state.transition(
                workflow_id, WorkflowState.EXECUTING
            )
            execution_results = await self._run_execution(tests)

            # Phase 6: Validation
            await self.state.transition(
                workflow_id, WorkflowState.VALIDATING
            )
            validation = await self._run_validation(
                tests, execution_results
            )

            # Phase 7: Human Review (if configured)
            if self._requires_review(validation):
                await self.state.transition(
                    workflow_id, WorkflowState.REVIEWING
                )
                review_result = await self._await_human_review(
                    tests, validation
                )
                if not review_result.approved:
                    return WorkflowResult(
                        status="rejected",
                        reason=review_result.feedback
                    )

            # Complete
            await self.state.transition(
                workflow_id, WorkflowState.COMPLETED
            )

            return WorkflowResult(
                status="completed",
                tests=tests,
                execution_results=execution_results,
                validation=validation
            )

        except Exception as e:
            await self.state.transition(
                workflow_id, WorkflowState.FAILED
            )
            raise
\end{verbatim}

%----------------------------------------------------------------------------------------

\section{Integration with Development Environment}
\label{sec:dev_integration}

\subsection{Command-Line Interface}

The CLI provides the primary user interface:

\begin{verbatim}
import click

@click.group()
def cli():
    """MAS Testing - Multi-Agent Software Testing System"""
    pass

@cli.command()
@click.argument('project_path')
@click.option('--requirements', '-r',
              help='Testing requirements')
@click.option('--output', '-o', default='tests/generated',
              help='Output directory for generated tests')
@click.option('--model', default='gpt-4-turbo',
              help='LLM model to use')
@click.option('--local', is_flag=True,
              help='Use local LLM instead of API')
def generate(project_path, requirements, output, model, local):
    """Generate tests for a project."""
    config = load_config()

    if local:
        config.llm_provider = "ollama"
        config.model = "codellama:34b"
    else:
        config.model = model

    workflow = TestingWorkflow.from_config(config)

    with click.progressbar(length=100) as bar:
        result = asyncio.run(
            workflow.run(project_path, requirements)
        )

    if result.status == "completed":
        write_tests(result.tests, output)
        click.echo(f"Generated {len(result.tests)} tests")
    else:
        click.echo(f"Workflow failed: {result.reason}", err=True)

@cli.command()
@click.argument('project_path')
def analyze(project_path):
    """Analyze a project without generating tests."""
    # Implementation
    pass

if __name__ == '__main__':
    cli()
\end{verbatim}

\subsection{CI/CD Integration}

GitHub Actions workflow for automated test generation:

\begin{verbatim}
# .github/workflows/mas-testing.yml
name: MAS Test Generation

on:
  pull_request:
    types: [opened, synchronize]
    paths:
      - 'src/**/*.py'

jobs:
  generate-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install MAS Testing
        run: pip install mas-testing

      - name: Generate Tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          mas-testing generate . \
            --requirements "Generate tests for changed files" \
            --output tests/generated \
            --model gpt-4-turbo

      - name: Run Generated Tests
        run: pytest tests/generated -v

      - name: Upload Test Report
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: test-report.xml
\end{verbatim}

%----------------------------------------------------------------------------------------

\section{Summary}
\label{sec:impl_summary}

This chapter has presented the implementation details of the secure multi-agent testing prototype. Key implementation highlights include:

\begin{itemize}
    \item \textbf{Modular Agent Architecture}: Six specialized agents with clear responsibilities and a common base class providing shared functionality.

    \item \textbf{Security Controls}: Defense-in-depth implementation with sandboxed execution, PII scrubbing, permission enforcement, and comprehensive audit logging.

    \item \textbf{Flexible LLM Integration}: Support for multiple providers (OpenAI, Anthropic, local models) through a unified interface.

    \item \textbf{Practical Integration}: Command-line interface and CI/CD integration enabling real-world deployment.
\end{itemize}

The implementation totals approximately 5,000 lines of Python code across the core components, with an additional 2,000 lines of configuration, tests, and tooling. The modular design enables extension and customization for specific organizational requirements.

The following chapter presents the experimental evaluation of this implementation, assessing its effectiveness, security properties, and performance characteristics.
