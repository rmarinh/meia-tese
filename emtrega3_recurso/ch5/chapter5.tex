% Chapter 5 - Solution Implementation

\chapter{Solution Implementation}
\label{chap:implementation}

This chapter details the implementation of the TestForge prototype, covering the development environment, repository structure, the implementation of each agent, the web interface, and the challenges encountered during development.

%----------------------------------------------------------------------------------------

\section{Development Environment and Repository Structure}
\label{sec:dev_environment}

The prototype was developed in Python 3.11+ and is distributed as a standard Python package via \texttt{pyproject.toml} using the Hatch build system. The repository structure follows the \texttt{src} layout convention:

\begin{lstlisting}[language={}, caption={Repository structure of the TestForge platform.}, label={lst:repo_structure}]
testforge/
  src/testforge/
    main.py                  # FastAPI + CLI entry point
    config.py                # Pydantic Settings
    context_store.py         # Persistent app context (JSON)
    letta_agent.py           # Letta agent with custom tools
    agents/
      base.py                # BaseAgent[InputT, OutputT]
      observer/
        http_proxy.py        # HTTP traffic capture
      mapper/
        api_mapper.py        # Endpoint mapping + schema inference
      analyzer/
        ast_analyzer.py      # Tree-sitter AST parsing
        pattern_extractor.py # Style guide aggregation
      generator/
        api_test_gen.py      # LLM-based test generation
      executor/
        runner.py            # pytest subprocess runner
      validator/
        quality.py           # Quality scoring
        flakiness.py         # Flakiness detection
    orchestration/
      engine.py              # Pipeline dispatcher
      golden_pipeline.py     # Golden Examples pipeline
      observer_pipeline.py   # Observer pipeline
    models/
      interactions.py        # HTTPExchange, BrowserEvent
      style_guide.py         # GoldenExample, TestStyleGuide
      test_model.py          # EndpointMap, TestSuite
      results.py             # TestResult, ValidationResult
      app_context.py         # AppContext, RunRecord
    llm/
      gateway.py             # LiteLLM unified gateway
      prompts/
        api_test_gen.py      # Prompt templates
    ui/
      streamlit_app.py       # Web interface
  examples/
    flask_api_tests/
      demo_app.py            # Flask CRUD API for evaluation
      test_users_golden_1.py # Golden example: basic CRUD
      test_users_golden_2.py # Golden example: update/delete/search
      test_users_golden_3.py # Golden example: edge cases
  pyproject.toml
\end{lstlisting}

The project declares 14 runtime dependencies and 4 optional dependency groups (\texttt{observer}, \texttt{letta-server}, \texttt{docker}, \texttt{dev}) in \texttt{pyproject.toml}.

%----------------------------------------------------------------------------------------

\section{Implementation of the Analyser Agent}
\label{sec:impl_analyzer}

The Analyser Agent is implemented in two modules: \texttt{ast\_analyzer.py} handles Tree-sitter parsing of individual files, and \texttt{pattern\_extractor.py} aggregates results across files into a \texttt{TestStyleGuide}.

\subsection{Tree-sitter Integration}

The AST analyser initialises a Tree-sitter parser \parencite{treesitter} with the Python grammar and traverses the syntax tree to extract structural elements:

\begin{lstlisting}[language=Python, caption={Tree-sitter AST parsing of test files (simplified).}, label={lst:ast_parser}]
import tree_sitter_python as tspython
from tree_sitter import Language, Parser

PY_LANGUAGE = Language(tspython.language())
parser = Parser(PY_LANGUAGE)

def parse_test_file(source: str) -> GoldenExample:
    tree = parser.parse(source.encode("utf-8"))
    root = tree.root_node
    imports = _extract_imports(root)
    fixtures = _extract_fixtures(root)
    test_functions = _extract_test_functions(root, source)
    # ... returns GoldenExample with all extracted elements
\end{lstlisting}

\subsection{HTTP Method and Endpoint Extraction}

For each test function, the analyser detects HTTP client calls and extracts the targeted endpoints using regular expressions applied to the function body text:

\begin{lstlisting}[language=Python, caption={Endpoint extraction from f-string URLs.}, label={lst:endpoint_extraction}]
import re

HTTP_METHODS = ["get", "post", "put", "delete", "patch"]

for method in HTTP_METHODS:
    # Match: requests.post(f"{base_url}/api/users/{id}")
    pattern = rf'\.{method}\s*\(\s*f["\'][^"\']*\}}(/[^"\']*)["\']'
    match = re.search(pattern, func_body, re.IGNORECASE)
    if match:
        endpoint = match.group(1)
        # Normalise: /api/users/{...} -> /api/users/{id}
\end{lstlisting}

\subsection{Style Guide Aggregation}

The \texttt{AnalyserAgent} calls \texttt{parse\_golden\_files()} on all input files, then builds a \texttt{TestStyleGuide}. It detects the testing framework from imports (presence of \texttt{pytest} or \texttt{unittest}) and identifies the HTTP client (presence of \texttt{requests}, \texttt{httpx}, or \texttt{aiohttp}). It then collects imports that appear in at least 50\% of files as ``common imports'' and fixtures that appear in two or more files as ``common fixtures''. Finally, it computes average metrics including assertions per test, test function length, and docstring coverage.

%----------------------------------------------------------------------------------------

\section{Implementation of the Generator Agent}
\label{sec:impl_generator}

The Generator Agent constructs a multi-part prompt and invokes the LLM via the LiteLLM gateway \parencite{litellm}.

\subsection{Prompt Construction}

The system prompt establishes the agent's role and defines eight categories of insightful tests, ranging from boundary-value analysis and error-handling verification to state-transition and concurrency checks. The user prompt is assembled from four distinct sections that together give the LLM enough context to produce tests that mirror the project's existing style. The first section contains the style guide summary, which includes the detected testing framework, the HTTP client library in use, naming conventions observed in the golden files, and any common fixtures that tests are expected to rely on. The second section provides the full source code of every golden example file, serving as few-shot context so that the LLM can observe real assertion patterns, setup and teardown idioms, and the level of detail expected in docstrings. The third section supplies application context when it is available from a previous run or from the ContextStore: this includes a list of already-tested endpoints, a list of untested endpoints discovered by the Observer pipeline, and any coverage gaps identified by the Validator. Including this information steers the LLM toward generating tests for parts of the API that have not yet been exercised. The fourth and final section is the generation instruction itself, which specifies the target number of test functions and reminds the LLM to produce insightful, non-trivial tests rather than simple status-code assertions.

\subsection{Response Parsing}

The LLM's response is processed through several stages to extract executable Python code. Because different LLM providers wrap their output in varying Markdown conventions, the parser first strips any Markdown code fences such as \texttt{```python ... ```} that surround the generated source. The cleaned response is then split into individual test functions by scanning for \texttt{def test\_} patterns; each match marks the beginning of a new function, and everything up to the next match or the end of the response is treated as the function body. Before the first test function, the parser identifies a preamble section that typically contains import statements, module-level constants, and helper functions, and this preamble is extracted separately so that it can be written once at the top of the output file rather than duplicated across tests. In cases where the LLM omits import statements entirely, which occurs occasionally with smaller local models, the generator falls back to the common imports collected by the Analyser Agent from the style guide. Finally, each extracted test function is wrapped in a \texttt{GeneratedTest} Pydantic model that carries the function name, its source code, a confidence score estimated from the LLM's own hedging language, and metadata about which endpoint it targets.

%----------------------------------------------------------------------------------------

\section{Implementation of the Observer and Mapper Agents}
\label{sec:impl_observer_mapper}

\subsection{Observer Agent}

The Observer Agent is responsible for capturing HTTP traffic between a client and the target application, and it supports three capture modes to accommodate different stages of the testing workflow. The simplest mode is the pre-captured mode, which accepts a list of Python dictionaries containing request and response fields and converts them into typed \texttt{HTTPExchange} objects. This mode is useful when traffic has already been collected by an external tool or when the user wants to supply endpoint definitions manually through the Streamlit interface. The second mode is HAR file parsing, which reads the standard HTTP Archive format exported by browser developer tools. The parser iterates over the \texttt{log.entries} array in the HAR JSON structure, extracting the request method, full URL, headers, request body, response status code, and response body for each entry. This mode bridges the gap between manual exploratory testing in a browser and automated test generation, since testers can simply export their browsing session and feed it into TestForge. The third and most dynamic mode is the live proxy, which uses mitmproxy \parencite{mitmproxy} to intercept HTTP traffic in real time. The implementation registers a custom mitmproxy addon class that implements the \texttt{response()} hook; each time a complete request--response cycle finishes, the addon records the exchange into an in-memory list that the Observer Agent later returns to the pipeline.

\subsection{Mapper Agent}

The Mapper Agent processes normalised exchanges into an \texttt{EndpointMap}:

\begin{lstlisting}[language=Python, caption={Path normalisation in the Mapper Agent.}, label={lst:path_normalisation}]
def _normalize_path(self, path: str) -> str:
    parts = path.strip("/").split("/")
    normalised = []
    for part in parts:
        if part.isdigit():
            normalised.append("{id}")
        elif self._is_uuid(part):
            normalised.append("{uuid}")
        else:
            normalised.append(part)
    return "/" + "/".join(normalised)
\end{lstlisting}

Schema inference examines JSON bodies across multiple exchanges to the same normalised endpoint, recording field names, types, and frequency of occurrence.

%----------------------------------------------------------------------------------------

\section{Implementation of the Executor Agent}
\label{sec:impl_executor}

The Executor Agent writes the generated test suite to a temporary directory, generates a \texttt{conftest.py} with common fixtures, and runs pytest \parencite{pytest2024} as a subprocess.

\subsection{Auto-generated conftest.py}

When the test suite does not include its own conftest, the Executor auto-generates one that provides the fixtures most commonly referenced in golden examples. The first fixture, \texttt{base\_url}, reads the application's base URL from the \texttt{BASE\_URL} environment variable and returns it as a plain string, allowing every test function to construct endpoint URLs without hard-coding a host or port. The second fixture, \texttt{created\_user}, performs a full lifecycle: it sends a POST request to the user-creation endpoint with a unique email and a default role, yields the resulting user dictionary to the test function so that assertions can inspect it, and then issues a DELETE request in its teardown phase to clean up. The third fixture, \texttt{sample\_user}, follows the same pattern but creates a user with a different role, enabling test scenarios that require two distinct users (for example, verifying that search filters correctly distinguish between roles). Each of these fixtures generates a unique email address by calling \texttt{id(object())}, which produces a different integer on every invocation and thus avoids duplicate-detection conflicts both within a single test run and across consecutive runs against the same application instance.

\subsection{Pytest Output Parsing}

The Executor runs pytest with the \texttt{-v} (verbose) and \texttt{--tb=short} flags and then applies a multi-stage parser to the captured standard output. In the first stage, the parser scans each line for the verbose-mode pattern \texttt{::test\_name PASSED/FAILED/ERROR/SKIPPED}, which pytest emits once per collected test item. For every line that matches, the parser records the test name and its outcome into a \texttt{TestResult} object. In the second stage, the parser locates the \texttt{FAILURES} section of the pytest output and associates each failure traceback with the corresponding test name, so that the Validator and the user can inspect exactly which assertion failed and on which line. If the verbose-mode parsing yields no individual results, which can happen when pytest itself crashes during collection or encounters a syntax error in the generated code, the parser falls back to the short test summary lines that pytest prints near the bottom of its output, matching lines of the form \texttt{FAILED path::test\_name}. As a last resort, when even the short summary is absent, the parser examines the process exit code and creates a single suite-level result that marks the entire run as passed or failed. This layered approach ensures that the Executor always returns structured results regardless of how the generated tests behave at runtime.

%----------------------------------------------------------------------------------------

\section{Implementation of the Validator Agent}
\label{sec:impl_validator}

The Validator Agent scores each generated test on a 0--1 scale across four dimensions, each of which captures a different aspect of test quality. This section describes how each dimension is computed and how the overall score is derived.

The first dimension is Assertion Quality, scored from 0 to 1. The scorer counts the number of \texttt{assert} statements in each test function and classifies them by type: status-code checks, response-body field checks, collection-length checks, and exception assertions. A test that contains at least three assertions spanning two or more of these categories receives the highest score, while a test with a single status-code assertion scores significantly lower. This encourages the Generator to produce tests that verify multiple properties of an API response rather than merely checking that the server returned HTTP 200.

The second dimension is Coverage Breadth, also scored from 0 to 1. It measures how many distinct API endpoints are exercised across the entire test suite. The Validator normalises paths using the same logic as the Mapper Agent, so that \texttt{/api/users/1} and \texttt{/api/users/42} both map to \texttt{/api/users/\{id\}}. Suites that spread their tests across more normalised endpoints receive higher scores, which incentivises the Generator to avoid clustering all tests on a single endpoint.

The third dimension is Readability, scored from 0 to 1. It assesses three sub-factors: test function length, where both very short functions (fewer than three lines) and very long functions (more than fifty lines) are penalised; the presence of a docstring that explains the test's purpose; and whether the function name follows a descriptive naming convention such as \texttt{test\_create\_user\_with\_duplicate\_email}. These heuristics reward tests that a human reviewer could understand without tracing through the implementation.

The fourth dimension is Execution Result, which is binary: a test receives a score of 1 if pytest reports it as passed and 0 if it failed or raised an error. When execution was skipped (for instance, because the target application was offline), this dimension is excluded from the average so that it does not unfairly penalise the suite. The overall quality score is computed as the arithmetic mean of all available dimensions. In addition to the numerical score, the Validator flags common issues such as tests with no assertions at all, tests that omit status-code checks, and tests exceeding fifty lines, providing actionable feedback that can guide regeneration in future iterations.

%----------------------------------------------------------------------------------------

\section{Streamlit Web Interface}
\label{sec:streamlit_ui}

The web interface provides three tabs, each corresponding to one of the three pipeline modes supported by TestForge. This section describes the layout and functionality of each tab as well as the global configuration sidebar.

The first tab, Golden Examples, is designed for users who already possess a set of hand-written test files that embody their preferred testing style. The tab presents a file-upload widget that accepts one or more Python test files, together with text fields for the target application's base URL, a free-text application description that is forwarded to the LLM as additional context, and a numeric input for the desired number of tests to generate. Once the user clicks the generation button, the tab displays the generated test code in a syntax-highlighted code block, followed by a table of execution results showing each test's name and pass/fail status, and finally a summary of the quality scores computed by the Validator Agent.

The second tab, Observer Mode, targets users who want to generate tests from observed HTTP traffic rather than from existing test files. It accepts either a JSON text area where the user can paste endpoint definitions as a list of dictionaries, or a HAR file upload for traffic captured in a browser. After processing, the tab displays the discovered endpoint map (a table listing each normalised path, the HTTP methods observed, and the inferred request and response schemas), followed by the generated tests and their execution results.

The third tab, Combined Mode, merges both input sources. It presents the golden file upload alongside the observer data input, allowing TestForge to use the style guide extracted from golden examples while also incorporating endpoint coverage information from observed traffic. The tab displays the style guide summary and the endpoint map side by side, followed by the generation output and the execution and validation results.

A sidebar visible on all tabs provides global configuration options: the base URL of the target application, a free-text description field, the number of tests to generate, a dropdown for selecting the LLM model (which lists all models available through the configured LiteLLM gateway), and the API base URL for the LLM provider, defaulting to the local Ollama endpoint.

%----------------------------------------------------------------------------------------

\section{Implementation Challenges and Solutions}
\label{sec:impl_challenges}

Several significant challenges were encountered during development, each of which required iterative investigation before a workable solution emerged. This section documents the most impactful issues and the strategies adopted to resolve them.

The first major challenge concerned deploying the Letta server. The Letta framework \parencite{packer2023memgpt} relies on PostgreSQL for its internal scheduler and memory-persistence components. Initial attempts to install Letta directly via pip on the development machine failed because the \texttt{asyncpg} Python package requires a locally installed PostgreSQL server and its associated C headers, which were not present. After exploring several workarounds, including installing PostgreSQL natively and attempting to stub out the dependency, the most reliable solution proved to be deploying Letta through its official Docker image, which bundles both the Letta server and a PostgreSQL instance in a single container accessible on port 8283.

A related difficulty was parsing the responses returned by the Letta client. Each call to the Letta API returns a \texttt{LettaResponse} object that may contain multiple message types interleaved: internal reasoning steps, tool-call invocations, and user-facing assistant messages. Naively concatenating all messages produced noisy output that included the agent's chain-of-thought reasoning. The solution was to filter explicitly for \texttt{AssistantMessage} objects whose \texttt{content} field is non-empty, discarding all other message types before presenting the response to the user.

Local LLM inference introduced a timeout challenge. Running Ollama with the llama3.1:8b model on consumer hardware is significantly slower than calling a cloud API, with individual generation requests sometimes taking over two minutes to complete. The default HTTP client timeout of thirty seconds was therefore insufficient, causing requests to fail midway through generation. This was resolved by introducing separate timeout parameters in the LiteLLM gateway configuration: 300 seconds for the inference timeout and 30 seconds for the initial connection timeout.

The AST analyser initially could not extract endpoints from f-string URLs. When golden examples contained calls such as \texttt{requests.post(f"\{base\_url\}/api/users")}, the Tree-sitter parser represented the f-string as a \texttt{formatted\_string} node whose children are interpolation fragments, making it difficult to reconstruct the literal path portion. Adding a regex-based extraction pass that operates on the raw function body text, rather than on the AST, provided a complementary strategy that reliably captures the path segment after the interpolated base URL variable.

Conftest fixture generation was another source of failures. The LLM, having observed fixtures such as \texttt{created\_user} and \texttt{sample\_user} in the golden examples, would generate tests that referenced these fixtures by name. However, because the auto-generated conftest did not initially include them, pytest would report collection errors for every such test. The fix was to include these commonly referenced fixtures in the default conftest template so that they are always available, regardless of whether the golden examples shipped their own conftest file.

Test state pollution caused intermittent failures across consecutive runs. Early versions of the conftest fixtures used hardcoded email addresses when creating test users, which meant that a second run against the same application instance would trigger duplicate-detection logic in the target API and return HTTP 409 Conflict responses. Replacing the hardcoded values with dynamically generated email addresses based on \texttt{id(object())}, which produces a unique integer on each call, eliminated this class of failure entirely.

Finally, pytest output parsing proved unexpectedly fragile. The initial implementation passed both \texttt{-v} (verbose) and \texttt{-q} (quiet) flags to pytest, which are contradictory: the quiet flag suppresses the per-test status lines that the verbose flag would normally emit. The resulting output was a compact summary that the regex-based parser could not decompose into individual test results. Removing the \texttt{-q} flag and relying solely on \texttt{-v} together with \texttt{--tb=short} restored the expected output format and allowed the parser to extract per-test outcomes reliably.
