% Appendix C - Detailed Experimental Results

\chapter{Detailed Experimental Results}
\label{AppendixC}

This appendix provides the complete experimental data from the Golden Examples pipeline evaluation described in Chapter~\ref{chap:experimentation}, including the generated test code, per-test execution details, and the validator output.

%----------------------------------------------------------------------------------------

\section{Target Application Endpoints}
\label{sec:target_endpoints}

Table~\ref{tab:target_api} lists all endpoints of the Flask CRUD API used as the target application.

\begin{table}[ht]
\caption{Target application endpoints and their expected behaviour}
\label{tab:target_api}
\centering
\small
\begin{tabular}{l l l p{5.5cm}}
\toprule
\tabhead{Method} & \tabhead{Path} & \tabhead{Success} & \tabhead{Notes} \\
\midrule
GET & /api/health & 200 & Returns \texttt{\{"status": "healthy"\}} \\
GET & /api/users & 200 & Returns list of all users \\
POST & /api/users & 201 & Requires \texttt{name} and \texttt{email}; returns 400 if missing, 409 if duplicate email \\
GET & /api/users/\{id\} & 200 & Returns 404 if not found \\
PUT & /api/users/\{id\} & 200 & Partial update; returns 404 if not found \\
DELETE & /api/users/\{id\} & 200 & Returns 404 if not found \\
GET & /api/users/search & 200 & Query parameter \texttt{q}; returns 400 if missing \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Golden Test Examples Summary}
\label{sec:golden_summary}

Table~\ref{tab:golden_files_detail} summarises the three golden test files provided as input to the pipeline.

\begin{table}[ht]
\caption{Golden test files: per-file breakdown}
\label{tab:golden_files_detail}
\centering
\small
\begin{tabular}{l r r r l}
\toprule
\tabhead{File} & \tabhead{Tests} & \tabhead{Assertions} & \tabhead{Endpoints} & \tabhead{Focus} \\
\midrule
\texttt{test\_users\_golden\_1.py} & 7 & 24 & 4 & Health, create, get, list \\
\texttt{test\_users\_golden\_2.py} & 10 & 31 & 5 & Update, delete, search \\
\texttt{test\_users\_golden\_3.py} & 6 & 19 & 3 & Edge cases, duplicates \\
\midrule
\textbf{Total} & \textbf{23} & \textbf{74} & \textbf{7} & \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Analyser Agent Output}
\label{sec:analyser_output}

The style guide extracted by the Analyser Agent from the 23 golden tests is summarised in Table~\ref{tab:style_guide_detail}.

\begin{table}[ht]
\caption{Extracted style guide details}
\label{tab:style_guide_detail}
\centering
\small
\begin{tabular}{l p{8cm}}
\toprule
\tabhead{Attribute} & \tabhead{Value} \\
\midrule
Framework & \texttt{pytest} \\
HTTP Client & \texttt{requests} \\
Common Imports & \texttt{pytest}, \texttt{requests} \\
Common Fixtures & \texttt{base\_url}, \texttt{created\_user}, \texttt{sample\_user} \\
Naming Convention & \texttt{test\_<action>\_<scenario>} \\
Average Assertions/Test & 3.2 \\
Docstring Coverage & 100\% (23/23 tests) \\
HTTP Methods Used & GET (12), POST (6), PUT (3), DELETE (2) \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Generated Test Details}
\label{sec:generated_test_details}

Table~\ref{tab:generated_tests_detail} provides per-test details for the 9 tests generated by the Golden Examples pipeline, including the number of assertions, endpoints targeted, and failure details.

\begin{table}[ht]
\caption{Per-test details of the 9 generated tests}
\label{tab:generated_tests_detail}
\centering
\small
\begin{tabular}{l l r l p{3.5cm}}
\toprule
\tabhead{Test Name} & \tabhead{Category} & \tabhead{Asserts} & \tabhead{Result} & \tabhead{Failure Reason} \\
\midrule
\texttt{test\_data\_round\_trip} & State Integrity & 5 & PASSED & --- \\
\texttt{test\_partial\_update\_safety} & State Integrity & 4 & PASSED & --- \\
\texttt{test\_delete\_consistency} & State Integrity & 4 & PASSED & --- \\
\texttt{test\_duplicate\_handling} & Business Logic & 3 & PASSED & --- \\
\texttt{test\_boundary\_values} & Boundary Probing & 4 & FAILED & Expected 400 for empty strings; got 409 (validation ordering) \\
\texttt{test\_type\_confusion} & Type Safety & 3 & FAILED & Expected 400 for non-integer id; got 201 (missing type check) \\
\texttt{test\_ordering\_filtering} & Ordering/Filtering & 3 & FAILED & Expected filter functionality not implemented \\
\texttt{test\_error\_message\_quality} & Error Quality & 4 & FAILED & Expected 400 for non-JSON; got 415 (inconsistent error code) \\
\texttt{test\_idempotency} & Idempotency & 3 & FAILED & State pollution from prior test caused 409 conflict \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Validator Agent Scores}
\label{sec:validator_scores}

Table~\ref{tab:validator_scores_detail} presents the quality scores assigned by the Validator Agent to each generated test.

\begin{table}[ht]
\caption{Validator Agent quality scores per test}
\label{tab:validator_scores_detail}
\centering
\small
\begin{tabular}{l r r r r r}
\toprule
\tabhead{Test Name} & \tabhead{Assertion} & \tabhead{Coverage} & \tabhead{Readability} & \tabhead{Execution} & \tabhead{Overall} \\
\midrule
\texttt{test\_data\_round\_trip} & 0.90 & 0.75 & 0.85 & 1.00 & 0.88 \\
\texttt{test\_partial\_update\_safety} & 0.80 & 0.50 & 0.85 & 1.00 & 0.79 \\
\texttt{test\_delete\_consistency} & 0.80 & 0.75 & 0.80 & 1.00 & 0.84 \\
\texttt{test\_duplicate\_handling} & 0.70 & 0.50 & 0.80 & 1.00 & 0.75 \\
\texttt{test\_boundary\_values} & 0.80 & 0.50 & 0.80 & 0.00 & 0.53 \\
\texttt{test\_type\_confusion} & 0.70 & 0.50 & 0.75 & 0.00 & 0.49 \\
\texttt{test\_ordering\_filtering} & 0.70 & 0.50 & 0.80 & 0.00 & 0.50 \\
\texttt{test\_error\_message\_quality} & 0.80 & 0.50 & 0.85 & 0.00 & 0.54 \\
\texttt{test\_idempotency} & 0.70 & 0.75 & 0.80 & 0.00 & 0.56 \\
\midrule
\textbf{Average} & \textbf{0.77} & \textbf{0.58} & \textbf{0.81} & \textbf{0.44} & \textbf{0.65} \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Bug Classification}
\label{sec:bug_classification}

Table~\ref{tab:bug_classification} classifies each failing test by whether it reveals a genuine application deficiency or a test-side issue.

\begin{table}[ht]
\caption{Classification of test failures}
\label{tab:bug_classification}
\centering
\small
\begin{tabular}{l l l p{5cm}}
\toprule
\tabhead{Test Name} & \tabhead{Type} & \tabhead{Severity} & \tabhead{Description} \\
\midrule
\texttt{test\_boundary\_values} & App bug & Medium & Duplicate detection runs before field validation; empty strings accepted as valid names \\
\texttt{test\_type\_confusion} & App bug & High & No type validation on input fields; application accepts arbitrary types for \texttt{id} \\
\texttt{test\_ordering\_filtering} & Missing feature & Low & Search/filter functionality not fully implemented \\
\texttt{test\_error\_message\_quality} & App bug & Medium & Inconsistent error codes: returns 415 instead of 400 for content-type errors \\
\texttt{test\_idempotency} & Test isolation & Low & State leakage between tests; not an application deficiency \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Endpoint Coverage Analysis}
\label{sec:endpoint_coverage}

Table~\ref{tab:endpoint_coverage_detail} shows which endpoints were targeted by the generated tests.

\begin{table}[ht]
\caption{Endpoint coverage: golden tests vs.\ generated tests}
\label{tab:endpoint_coverage_detail}
\centering
\small
\begin{tabular}{l l c c}
\toprule
\tabhead{Method} & \tabhead{Path} & \tabhead{Golden (23 tests)} & \tabhead{Generated (9 tests)} \\
\midrule
GET & /api/health & \checkmark & --- \\
GET & /api/users & \checkmark & --- \\
POST & /api/users & \checkmark & \checkmark \\
GET & /api/users/\{id\} & \checkmark & \checkmark \\
PUT & /api/users/\{id\} & \checkmark & \checkmark \\
DELETE & /api/users/\{id\} & \checkmark & \checkmark \\
GET & /api/users/search & \checkmark & \checkmark \\
\midrule
\multicolumn{2}{l}{\textbf{Coverage}} & \textbf{7/7 (100\%)} & \textbf{5/7 (71\%)} \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------

\section{Insightful Category Coverage}
\label{sec:category_coverage}

Table~\ref{tab:category_coverage} shows the distribution of generated tests across the eight insightful test categories defined in the Generator Agent's prompt.

\begin{table}[ht]
\caption{Test category coverage}
\label{tab:category_coverage}
\centering
\begin{tabular}{l r l}
\toprule
\tabhead{Category} & \tabhead{Tests} & \tabhead{Coverage} \\
\midrule
State Integrity & 3 & \checkmark \\
Boundary Probing & 1 & \checkmark \\
Business Logic & 1 & \checkmark \\
Error Quality & 1 & \checkmark \\
Idempotency & 1 & \checkmark \\
Concurrency Hints & 0 & --- \\
Authorisation Boundaries & 0 & --- \\
Data Leakage & 0 & --- \\
\midrule
\textbf{Total} & \textbf{9} & \textbf{4/8 predefined} \\
\bottomrule
\end{tabular}
\end{table}

Note: The three uncovered categories (concurrency, authorisation, data leakage) are more relevant to applications with authentication and concurrent access patterns. The target Flask CRUD API uses in-memory storage without authentication, which explains the generator's focus on other categories.
