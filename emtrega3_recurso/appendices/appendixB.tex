% Appendix B - Extended Code Listings

\chapter{Extended Code Listings}
\label{AppendixB}

This appendix provides extended code listings for key components of the prototype implementation described in Chapter~\ref{chap:implementation}.

%----------------------------------------------------------------------------------------

\section{Configuration Schema}
\label{sec:config_schema}

\begin{lstlisting}[language=Python, caption={System configuration schema (config/schema.py)}, label={lst:config_schema}]
from pydantic import BaseModel, Field
from typing import Dict, List, Optional
from enum import Enum

class LLMProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"

class LLMConfig(BaseModel):
    """Configuration for LLM provider."""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = Field(default=0.2, ge=0, le=2)
    max_tokens: int = Field(default=4096, ge=1)
    timeout: int = Field(default=60, ge=1)

class SandboxConfig(BaseModel):
    """Configuration for sandbox environments."""
    base_image: str = "python:3.11-slim"
    cpu_limit: int = Field(default=2, ge=1)
    memory_limit: str = "2g"
    timeout_seconds: int = Field(default=300, ge=1)
    network_mode: str = "none"
    allowed_hosts: List[str] = []

class SecurityConfig(BaseModel):
    """Security-related configuration."""
    enable_pii_scrubbing: bool = True
    pii_categories: List[str] = [
        "EMAIL_ADDRESS", "PHONE_NUMBER", "PERSON",
        "CREDIT_CARD", "IP_ADDRESS", "LOCATION"
    ]
    secret_patterns_file: Optional[str] = None
    audit_log_path: str = "logs/audit.jsonl"
    require_human_approval: bool = False
    approval_threshold: float = 0.8

class AgentConfig(BaseModel):
    """Configuration for individual agents."""
    agent_id: str
    role: str
    llm_config: LLMConfig
    tools: List[str] = []
    permissions: Dict[str, List[str]] = {}

class SystemConfig(BaseModel):
    """Top-level system configuration."""
    project_name: str
    agents: Dict[str, AgentConfig]
    sandbox: SandboxConfig
    security: SecurityConfig
    default_llm: LLMConfig
    output_directory: str = "tests/generated"
    log_level: str = "INFO"
\end{lstlisting}

%----------------------------------------------------------------------------------------

\section{LLM Gateway Implementation}
\label{sec:llm_gateway}

\begin{lstlisting}[language=Python, caption={LLM Gateway with provider abstraction (llm/gateway.py)}, label={lst:llm_gateway}]
from abc import ABC, abstractmethod
from typing import AsyncIterator, List, Optional
import asyncio
import hashlib
from functools import lru_cache

class LLMResponse(BaseModel):
    content: str
    model: str
    usage: Dict[str, int]
    finish_reason: str

class BaseLLMProvider(ABC):
    """Abstract base class for LLM providers."""

    @abstractmethod
    async def complete(
        self,
        messages: List[Dict],
        **kwargs
    ) -> LLMResponse:
        pass

    @abstractmethod
    async def stream(
        self,
        messages: List[Dict],
        **kwargs
    ) -> AsyncIterator[str]:
        pass

class OpenAIProvider(BaseLLMProvider):
    """OpenAI API provider implementation."""

    def __init__(self, config: LLMConfig):
        from openai import AsyncOpenAI
        self.client = AsyncOpenAI(api_key=config.api_key)
        self.config = config

    async def complete(
        self,
        messages: List[Dict],
        **kwargs
    ) -> LLMResponse:
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=kwargs.get("temperature", self.config.temperature),
            max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
        )
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens
            },
            finish_reason=response.choices[0].finish_reason
        )

class LLMGateway:
    """Central gateway for LLM interactions with caching and rate limiting."""

    def __init__(
        self,
        config: SystemConfig,
        security: "SecurityContext"
    ):
        self.config = config
        self.security = security
        self.providers: Dict[str, BaseLLMProvider] = {}
        self.cache: Dict[str, LLMResponse] = {}
        self.rate_limiter = asyncio.Semaphore(10)
        self._init_providers()

    def _init_providers(self):
        """Initialize configured LLM providers."""
        for agent_id, agent_config in self.config.agents.items():
            llm_config = agent_config.llm_config
            if llm_config.provider == LLMProvider.OPENAI:
                self.providers[agent_id] = OpenAIProvider(llm_config)
            elif llm_config.provider == LLMProvider.ANTHROPIC:
                self.providers[agent_id] = AnthropicProvider(llm_config)
            elif llm_config.provider == LLMProvider.OLLAMA:
                self.providers[agent_id] = OllamaProvider(llm_config)

    async def complete(
        self,
        agent_id: str,
        prompt: str,
        system: Optional[str] = None,
        use_cache: bool = True,
        **kwargs
    ) -> LLMResponse:
        """Execute LLM completion with security controls."""

        # Scrub PII from prompt
        scrubbed_prompt, scrub_log = self.security.scrub_pii(prompt)

        # Check cache
        cache_key = self._cache_key(agent_id, scrubbed_prompt, system)
        if use_cache and cache_key in self.cache:
            return self.cache[cache_key]

        # Rate limiting
        async with self.rate_limiter:
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": scrubbed_prompt})

            provider = self.providers.get(agent_id)
            if not provider:
                provider = self._get_default_provider()

            response = await provider.complete(messages, **kwargs)

        # Audit logging
        self.security.audit_log(
            event="llm_completion",
            agent_id=agent_id,
            prompt_hash=hashlib.sha256(scrubbed_prompt.encode()).hexdigest()[:16],
            response_length=len(response.content),
            tokens_used=sum(response.usage.values()),
            scrub_actions=len(scrub_log)
        )

        # Cache response
        if use_cache:
            self.cache[cache_key] = response

        return response

    def _cache_key(self, agent_id: str, prompt: str, system: str) -> str:
        content = f"{agent_id}:{system or ''}:{prompt}"
        return hashlib.sha256(content.encode()).hexdigest()
\end{lstlisting}

%----------------------------------------------------------------------------------------

\section{Prompt Templates}
\label{sec:prompt_templates}

\begin{lstlisting}[language=Python, caption={Prompt templates for test generation (llm/prompts/test\_generation.py)}, label={lst:prompts}]
SYSTEM_PROMPT_TEST_GENERATION = """You are an expert software test engineer.
Your task is to generate high-quality pytest test cases.

Guidelines:
1. Each test should verify ONE specific behavior
2. Use descriptive names: test_<function>_<scenario>_<expected>
3. Follow the Arrange-Act-Assert pattern
4. Include edge cases and error conditions
5. Use appropriate fixtures and mocking
6. Generate meaningful assertions (not just `assert True`)

Output format:
Return a JSON array of test objects with this structure:
{
  "test_name": "test_function_scenario_expected",
  "test_code": "def test_function_scenario_expected():\n    ...",
  "target_function": "function_name",
  "rationale": "Why this test is important",
  "assertions": ["list", "of", "assertions"]
}
"""

TEST_GENERATION_PROMPT = """Generate pytest tests for the following code:

## Target Function
```python
{function_code}
```

## Function Context
- Module: {module_name}
- Dependencies: {dependencies}
- Docstring: {docstring}

## Additional Context
{additional_context}

## Requirements
- Generate tests for normal behavior
- Generate tests for edge cases: {edge_cases}
- Generate tests for error handling
- Use these fixtures if needed: {available_fixtures}

Generate comprehensive tests as a JSON array.
"""

VALIDATION_PROMPT = """Review the following generated test for quality issues:

```python
{test_code}
```

Target function: {target_function}

Evaluate:
1. Does the test verify meaningful behavior?
2. Are assertions specific and non-trivial?
3. Is the test isolated and deterministic?
4. Does it follow testing best practices?
5. Are there any security concerns in the test code?

Return a JSON object with:
{
  "quality_score": 0.0-1.0,
  "issues": [{"severity": "...", "description": "..."}],
  "recommendations": ["..."],
  "approved": true/false
}
"""
\end{lstlisting}

%----------------------------------------------------------------------------------------

\section{Security Components}
\label{sec:security_components}

\begin{lstlisting}[language=Python, caption={Complete PII Scrubber implementation (security/scrubber.py)}, label={lst:scrubber_full}]
import re
from typing import Dict, List, Tuple
from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig

class PIIScrubber:
    """
    Comprehensive PII scrubbing for code and text.
    Combines regex patterns for code-specific secrets with
    NER-based detection for general PII.
    """

    # Code-specific secret patterns
    SECRET_PATTERNS = [
        # API Keys
        (r'(?i)(api[_-]?key|apikey)\s*[=:]\s*["\']?[\w-]{20,}["\']?',
         '<API_KEY_REDACTED>'),
        # Passwords
        (r'(?i)(password|passwd|pwd|secret)\s*[=:]\s*["\']?[^\s"\']{8,}["\']?',
         '<PASSWORD_REDACTED>'),
        # AWS Keys
        (r'AKIA[0-9A-Z]{16}', '<AWS_ACCESS_KEY_REDACTED>'),
        (r'(?i)aws[_-]?secret[_-]?access[_-]?key\s*[=:]\s*["\']?[\w/+=]{40}["\']?',
         '<AWS_SECRET_REDACTED>'),
        # GitHub tokens
        (r'ghp_[a-zA-Z0-9]{36}', '<GITHUB_TOKEN_REDACTED>'),
        (r'gho_[a-zA-Z0-9]{36}', '<GITHUB_OAUTH_REDACTED>'),
        # OpenAI keys
        (r'sk-[a-zA-Z0-9]{48}', '<OPENAI_KEY_REDACTED>'),
        # Generic tokens
        (r'(?i)(bearer|token)\s+[a-zA-Z0-9\-_.]{20,}', '<BEARER_TOKEN_REDACTED>'),
        # Private keys
        (r'-----BEGIN (?:RSA |EC |DSA )?PRIVATE KEY-----[\s\S]*?-----END (?:RSA |EC |DSA )?PRIVATE KEY-----',
         '<PRIVATE_KEY_REDACTED>'),
        # Connection strings
        (r'(?i)(mongodb|postgres|mysql|redis)://[^\s]+', '<CONNECTION_STRING_REDACTED>'),
        # JWT tokens
        (r'eyJ[a-zA-Z0-9_-]*\.eyJ[a-zA-Z0-9_-]*\.[a-zA-Z0-9_-]*', '<JWT_REDACTED>'),
    ]

    def __init__(self, config: Optional[SecurityConfig] = None):
        self.config = config or SecurityConfig()

        # Initialize Presidio
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()

        # Add custom recognizers for code patterns
        self._add_custom_recognizers()

    def _add_custom_recognizers(self):
        """Add custom pattern recognizers for code-specific secrets."""
        for i, (pattern, _) in enumerate(self.SECRET_PATTERNS):
            recognizer = PatternRecognizer(
                supported_entity=f"CODE_SECRET_{i}",
                patterns=[Pattern(name=f"secret_{i}", regex=pattern, score=0.9)]
            )
            self.analyzer.registry.add_recognizer(recognizer)

    def scrub(self, text: str) -> Tuple[str, List[Dict]]:
        """
        Scrub PII and secrets from text.

        Args:
            text: Input text potentially containing PII

        Returns:
            Tuple of (scrubbed_text, scrub_log)
        """
        scrub_log = []
        cleaned = text

        # First pass: regex-based secret detection (faster)
        for pattern, replacement in self.SECRET_PATTERNS:
            matches = list(re.finditer(pattern, cleaned))
            if matches:
                cleaned = re.sub(pattern, replacement, cleaned)
                scrub_log.append({
                    "type": "code_secret",
                    "pattern": pattern[:30] + "...",
                    "count": len(matches),
                    "replacement": replacement
                })

        # Second pass: Presidio NER-based detection
        if self.config.pii_categories:
            results = self.analyzer.analyze(
                cleaned,
                entities=self.config.pii_categories,
                language="en"
            )

            if results:
                # Configure anonymization operators
                operators = {
                    "EMAIL_ADDRESS": OperatorConfig("replace", {"new_value": "<EMAIL_REDACTED>"}),
                    "PHONE_NUMBER": OperatorConfig("replace", {"new_value": "<PHONE_REDACTED>"}),
                    "PERSON": OperatorConfig("replace", {"new_value": "<PERSON_REDACTED>"}),
                    "CREDIT_CARD": OperatorConfig("replace", {"new_value": "<CC_REDACTED>"}),
                    "IP_ADDRESS": OperatorConfig("replace", {"new_value": "<IP_REDACTED>"}),
                    "LOCATION": OperatorConfig("replace", {"new_value": "<LOCATION_REDACTED>"}),
                }

                anonymized = self.anonymizer.anonymize(
                    cleaned,
                    results,
                    operators=operators
                )
                cleaned = anonymized.text

                for result in results:
                    scrub_log.append({
                        "type": result.entity_type,
                        "score": result.score,
                        "start": result.start,
                        "end": result.end
                    })

        return cleaned, scrub_log

    def scan_only(self, text: str) -> List[Dict]:
        """Scan for PII without modifying text."""
        _, log = self.scrub(text)
        return log
\end{lstlisting}

%----------------------------------------------------------------------------------------

\section{Docker Sandbox Configuration}
\label{sec:docker_config}

\begin{lstlisting}[caption={Dockerfile for secure sandbox environment (docker/Dockerfile.sandbox)}, label={lst:dockerfile}]
# Secure Python sandbox for test execution
FROM python:3.11-slim-bookworm

# Security: Create non-root user
RUN groupadd -r sandbox && useradd -r -g sandbox sandbox

# Install minimal dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Security: Remove unnecessary tools
RUN rm -rf /usr/bin/wget /usr/bin/curl 2>/dev/null || true

# Create workspace with proper permissions
RUN mkdir -p /workspace /home/sandbox && \
    chown -R sandbox:sandbox /workspace /home/sandbox

# Install Python testing tools
COPY requirements-sandbox.txt /tmp/
RUN pip install --no-cache-dir -r /tmp/requirements-sandbox.txt && \
    rm /tmp/requirements-sandbox.txt

# Security: Switch to non-root user
USER sandbox
WORKDIR /workspace

# Security: Set restrictive umask
ENV UMASK=077

# Default command (overridden at runtime)
CMD ["python", "--version"]
\end{lstlisting}

\begin{lstlisting}[caption={Seccomp security profile (docker/seccomp-profile.json)}, label={lst:seccomp}]
{
  "defaultAction": "SCMP_ACT_ERRNO",
  "architectures": ["SCMP_ARCH_X86_64"],
  "syscalls": [
    {
      "names": [
        "read", "write", "open", "close", "stat", "fstat",
        "lstat", "poll", "lseek", "mmap", "mprotect", "munmap",
        "brk", "rt_sigaction", "rt_sigprocmask", "rt_sigreturn",
        "ioctl", "access", "pipe", "select", "sched_yield",
        "mremap", "msync", "mincore", "madvise", "dup", "dup2",
        "nanosleep", "getpid", "getuid", "getgid", "geteuid",
        "getegid", "getppid", "getpgrp", "setsid", "getgroups",
        "uname", "fcntl", "flock", "fsync", "fdatasync",
        "truncate", "ftruncate", "getcwd", "chdir", "rename",
        "mkdir", "rmdir", "link", "unlink", "symlink", "readlink",
        "chmod", "fchmod", "chown", "fchown", "umask", "getrlimit",
        "getrusage", "times", "clock_gettime", "clock_getres",
        "exit", "exit_group", "wait4", "kill", "clone", "fork",
        "vfork", "execve", "arch_prctl", "set_tid_address",
        "set_robust_list", "futex", "sched_getaffinity",
        "openat", "newfstatat", "readlinkat", "faccessat",
        "pread64", "pwrite64", "getrandom"
      ],
      "action": "SCMP_ACT_ALLOW"
    }
  ]
}
\end{lstlisting}

%----------------------------------------------------------------------------------------

\section{CLI Implementation}
\label{sec:cli_impl}

\begin{lstlisting}[language=Python, caption={Command-line interface (integration/cli.py)}, label={lst:cli_full}]
import click
import asyncio
from pathlib import Path
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()

@click.group()
@click.version_option(version="0.1.0")
def cli():
    """MAS Testing - Secure Multi-Agent Software Testing System"""
    pass

@cli.command()
@click.argument('project_path', type=click.Path(exists=True))
@click.option('--requirements', '-r', default="",
              help='Testing requirements in natural language')
@click.option('--output', '-o', default='tests/generated',
              help='Output directory for generated tests')
@click.option('--config', '-c', type=click.Path(exists=True),
              help='Path to configuration file')
@click.option('--model', default='gpt-4-turbo',
              help='LLM model to use')
@click.option('--local', is_flag=True,
              help='Use local LLM (Ollama) for privacy')
@click.option('--dry-run', is_flag=True,
              help='Analyze without generating tests')
@click.option('--verbose', '-v', is_flag=True,
              help='Enable verbose output')
def generate(project_path, requirements, output, config, model, local, dry_run, verbose):
    """Generate tests for a Python project."""

    console.print(f"[bold blue]MAS Testing[/bold blue] - Secure Test Generation")
    console.print(f"Project: {project_path}")

    # Load configuration
    system_config = load_config(config) if config else default_config()

    if local:
        system_config.default_llm.provider = LLMProvider.OLLAMA
        system_config.default_llm.model = "codellama:34b"
        console.print("[green]Using local LLM for privacy[/green]")
    else:
        system_config.default_llm.model = model

    # Initialize system
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:

        task = progress.add_task("Initializing agents...", total=None)
        workflow = TestingWorkflow.from_config(system_config)

        if dry_run:
            progress.update(task, description="Analyzing project...")
            analysis = asyncio.run(workflow.analyze_only(project_path))
            display_analysis(analysis)
            return

        progress.update(task, description="Generating tests...")
        result = asyncio.run(workflow.run(project_path, requirements))

    # Display results
    if result.status == "completed":
        output_path = Path(output)
        output_path.mkdir(parents=True, exist_ok=True)

        for test in result.tests:
            test_file = output_path / f"test_{test.target_function}.py"
            test_file.write_text(test.test_code)

        console.print(f"\n[green]Success![/green] Generated {len(result.tests)} tests")
        console.print(f"Output directory: {output_path}")

        # Summary table
        display_summary(result)
    else:
        console.print(f"\n[red]Failed:[/red] {result.reason}")
        if result.security_findings:
            console.print("\n[yellow]Security issues found:[/yellow]")
            for finding in result.security_findings:
                console.print(f"  - {finding.description}")

@cli.command()
@click.argument('project_path', type=click.Path(exists=True))
@click.option('--output', '-o', default='security-report.json',
              help='Output file for security report')
def audit(project_path, output):
    """Run security audit on a project."""
    console.print("[bold]Running security audit...[/bold]")

    # Initialize security agent only
    security_agent = SecurityAgent.from_default_config()

    with Progress(SpinnerColumn(), TextColumn("{task.description}")) as progress:
        task = progress.add_task("Scanning...", total=None)
        result = asyncio.run(security_agent.full_scan(project_path))

    # Write report
    Path(output).write_text(result.model_dump_json(indent=2))
    console.print(f"\n[green]Audit complete.[/green] Report: {output}")
    console.print(f"Risk level: {result.overall_risk}")

if __name__ == '__main__':
    cli()
\end{lstlisting}
