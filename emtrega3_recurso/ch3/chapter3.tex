% Chapter 3 - Data Collection and Pre-processing

\chapter{Data Collection and Pre-processing}
\label{chap:data_collection}

This chapter describes the data sources, collection methods, and pre-processing steps that feed the two pipelines of the TestForge platform. The Golden Examples pipeline relies on existing test files as its primary data source, while the Black-Box Observer pipeline captures HTTP traffic from running applications. Both pipelines transform raw inputs into structured representations suitable for LLM-based test generation.

%----------------------------------------------------------------------------------------

\section{Data Sources and Selection}
\label{sec:data_sources}

\subsection{Selection Criteria and Objectives}

The quality of generated tests depends critically on the quality and representativeness of input data. For the Golden Examples pipeline, this means selecting reference test files that demonstrate good testing practices. For the Observer pipeline, it means capturing sufficiently diverse HTTP interactions to cover the application's API surface.

Several criteria guided data source selection for both pipelines. Input data should be representative, covering the range of testing patterns and API behaviours that the platform will encounter in practice. Golden test examples should demonstrate quality through meaningful assertions, proper fixture usage, and comprehensive scenario coverage rather than trivial smoke tests. Sources should offer diversity, spanning different testing styles, HTTP methods, error handling patterns, and application domains to prevent overfitting to a single pattern. Finally, traceability requires that each data source be documented with its origin, collection method, and any transformations applied.

\subsection{Golden Test Examples}
\label{sec:golden_examples}

Golden examples are existing, human-written test files that serve as reference patterns for the generation process. These files encode implicit knowledge about testing framework conventions such as pytest fixtures, parametrize decorators, and assertion styles, as well as HTTP client usage patterns including requests library calls, authentication headers, and response parsing. They also capture test naming conventions and documentation practices, fixture design patterns covering setup/teardown semantics, dependency injection, and scope management, and assertion strategies ranging from status code checking through response body validation to error message verification.

For the evaluation presented in Chapter~\ref{chap:experimentation}, golden examples were collected from a Flask CRUD API application comprising 23 test functions across 3 test files. These tests cover all CRUD operations (Create, Read, Update, Delete) and include both happy-path and error scenarios. All tests make consistent use of pytest fixtures with the \texttt{requests} HTTP client, and include search endpoint testing with query parameter validation.

\subsection{HTTP Traffic Captures}
\label{sec:http_captures}

The Observer pipeline accepts HTTP traffic data from three sources: pre-captured exchange data in the form of JSON arrays of HTTP request/response pairs provided directly via the API, HAR (HTTP Archive) files \parencite{w3char2012} in the industry-standard format exported from browser developer tools or proxy servers, and live traffic capture through real-time interception via mitmproxy.

Each captured exchange includes: HTTP method, URL, path, query parameters, request headers, request body, response status code, response headers, response body, and timing information.

%----------------------------------------------------------------------------------------

\section{Pre-processing: Golden Examples Pipeline}
\label{sec:preprocessing_golden}

\subsection{AST Parsing with Tree-sitter}
\label{sec:ast_parsing}

The Analyser Agent uses Tree-sitter, an incremental parsing library \parencite{treesitter}, to perform \acrfull{AST} analysis on golden test files. Tree-sitter was selected over Python's built-in \texttt{ast} module for its ability to handle partial or syntactically incomplete code gracefully, a practical consideration when processing real-world test files that may contain syntax variations.

The parsing process extracts the following structural elements from each test file:

\begin{description}
    \item[Imports] All import statements, distinguishing between standard library, third-party, and local imports. This information determines which libraries the generated tests should reference.

    \item[Fixtures] Functions decorated with \texttt{@pytest.fixture}, including their names, parameter lists, scope declarations, and whether they use \texttt{yield} for setup/teardown semantics.

    \item[Test Functions] Functions whose names begin with \texttt{test\_}, including their docstrings, parameter dependencies (fixture injection), and the HTTP methods and endpoints they target.

    \item[Assertions] All assertion statements within test functions, classified by type: status code checks (\texttt{assert response.status\_code == ...}), response body validations, collection membership tests, and exception assertions.

    \item[Helper Functions] Non-test, non-fixture functions that provide utility support (e.g., data factories, response parsers).
\end{description}

\subsection{HTTP Endpoint Detection}
\label{sec:endpoint_detection}

A critical pre-processing step is extracting the HTTP methods and endpoints targeted by each test function. The analyser employs a multi-strategy approach. The first strategy, direct method call detection, identifies calls to \texttt{requests.get()}, \texttt{requests.post()}, \texttt{requests.put()}, \texttt{requests.delete()}, and \texttt{requests.patch()} within each test function body. The second strategy, f-string URL extraction, parses f-string arguments to HTTP client calls (e.g., \texttt{f"\{base\_url\}/api/users/\{user\_id\}"}) using regular expressions to extract the path component after the base URL variable. The third strategy handles non-f-string URL arguments where the path is embedded as a plain string literal.

For each detected HTTP call, the analyser records the HTTP method (GET, POST, PUT, DELETE, PATCH), the endpoint path (e.g., \texttt{/api/users}, \texttt{/api/users/\{id\}}), and the position within the test function, distinguishing setup calls from assertion targets.

\subsection{Pattern Aggregation into Style Guide}
\label{sec:style_guide}

After parsing all golden examples, the Analyser Agent aggregates the extracted patterns into a \texttt{TestStyleGuide}, a structured summary of the testing conventions observed across the example files. The style guide captures the detected testing framework (e.g., \texttt{pytest}) and HTTP client library (e.g., \texttt{requests}), along with the test function naming pattern (e.g., \texttt{test\_<action>\_<scenario>}). It also records the set of common imports shared across multiple golden files, common fixtures that indicate reusable test infrastructure, and the most frequently used assertion forms with their relative frequencies. Finally, it includes average metrics such as assertions per test, test length, and the proportion of tests containing docstrings. This style guide serves as structured context for the Generator Agent's LLM prompt, enabling few-shot generation that aligns with the project's existing conventions.

%----------------------------------------------------------------------------------------

\section{Pre-processing: Observer Pipeline}
\label{sec:preprocessing_observer}

\subsection{HTTP Exchange Normalisation}
\label{sec:exchange_normalisation}

Raw HTTP traffic data varies significantly in format depending on the capture method. The Observer Agent normalises all inputs into a common \texttt{HTTPExchange} representation through several transformations. Method normalisation uppercases HTTP methods and validates them against the standard set. URL decomposition splits full URLs into base URL, path, and query parameters. Body parsing processes request and response bodies as JSON where the content type indicates \texttt{application/json}, storing them as raw text otherwise. Header filtering removes headers irrelevant to API semantics (e.g., \texttt{Connection}, \texttt{Accept-Encoding}) while preserving authentication-related headers.

For HAR file input, the parser follows the W3C HTTP Archive specification, which contains detailed timing, cookie, and redirect information beyond what is needed for test generation. The HAR parser extracts only the fields relevant to API testing: it iterates over the \texttt{log.entries} array, extracting for each entry the \texttt{request.method}, \texttt{request.url}, \texttt{request.headers}, and \texttt{request.postData}, along with the \texttt{response.status}, \texttt{response.headers}, and \texttt{response.content.text}. Non-API requests (static assets, tracking pixels) are filtered out based on content type and URL patterns.

\subsection{Endpoint Mapping and Schema Inference}
\label{sec:endpoint_mapping}

The Mapper Agent transforms normalised HTTP exchanges into an \texttt{EndpointMap}, a structured representation of the application's API surface. This process involves four key operations.

\textbf{Path normalisation} detects dynamic path segments and replaces them with parameter placeholders. Numeric segments (e.g., \texttt{/users/42}) are replaced with \texttt{\{id\}}, and UUID segments are replaced with \texttt{\{uuid\}}. The resulting normalised paths group exchanges that target the same logical endpoint.

\textbf{Schema inference} examines multiple exchanges to the same endpoint to infer JSON schemas for request and response bodies. It records field names, types, and whether fields appear consistently (required) or intermittently (optional), and detects common patterns such as pagination fields, error response structures, and nested objects.

\textbf{Authentication pattern detection} analyses request headers across all exchanges to identify authentication mechanisms. Bearer token authentication is detected via \texttt{Authorization: Bearer ...} headers, while basic authentication is identified through \texttt{Authorization: Basic ...} headers. API key authentication is recognised by common header names such as \texttt{X-API-Key} and \texttt{api-key}, and cookie-based sessions are detected through \texttt{Cookie} or \texttt{Set-Cookie} headers.

\textbf{Dependency chain detection} identifies relationships between endpoints by analysing the flow of identifiers. A \texttt{POST} endpoint that returns an \texttt{id} field is linked to subsequent \texttt{GET}, \texttt{PUT}, and \texttt{DELETE} endpoints that use the same identifier as a path parameter. These dependencies inform the test generation order, ensuring that tests create required resources before attempting to read, update, or delete them.

%----------------------------------------------------------------------------------------

\section{Data Representation Models}
\label{sec:data_models}

All data flowing through the TestForge pipelines is represented using Pydantic v2 models \parencite{pydantic2017}, which provide runtime type validation, serialisation, and documentation. The key data models are:

\begin{description}
    \item[\texttt{GoldenExample}] A parsed test file containing lists of imports, fixtures, test functions, helper functions, and class definitions. Each test function includes its name, docstring, HTTP method, endpoint, parameter list, and assertions.

    \item[\texttt{TestStyleGuide}] An aggregated summary of testing conventions extracted from multiple golden examples, as described in Section~\ref{sec:style_guide}.

    \item[\texttt{HTTPExchange}] A single HTTP request/response pair with method, URL, headers, body, status code, and timing.

    \item[\texttt{EndpointInfo}] A single API endpoint with its method, path, request/response schemas, authentication requirements, and sample exchanges.

    \item[\texttt{EndpointMap}] The complete API surface of a target application: a collection of \texttt{EndpointInfo} objects with detected authentication patterns and endpoint dependencies.

    \item[\texttt{TestSuite}] A collection of generated tests with metadata, quality scores, and a method for rendering the suite as executable Python code.

    \item[\texttt{AppContext}] A persistent per-application record of discovered endpoints, coverage status, previous run results, and learned patterns, enabling progressive improvement across sessions.
\end{description}

%----------------------------------------------------------------------------------------

\section{Risk Analysis and Data Bias}
\label{sec:risk_analysis}

\subsection{Golden Example Bias}

The quality and diversity of golden examples directly constrain the quality of generated tests. Several bias risks were identified. First, style overfitting may occur if golden examples use a narrow range of assertion patterns, causing the generator to produce tests limited to the same patterns and missing opportunities for more insightful test strategies. Second, coverage bias arises when golden examples disproportionately cover happy-path scenarios, leading to underrepresentation of error cases, boundary conditions, and security-related tests in generated output. Third, framework lock-in is a concern because the current implementation assumes pytest and the requests library; golden examples using other frameworks such as unittest or aiohttp would require adapter support.

\subsection{Observer Data Limitations}

HTTP traffic captures may not represent the full API surface. Incomplete coverage is a primary concern: if the observed session does not exercise all endpoints, the Mapper Agent will produce an incomplete endpoint map. Sampling bias compounds this problem, as traffic captures may overrepresent frequently-used endpoints and underrepresent administrative or edge-case operations. Additionally, state dependency poses a challenge because some API behaviours depend on prior state (e.g., deleting a resource requires first creating it), and a single observation session may not capture all state transitions.

\subsection{Mitigation Strategies}

To address these biases, the platform implements several countermeasures. The LLM prompt explicitly instructs the generator to produce tests in eight insightful categories (state integrity, boundary probing, business logic, error quality, concurrency hints, authorisation boundaries, data leakage, and regression traps) regardless of the patterns observed in golden examples. The persistent memory system (\texttt{AppContext}) tracks which endpoints have been tested and which remain untested, guiding subsequent generation runs toward coverage gaps. Furthermore, the combined pipeline mode allows golden examples and observer data to be used together, compensating for the limitations of each approach individually.
